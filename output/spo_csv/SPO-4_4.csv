input,subject,relation,object
第4章知识抽取与知识挖掘王志春北京师范大学，陈华钧浙江大学，王昊奋上海乐言信息科技有限公司知识抽取是构建大规模知识图谱的重要环节，而知识挖掘则是在已有知识图谱的基础上发现其隐藏的知识。,知识抽取,依靠,构建大规模知识图谱
知识抽取和知识挖掘对于知识图谱的构建及应用具有重要的意义。,知识抽取,依靠,知识挖掘
本章将首先介绍知识抽取的技术和方法，然后介绍知识内容挖掘和知识结构挖掘。,知识抽取,依靠,知识内容挖掘
本章将首先介绍知识抽取的技术和方法，然后介绍知识内容挖掘和知识结构挖掘。,知识抽取,依靠,知识结构挖掘
4.1知识抽取任务及相关竞赛4.1.1知识抽取任务定义知识抽取是实现自动化构建大规模知识图谱的重要技术，其目的在于从不同来源、不同结构的数据中进行知识提取并存入知识图谱中。,知识抽取,中文名,从不同来源、不同结构的数据中进行知识提取并存入知识图谱中
4.1知识抽取任务及相关竞赛4.1.1知识抽取任务定义知识抽取是实现自动化构建大规模知识图谱的重要技术，其目的在于从不同来源、不同结构的数据中进行知识提取并存入知识图谱中。,知识抽取,依靠,从不同来源、不同结构的数据中进行知识提取并存入知识图谱中。
知识抽取的数据源可以是结构化数据（如链接数据、数据库）、半结构化数据（如网页中的表格、列表）或者非结构化数据（即纯文本数据）。,知识抽取,依靠,结构化数据
知识抽取的数据源可以是结构化数据（如链接数据、数据库）、半结构化数据（如网页中的表格、列表）或者非结构化数据（即纯文本数据）。,知识抽取,依靠,半结构化数据
知识抽取的数据源可以是结构化数据（如链接数据、数据库）、半结构化数据（如网页中的表格、列表）或者非结构化数据（即纯文本数据）。,知识抽取,依靠,非结构化数据
面向不同类型的数据源，知识抽取涉及的关键技术和需要解决的技术难点有所不同。,知识抽取,依靠,不同类型的数据源
知识抽取的概念最早在20世纪70年代后期出现于NLP研究领域，是指自动化地从文本中发现和抽取相关信息，并将多个文本碎片中的信息进行合并，将非结构化数据转换为结构化数据，包括某一特定领域的模式、实体关系或RDF三元组。,知识抽取,依靠,"自动化地从文本中发现和抽取相关信息,并将多个文本碎片中的信息进行合并,将非结构化数据转换为结构化数据"
给定一段关于苹果公司的文字描述，知识抽取方法可以自动获取关于苹果公司的结构化信息，包括其总部地址、创始人以及创立时间。,知识抽取方法,中文名,获取关于苹果公司的结构化信息
给定一段关于苹果公司的文字描述，知识抽取方法可以自动获取关于苹果公司的结构化信息，包括其总部地址、创始人以及创立时间。,知识抽取方法,依靠,自动获取关于苹果公司的结构化信息
图4-1知识抽取的典型例子具体地，知识抽取包括以下子任务：1.命名实体识别从文本中检测出命名实体，并将其分类到预定义的类别中，例如人物、组织、地点、时间等。,知识抽取,依靠,命名实体识别
2.关系抽取从文本中识别抽取实体及实体之间的关系。,关系抽取,依靠,从文本中识别抽取实体及实体之间的关系
例如，从句子“[王思聪]是万达集团董事长[王健林]的独子”中识别出实体“[王健林]”和“[王思聪]”之间具有“父子”关系。,[王思聪],依靠,[王健林]的独子
3.事件抽取识别文本中关于事件的信息，并以结构化的形式呈现。,事件抽取,依靠,识别文本中关于事件的信息，并以结构化的形式呈现。
例如，从恐怖袭击事件的新闻报道中识别袭击发生的地点、时间、袭击目标和受害人等信息。,从恐怖袭击事件的新闻报道中识别袭击发生的地点、时间、袭击目标和受害人等信息,依靠,从恐怖袭击事件的新闻报道中识别袭击发生的地点、时间、袭击目标和受害人等信息
4.1.2知识抽取相关竞赛一些重要的竞赛对知识抽取技术的发展起到了巨大的推动作用。,知识抽取相关竞赛,依靠,推动
这些竞赛一般与学术会议同时举办，在明确定义知识抽取相关任务的基础上，提供标准评测数据和评测指标，吸引了大量的参与者。,知识抽取,依靠,学术会议同时举办，在明确定义知识抽取相关任务的基础上，提供标准评测数据和评测指标
本节将介绍知识抽取相关的重要竞赛。,知识抽取相关的重要竞赛,依靠,本节将介绍
"1.消息理解会议（Message_Understanding_Conference,MUC）MUC由美国国防部高级研究计划局（DARPA）启动并资助，目的是鼓励和开发更好的信息抽取方法。",MUC,中文名,消息理解会议
"1.消息理解会议（Message_Understanding_Conference,MUC）MUC由美国国防部高级研究计划局（DARPA）启动并资助，目的是鼓励和开发更好的信息抽取方法。",MUC,依靠,DARPA启动并资助
1987—1998年，MUC会议共举办了七届。,MUC会议,依靠,1987—1998年
MUC不仅仅是学术会议，其更重要的是在于对信息抽取系统的评测。,MUC,依靠,对信息抽取系统的评测
在每届MUC会议前，组织者向参加者提供消息文本的样例和信息抽取任务的说明；参加者开发参赛系统并提交系统的输出结果。,参加MUC会议的参赛系统,依靠,组织者提供消息文本的样例和信息抽取任务的说明
各个系统的结果与标准结果比对后得到最终的评测结果，参与者最后在会议上交流技术和感受。,评测结果的生成,依靠,各个系统的结果与标准结果比对
在MUC的评测中，召回率（Recall）和精确率（Precision）是评价信息抽取系统性能的两个重要评价指标。,召回率,依靠,评价信息抽取系统性能的两个重要评价指标
在MUC的评测中，召回率（Recall）和精确率（Precision）是评价信息抽取系统性能的两个重要评价指标。,精确率,依靠,评价信息抽取系统性能的两个重要评价指标
召回率是系统抽取的正确结果占标准结果的比例；精确率是系统抽取的正确结果占其抽取的所有结果的比例。,召回率,中文名,系统抽取的正确结果占标准结果的比例
召回率是系统抽取的正确结果占标准结果的比例；精确率是系统抽取的正确结果占其抽取的所有结果的比例。,精确率,中文名,系统抽取的正确结果占其抽取的所有结果的比例
召回率是系统抽取的正确结果占标准结果的比例；精确率是系统抽取的正确结果占其抽取的所有结果的比例。,召回率,依靠,正确结果所占的比例
召回率是系统抽取的正确结果占标准结果的比例；精确率是系统抽取的正确结果占其抽取的所有结果的比例。,精确率,依靠,正确结果所占的比例
为了综合两个方面的因素考量系统的性能，通常基于召回率和准确率计算F1值。,F1值,依靠,召回率和准确率
MUC会议积极推动了命名实体识别和共指消解等技术的进步与发展。,MUC会议,依靠,命名实体识别和共指消解等技术
在MUC会议中，出现了一些F1值高达90%左右的系统，接近于人工标注的质量。,F1,依靠,质量
MUC定义的一系列规范以及确立的评价体系也已经成为知识抽取领域的标准。,MUC,依靠,一系列规范以及确立的评价体系
"2.自动内容抽取（Automatic_Content_Extraction,ACE）ACE是一项由美国国家标准技术研究所（NIST）组织的评测会议，该会议从1999年至2008年共举办了八次评测。",ACE,中文名,自动内容抽取
"2.自动内容抽取（Automatic_Content_Extraction,ACE）ACE是一项由美国国家标准技术研究所（NIST）组织的评测会议，该会议从1999年至2008年共举办了八次评测。",ACE,依靠,NIST组织的评测会议
ACE与MUC解决的问题类似，但是ACE对MUC定义的任务进行了融合、分类和细化。,ACE,依靠,MUC解决的问题
ACE评测涉及英语、阿拉伯语和汉语三种语言，主要包括以下任务：（1）实体检测和跟踪。,ACE评测,依靠,英语
ACE评测涉及英语、阿拉伯语和汉语三种语言，主要包括以下任务：（1）实体检测和跟踪。,ACE评测,依靠,阿拉伯语
ACE评测涉及英语、阿拉伯语和汉语三种语言，主要包括以下任务：（1）实体检测和跟踪。,ACE评测,依靠,汉语
"这是ACE最基础和核心的任务，该任务要求识别文本中的实体，实体类型包括人物（Person,PER）、组织（Organization,ORG）、设施（Facility,FAC）、地缘政治实体（Geographical_Political_Entity,GPE）和位置（Location,LOC）等。",ACE,中文名,实体识别
"这是ACE最基础和核心的任务，该任务要求识别文本中的实体，实体类型包括人物（Person,PER）、组织（Organization,ORG）、设施（Facility,FAC）、地缘政治实体（Geographical_Political_Entity,GPE）和位置（Location,LOC）等。",实体识别,依靠,人物实体
（2）关系检测与表征。,关系检测,依靠,概念/产品
该任务要求识别和表征实体间的关系，关系被分为五大类，包括角色（role）关系、部分整体（part-whole）关系、位于（at）关系、邻近（near）关系和社会（social）关系，每个大类关系又被进一步细分，总共有24种类型。,关系,中文名,角色（role）关系
该任务要求识别和表征实体间的关系，关系被分为五大类，包括角色（role）关系、部分整体（part-whole）关系、位于（at）关系、邻近（near）关系和社会（social）关系，每个大类关系又被进一步细分，总共有24种类型。,关系,依靠,五大类
（3）事件检测与表征。,事件检测,依靠,概念/产品
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,中文名,交互（interaction）事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,中文名,移动（movement）事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,中文名,转移（transfer）事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,中文名,创建（creation）事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,中文名,销毁（destruction）事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,依靠,交互事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,依靠,移动事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,依靠,转移事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,依靠,创建事件
该任务要求识别实体参与的五类事件，包括交互（interaction）、移动（movement）、转移（transfer）、创建（creation）和销毁（destruction）事件。,五类事件,依靠,销毁事件
任务要求自动标注每个事件的文本提及或锚点，并按类型和子类型对其进行分类；最后，还需要根据类型特定的模板进一步确定事件参数和属性。,自动标注每个事件的文本提及或锚点,中文名,根据类型特定的模板进一步确定事件参数和属性
任务要求自动标注每个事件的文本提及或锚点，并按类型和子类型对其进行分类；最后，还需要根据类型特定的模板进一步确定事件参数和属性。,自动标注每个事件的文本提及或锚点,依靠,根据类型特定的模板进一步确定事件参数和属性
"3.知识库填充（Knowledge_Base_Population,KBP）KBP评测由文本分析会议（Text_Analysis_Conference,TAC）主办，其目标是开发和评估从非结构化文本中获取知识填充知识库的技术。",知识库填充,中文名,知识填充知识库的技术
"3.知识库填充（Knowledge_Base_Population,KBP）KBP评测由文本分析会议（Text_Analysis_Conference,TAC）主办，其目标是开发和评估从非结构化文本中获取知识填充知识库的技术。",KBP评测,依靠,从非结构化文本中获取知识填充知识库的技术
KBP评测从2009年开始，每年举办一次，截至2017年，已经举办了九届。,KBP评测,依靠,每年举办一次
KBP评测覆盖了知识库填充的独立子任务以及被称为“冷启动”的端到端知识库构建任务。,KBP评测,依靠,知识库填充的独立子任务
"独立子任务主要包括以下四个方面：（1）实体发现与链接（Entity_Discovery_and_Linking,EDL）。",独立子任务,依靠,实体发现与链接
主要的EDL任务是在评估文档集合中提取特定个人（PER）、组织（ORG）、设施（FAC）、地缘政治实体（GPE）和位置（LOC）实体的名称和提及，并将每个提及链接到其对应的KB节点。,EDL,中文名,在评估文档集合中提取特定个人（PER）、组织（ORG）、设施（FAC）、地缘政治实体（GPE）和位置（LOC）实体的名称和提及，并将每个提及链接到其对应的KB节点。
主要的EDL任务是在评估文档集合中提取特定个人（PER）、组织（ORG）、设施（FAC）、地缘政治实体（GPE）和位置（LOC）实体的名称和提及，并将每个提及链接到其对应的KB节点。,EDL,依靠,评估文档集合中提取特定个人（PER）、组织（ORG）、设施（FAC）、地缘政治实体（GPE）和位置（LOC）实体的名称和提及
"（2）槽填充（Slot_Filling,SF）。",槽填充,依靠,语义理解
插槽填充任务是搜索文档集合以填充特定实体的特定属性（“插槽”）值。,插槽填充任务,依靠,搜索文档集合以填充特定实体的特定属性（“插槽”）值
（3）事件跟踪（Event_Track）。,事件跟踪,依靠,数据采集
事件跟踪旨在从非结构化文本中提取关于事件的信息，使其作为结构化知识库的输入。,事件跟踪,依靠,从非结构化文本中提取关于事件的信息
该任务具体包括事件块（Event_Nugget）任务（检测和链接事件块）和事件参数（Event_Argument）任务（提取属于同一事件的事件参数和链接属于同一事件的参数）。,事件块任务,中文名,检测和链接事件块
该任务具体包括事件块（Event_Nugget）任务（检测和链接事件块）和事件参数（Event_Argument）任务（提取属于同一事件的事件参数和链接属于同一事件的参数）。,事件参数任务,中文名,提取属于同一事件的事件参数和链接属于同一事件的参数
该任务具体包括事件块（Event_Nugget）任务（检测和链接事件块）和事件参数（Event_Argument）任务（提取属于同一事件的事件参数和链接属于同一事件的参数）。,事件块任务,依靠,检测和链接事件块
该任务具体包括事件块（Event_Nugget）任务（检测和链接事件块）和事件参数（Event_Argument）任务（提取属于同一事件的事件参数和链接属于同一事件的参数）。,事件参数任务,依靠,提取属于同一事件的事件参数和链接属于同一事件的参数
"（4）信念和情感（Belief_and_Sentiment,BeSt）。",信念和情感,依靠,情感分析
信仰和情感跟踪检测实体对另一个实体、关系或事件的信念和情绪。,信仰和情感跟踪,依靠,检测实体对另一个实体、关系或事件的信念和情绪。
端到端冷启动知识库构建任务基于给定的知识库模式（KB_schema）从文本中获取以下信息：实体，在实体发现与链接任务中定义的实体和实体提及；槽关系，在槽填充中涉及的实体属性（“槽”）；事件，在事件跟踪任务中的事件和事件块；事件参数，在事件跟踪任务中的事件参数；情绪，信念和情感任务中源实体向目标实体的情绪。,端到端冷启动知识库构建任务,中文名,基于给定的知识库模式（KB_schema）从文本中获取以下信息：实体，在实体发现与链接任务中定义的实体和实体提及；槽关系，在槽填充中涉及的实体属性（“槽”）；事件，在事件跟踪任务中的事件和事件块；事件参数，在事件跟踪任务中的事件参数；情绪，信念和情感任务中源实体向目标实体的情绪。
端到端冷启动知识库构建任务基于给定的知识库模式（KB_schema）从文本中获取以下信息：实体，在实体发现与链接任务中定义的实体和实体提及；槽关系，在槽填充中涉及的实体属性（“槽”）；事件，在事件跟踪任务中的事件和事件块；事件参数，在事件跟踪任务中的事件参数；情绪，信念和情感任务中源实体向目标实体的情绪。,端到端冷启动知识库构建任务,依靠,给定的知识库模式（KB_schema）从文本中获取以下信息：实体，在实体发现与链接任务中定义的实体和实体提及；槽关系，在槽填充中涉及的实体属性（“槽”）；事件，在事件跟踪任务中的事件和事件块；事件参数，在事件跟踪任务中的事件参数；情绪，信念和情感任务中源实体向目标实体的情绪。
"4.语义评测（Semantic_Evaluation,SemEval）SemEval是由ACL-SIGLEX组织的国际权威的词义消歧评测，目标是增进人们对词SenseEval，于1998年举办第一届。",语义评测,中文名,SemEval
"4.语义评测（Semantic_Evaluation,SemEval）SemEval是由ACL-SIGLEX组织的国际权威的词义消歧评测，目标是增进人们对词SenseEval，于1998年举办第一届。",语义评测,依靠,ACL-SIGLEX组织的国际权威的词义消歧评测
截至2017义与多义现象的理解。,义,依靠,多义现象的理解
该评测前期被称为年，已经成功举办了十一届。,评测前期,依靠,年
早期评测比较关注词义消歧问题，后来出现了更多文本语义理解的任务，包括语义角色标注、情感分析、跨语言语义分析等。,早期评测,依靠,词义消歧问题
早期评测比较关注词义消歧问题，后来出现了更多文本语义理解的任务，包括语义角色标注、情感分析、跨语言语义分析等。,早期评测,依靠,文本语义理解的任务
4.2面向非结构化数据的知识抽取大量的数据以非结构化数据（即自由文本）的形式存在，如新闻报道、科技文献和政府文件等，面向文本数据的知识抽取一直是广受关注的问题。,面向文本数据的知识抽取,中文名,面向非结构化数据的知识抽取
4.2面向非结构化数据的知识抽取大量的数据以非结构化数据（即自由文本）的形式存在，如新闻报道、科技文献和政府文件等，面向文本数据的知识抽取一直是广受关注的问题。,面向文本数据的知识抽取,依靠,非结构化数据的知识抽取
在前文介绍的知识抽取领域的评测竞赛中，评测数据大多属于非结构化文本数据。,知识抽取领域的评测竞赛,依靠,非结构化文本数据
本节将对这一类知识抽取技术和方法进行概要介绍，具体包括面向文本数据的实体抽取、关系抽取和事件抽取。,知识抽取,依靠,面向文本数据的实体抽取
4.2.1实体抽取实体抽取又称命名实体识别，其目的是从文本中抽取实体信息元素，包括人名、组织机构名、地理位置、时间、日期、字符值和金额值等。,实体抽取,中文名,从文本中抽取实体信息元素
4.2.1实体抽取实体抽取又称命名实体识别，其目的是从文本中抽取实体信息元素，包括人名、组织机构名、地理位置、时间、日期、字符值和金额值等。,实体抽取,依靠,从文本中抽取实体信息元素
实体抽取是解决很多自然语言处理问题的基础，也是知识抽取中最基本的任务。,实体抽取,依靠,知识抽取中最基本的任务
想要从文本中进行实体抽取，首先需要从文本中识别和定位实体，然后再将识别的实体分类到预定义的类别中去。,从文本中进行实体抽取,依靠,识别和定位实体
例如，给定一段新闻报道中的句子“北京时间10月25日，骑士后来居上，在主场以119∶112击退公牛”。,骑士,依靠,后来居上
例如，给定一段新闻报道中的句子“北京时间10月25日，骑士后来居上，在主场以119∶112击退公牛”。,公牛,依靠,击退
实体抽取旨在获取如图4-2所示的结果。,实体抽取,依靠,概念/产品
例句中的“北京”“10月25日”分别为地点和时间类型的实体，而“骑士”和“公牛”均为组织实体。,地点实体,依靠,地点类型的实体
例句中的“北京”“10月25日”分别为地点和时间类型的实体，而“骑士”和“公牛”均为组织实体。,时间实体,依靠,时间类型的实体
例句中的“北京”“10月25日”分别为地点和时间类型的实体，而“骑士”和“公牛”均为组织实体。,组织实体,依靠,组织实体
实体抽取问题的研究开展得比较早，该领域也积累了大量的方法。,实体抽取,依靠,领域也积累了大量的方法
总体上，可以将已有的方法分为基于规则的方法、基于统计模型的方法和基于深度学习的方法。,已有的方法,依靠,基于规则的方法
总体上，可以将已有的方法分为基于规则的方法、基于统计模型的方法和基于深度学习的方法。,已有的方法,依靠,基于统计模型的方法
总体上，可以将已有的方法分为基于规则的方法、基于统计模型的方法和基于深度学习的方法。,已有的方法,依靠,基于深度学习的方法
图4-2实体抽取举例1.基于规则的方法早期的命名实体识别方法主要采用人工编写规则的方式进行实体抽取。,命名实体识别方法,依靠,人工编写规则的方式
这类方法首先构建大量的实体抽取规则，一般由具有一定领域知识的专家手工构建。,实体抽取,依靠,具有一定领域知识的专家手工构建
然后，将规则与文本字符串进行匹配，识别命名实体。,规则,依靠,文本字符串进行匹配，识别命名实体。
这种实体抽取方式在小数据集上可以达到很高的准确率和召回率，但随着数据集的增大，规则集的构建周期变长，并且移植性较差。,实体抽取方式,依靠,规则集的构建周期
2.基于统计模型的方法基于统计模型的方法利用完全标注或部分标注的语料进行模型训练，主要采用的模型包括隐马尔可夫模型（Hidden_Markov_Model）、条件马尔可夫模型（Conditional_MarkovModel）、最大熵模型（Maximum_Entropy_Model）以及条件随机场模型（ConditionalRandom_Fields）。,基于统计模型的方法,中文名,基于统计模型的方法
2.基于统计模型的方法基于统计模型的方法利用完全标注或部分标注的语料进行模型训练，主要采用的模型包括隐马尔可夫模型（Hidden_Markov_Model）、条件马尔可夫模型（Conditional_MarkovModel）、最大熵模型（Maximum_Entropy_Model）以及条件随机场模型（ConditionalRandom_Fields）。,基于统计模型的方法 ,依靠,完全标注或部分标注的语料进行模型训练
该类方法将命名实体识别作为序列标注问题处理。,命名实体识别,依靠,序列标注问题处理
与普通的分类问题相比，序列标注问题中当前标签的预测不仅与当前的输入特征相关，还与之前的预测标签相关，即预测标签序列是有强相互依赖关系的。,序列标注问题中当前标签的预测,依靠,当前的输入特征
与普通的分类问题相比，序列标注问题中当前标签的预测不仅与当前的输入特征相关，还与之前的预测标签相关，即预测标签序列是有强相互依赖关系的。,序列标注问题中当前标签的预测,依靠,之前的预测标签相关
从自然文本中识别实体是一个典型的序列标注问题。,从自然文本中识别实体,依靠,序列标注问题
基于统计模型构建命名实体识别方法主要涉及训练语料标注、特征定义和模型训练三个方面。,命名实体识别,依靠,训练语料标注
基于统计模型构建命名实体识别方法主要涉及训练语料标注、特征定义和模型训练三个方面。,命名实体识别,依靠,特征定义
基于统计模型构建命名实体识别方法主要涉及训练语料标注、特征定义和模型训练三个方面。,命名实体识别,依靠,模型训练
（1）训练语料标注。,训练语料标注,依靠,概念/产品
为了构建统计模型的训练语料，人们一般采用Inside–Outside–Beginning（IOB）或Inside–Outside（IO）标注体系对文本进行人工标注。,Inside–Outside–Beginning（IOB）,依靠,对文本进行人工标注
在IOB标注体系中，文本中的每个词被标记为实体名称的起始词（B）、实体名称的后续词（I）或实体名称的外部词（D）。,B,中文名,实体名称的起始词
在IOB标注体系中，文本中的每个词被标记为实体名称的起始词（B）、实体名称的后续词（I）或实体名称的外部词（D）。,I,中文名,实体名称的后续词
在IOB标注体系中，文本中的每个词被标记为实体名称的起始词（B）、实体名称的后续词（I）或实体名称的外部词（D）。,D,中文名,实体名称的外部词
在IOB标注体系中，文本中的每个词被标记为实体名称的起始词（B）、实体名称的后续词（I）或实体名称的外部词（D）。,文本中的每个词,依靠,标记为实体名称的起始词（B）
在IOB标注体系中，文本中的每个词被标记为实体名称的起始词（B）、实体名称的后续词（I）或实体名称的外部词（D）。,文本中的每个词,依靠,标记为实体名称的后续词（I）
在IOB标注体系中，文本中的每个词被标记为实体名称的起始词（B）、实体名称的后续词（I）或实体名称的外部词（D）。,文本中的每个词,依靠,标记为实体名称的外部词（D）
而在IO标注体系中，文本中的词被标记为实体名称内部词（I）或实体名称外部词（D）。,实体名称内部词,中文名,实体名称内部词
而在IO标注体系中，文本中的词被标记为实体名称内部词（I）或实体名称外部词（D）。,实体名称外部词,中文名,实体名称外部词
而在IO标注体系中，文本中的词被标记为实体名称内部词（I）或实体名称外部词（D）。,实体名称内部词,依靠,标记为实体名称内部词（I）
而在IO标注体系中，文本中的词被标记为实体名称内部词（I）或实体名称外部词（D）。,实体名称外部词,依靠,标记为实体名称外部词（D）
表4-1以句子“苹果公司是一家美国的跨国公司”为例，给出了IOB和IO实体标注示例。,IOB,依靠,实体标注
表4-1IOB和IO实体标注示例（2）特征定义。,IOB,依靠,特征定义
在训练模型之前，统计模型需要计算每个词的一组特征作为模型的输入。,统计模型,依靠,计算每个词的一组特征
这些特征具体包括单词级别特征、词典特征和文档级特征等。,单词级别特征,依靠,分词
这些特征具体包括单词级别特征、词典特征和文档级特征等。,词典特征,依靠,向量空间模型
这些特征具体包括单词级别特征、词典特征和文档级特征等。,文档级特征,依靠,文本分类
单词级别特征包括是否首字母大写、是否以句点结尾、是否包含数字、词性、词的n-gram等。,单词级别特征,依靠,是否首字母大写
单词级别特征包括是否首字母大写、是否以句点结尾、是否包含数字、词性、词的n-gram等。,单词级别特征,依靠,是否以句点结尾
单词级别特征包括是否首字母大写、是否以句点结尾、是否包含数字、词性、词的n-gram等。,单词级别特征,依靠,是否包含数字
单词级别特征包括是否首字母大写、是否以句点结尾、是否包含数字、词性、词的n-gram等。,单词级别特征,依靠,词性
单词级别特征包括是否首字母大写、是否以句点结尾、是否包含数字、词性、词的n-gram等。,单词级别特征,依靠,词的n-gram
词典特征依赖外部词典定义，例如预定义的词表、地名列表等。,词典特征,依靠,外部词典定义
文档级特征基于整个语料文档集计算，例如文档集中的词频、同现词等。,文档级特征,依靠,整个语料文档集计算
斯坦福大学的NER[1]是一个被广泛使用的命名实体识别工具，具有较高的准确率。,斯坦福大学的NER,依靠,命名实体识别工具
Stanford_NER模型中定义的特征包括当前词、当前词的前一个词、当前词的后一个词、当前词的字符n-gram、当前词的词性、当前词上下文词性序列、当前词的词形、当前词上下文词形序列、当前词左侧窗口中的词（窗口大小为4）、当前词右侧窗口中的词（窗口大小为4）。,Stanford_NER模型中定义的特征,中文名,当前词、当前词的前一个词、当前词的后一个词、当前词的字符n-gram、当前词的词性、当前词上下文词性序列、当前词的词形、当前词上下文词形序列、当前词左侧窗口中的词（窗口大小为4）、当前词右侧窗口中的词（窗口大小为4）
Stanford_NER模型中定义的特征包括当前词、当前词的前一个词、当前词的后一个词、当前词的字符n-gram、当前词的词性、当前词上下文词性序列、当前词的词形、当前词上下文词形序列、当前词左侧窗口中的词（窗口大小为4）、当前词右侧窗口中的词（窗口大小为4）。,Stanford_NER模型中定义的特征,依靠,当前词、当前词的前一个词、当前词的后一个词、当前词的字符n-gram、当前词的词性、当前词上下文词性序列、当前词的词形、当前词上下文词形序列、当前词左侧窗口中的词（窗口大小为4）、当前词右侧窗口中的词（窗口大小为4）
定义何种特征对于命名实体识别结果有较大的影响，因此不同命名实体识别算法使用的特征有所不同。,命名实体识别算法,依靠,使用的特征
（3）模型训练。,模型训练,依靠,概念/产品
"隐马尔可夫模型（Hidden_Markov_Model,HMM）和条件随机场（Conditional_Random_Field,CRF）是两个常用于标注问题的统计学习模型，也被广泛应用于实体抽取问题。",隐马尔可夫模型,中文名,Hidden_Markov_Model
"隐马尔可夫模型（Hidden_Markov_Model,HMM）和条件随机场（Conditional_Random_Field,CRF）是两个常用于标注问题的统计学习模型，也被广泛应用于实体抽取问题。",条件随机场,中文名,Conditional_Random_Field
"隐马尔可夫模型（Hidden_Markov_Model,HMM）和条件随机场（Conditional_Random_Field,CRF）是两个常用于标注问题的统计学习模型，也被广泛应用于实体抽取问题。",隐马尔可夫模型,依靠,标注问题
"隐马尔可夫模型（Hidden_Markov_Model,HMM）和条件随机场（Conditional_Random_Field,CRF）是两个常用于标注问题的统计学习模型，也被广泛应用于实体抽取问题。",条件随机场,依靠,标注问题
HMM是一种有向图概率模型，模型中包含了隐藏的状态序列和可观察的观测序列。,HMM,依靠,有向图概率模型
每个状态代表了一个可观察的事件，观察到的事件是状态的随机函数。,状态,依靠,观察到的事件是状态的随机函数
HMM模型结构如图4-3所示，每个圆圈代表一个随机变量，随机变量xt是t时刻的隐藏状态；随机变量yt是t时刻的观测值，图中的箭头表示条件依赖关系。,HMM模型,中文名,隐马尔可夫模型
HMM模型结构如图4-3所示，每个圆圈代表一个随机变量，随机变量xt是t时刻的隐藏状态；随机变量yt是t时刻的观测值，图中的箭头表示条件依赖关系。,HMM模型,依靠,条件依赖关系
"HMM模型有两个基本假设：●在任意t时刻的状态只依赖于其前一时刻的状态，与其他观测及状态无关，即P（xt|xt−1,xt−2,...,x1,yt−1,yt−2,...,y1）=P（xt|xt−1）；●任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关，即P（yt|xt,xt−1,xt−2,...,x1,yt−1,yt−2,...,y1）=P（yt|xt）。",HMM模型,依靠,P（xt|xt−1）
"HMM模型有两个基本假设：●在任意t时刻的状态只依赖于其前一时刻的状态，与其他观测及状态无关，即P（xt|xt−1,xt−2,...,x1,yt−1,yt−2,...,y1）=P（xt|xt−1）；●任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关，即P（yt|xt,xt−1,xt−2,...,x1,yt−1,yt−2,...,y1）=P（yt|xt）。",HMM模型,依靠,P（yt|xt）
图4-3HMM模型结构在应用于命名实体识别问题时，HMM模型中的状态对应词的标记，标注问题可以看作是对给定的观测序列进行序列标注。,HMM模型结构,依靠,标注问题
"基于HMM的有代表性的命名实体识别方法可参考文献[2,3]。",基于HMM的有代表性的命名实体识别方法,依靠,参考文献
CRF是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。,CRF,依靠,给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型
在序列标注问题中，线性链CRF是常用的模型，其结构如图4-4所示。,线性链CRF,依靠,条件随机场
在序列标注问题中，状态序列变量x对应标记序列，y表示待标注的观测序列。,状态序列变量x,依靠,标记序列
在序列标注问题中，状态序列变量x对应标记序列，y表示待标注的观测序列。,观测序列,依靠,待标注的观测序列
图4-4线性链CRF模型结构给定训练数据集，模型可以通过极大似然估计得到条件概率模型；当标注新数据时，给定输入序列y，模型输出使条件概率P（x|y）最大化的x*。,线性链CRF模型结构,中文名,给定训练数据集，模型可以通过极大似然估计得到条件概率模型
图4-4线性链CRF模型结构给定训练数据集，模型可以通过极大似然估计得到条件概率模型；当标注新数据时，给定输入序列y，模型输出使条件概率P（x|y）最大化的x*。,模型,依靠,极大似然估计得到条件概率模型
图4-4线性链CRF模型结构给定训练数据集，模型可以通过极大似然估计得到条件概率模型；当标注新数据时，给定输入序列y，模型输出使条件概率P（x|y）最大化的x*。,模型,依靠,给定输入序列y，模型输出使条件概率P（x|y）最大化的x*
美国斯坦福大学开发的命名实体识别工具Stanford_NER是基于CRF的代表性系统[1]。,Stanford_NER,依靠,CRF
3.基于深度学习的方法随着深度学习方法在自然语言处理领域的广泛应用，深度神经网络也被成功应用于命名实体识别问题，并取得了很好的效果。,基于深度学习的方法,依靠,依靠深度神经网络
与传统统计模型相比，基于深度学习的方法直接以文本中词的向量为输入，通过神经网络实现端到端的命名实体识别，不再依赖人工定义的特征。,基于深度学习的方法,依靠,直接以文本中词的向量为输入
"目前，用于命名实体识别的神经网络主要有卷积神经网络（Convolutional_NeuralNetwork,CNN）、循环神经网络（Recurrent_Neural_Network,RNN）以及引入注意力机制（Attention_Mechanism）的神经网络。",命名实体识别的神经网络,中文名,卷积神经网络
"目前，用于命名实体识别的神经网络主要有卷积神经网络（Convolutional_NeuralNetwork,CNN）、循环神经网络（Recurrent_Neural_Network,RNN）以及引入注意力机制（Attention_Mechanism）的神经网络。",命名实体识别的神经网络,中文名,循环神经网络
"目前，用于命名实体识别的神经网络主要有卷积神经网络（Convolutional_NeuralNetwork,CNN）、循环神经网络（Recurrent_Neural_Network,RNN）以及引入注意力机制（Attention_Mechanism）的神经网络。",命名实体识别的神经网络,中文名,引入注意力机制的神经网络
"目前，用于命名实体识别的神经网络主要有卷积神经网络（Convolutional_NeuralNetwork,CNN）、循环神经网络（Recurrent_Neural_Network,RNN）以及引入注意力机制（Attention_Mechanism）的神经网络。",命名实体识别的神经网络,依靠,卷积神经网络
"目前，用于命名实体识别的神经网络主要有卷积神经网络（Convolutional_NeuralNetwork,CNN）、循环神经网络（Recurrent_Neural_Network,RNN）以及引入注意力机制（Attention_Mechanism）的神经网络。",命名实体识别的神经网络,依靠,循环神经网络
"目前，用于命名实体识别的神经网络主要有卷积神经网络（Convolutional_NeuralNetwork,CNN）、循环神经网络（Recurrent_Neural_Network,RNN）以及引入注意力机制（Attention_Mechanism）的神经网络。",命名实体识别的神经网络,依靠,引入注意力机制的神经网络
一般地，不同的神经网络结构在命名实体识别过程中扮演编码器的角色，它们基于初始输入以及词的上下文信息，得到每个词的新向量表示；最后再通过CRF模型输出对每个词的标注结果。,编码器,依靠,基于初始输入以及词的上下文信息，得到每个词的新向量表示
（1）LSTM-CRF模型。,LSTM-CRF模型,依靠,基于LSTM和CRF的组合
"该模型使用了长短时记忆神经网络（Long_Shot-Term_Memory_Neural_Network,LSTM）与CRF相结合进行命名实体识别。","长短时记忆神经网络（Long_Shot-Term_Memory_Neural_Network,LSTM）",依靠,命名实体识别
该模型自底向上分别是Embedding层、双向LSTM层和CRF层。,Embedding层,依靠,自底向上
Embedding层是句子中词的向量表示，作为双向LSTM的输入，通过词向量学习模型获得。,Embedding层,依靠,双向LSTM的输入
双向LSTM层通过一个正向LSTM和一个反向LSTM，分别计算每个词考虑左侧和右侧词时对应的向量，然后将每个词的两个向量进行连接，形成词的向量输出；最后，CRF层以双向LSTM输出的向量作为输入，对句子中的命名实体进行序列标注。,双向LSTM层,依靠,正向LSTM计算每个词考虑左侧词时对应的向量
双向LSTM层通过一个正向LSTM和一个反向LSTM，分别计算每个词考虑左侧和右侧词时对应的向量，然后将每个词的两个向量进行连接，形成词的向量输出；最后，CRF层以双向LSTM输出的向量作为输入，对句子中的命名实体进行序列标注。,双向LSTM层,依靠,反向LSTM计算每个词考虑右侧词时对应的向量
双向LSTM层通过一个正向LSTM和一个反向LSTM，分别计算每个词考虑左侧和右侧词时对应的向量，然后将每个词的两个向量进行连接，形成词的向量输出；最后，CRF层以双向LSTM输出的向量作为输入，对句子中的命名实体进行序列标注。,双向LSTM层,依靠,将每个词的两个向量进行连接形成词的向量输出
双向LSTM层通过一个正向LSTM和一个反向LSTM，分别计算每个词考虑左侧和右侧词时对应的向量，然后将每个词的两个向量进行连接，形成词的向量输出；最后，CRF层以双向LSTM输出的向量作为输入，对句子中的命名实体进行序列标注。,CRF层,依靠,双向LSTM层输出的向量
经过实验对比发现，双向LSTM与CRF组合的模型在英文测试数据上取得了与传统统计方法最好结果相近的结果，而传统方法中使用了大量的人工定义的特征以及外部资源；在德语测试数据上，深度学习模型取得了比统计学习方法更优的结果。,双向LSTM与CRF组合的模型,依靠,英文测试数据上取得了与传统统计方法最好结果相近的结果
经过实验对比发现，双向LSTM与CRF组合的模型在英文测试数据上取得了与传统统计方法最好结果相近的结果，而传统方法中使用了大量的人工定义的特征以及外部资源；在德语测试数据上，深度学习模型取得了比统计学习方法更优的结果。,深度学习模型,依靠,比统计学习方法更优的结果
图4-5LSTM-CRF命名实体识别模型[4]（2）LSTM-CNNs-CRF模型。,LSTM-CRF命名实体识别模型,依靠,LSTM
图4-5LSTM-CRF命名实体识别模型[4]（2）LSTM-CNNs-CRF模型。,LSTM-CNNs-CRF模型,依靠,LSTM-CRF命名实体识别模型
MA_Xuezhe等人发表于ACL2016的论文提出了将双向LSTM、CNN和CRF相结合的序列标注模型[5]，并成功地应用于命名实体识别问题中。,MA_Xuezhe,中文名,发表于ACL2016的论文
MA_Xuezhe等人发表于ACL2016的论文提出了将双向LSTM、CNN和CRF相结合的序列标注模型[5]，并成功地应用于命名实体识别问题中。,MA_Xuezhe,依靠,双向LSTM、CNN和CRF相结合的序列标注模型
该模型与LSTM-CRF模型十分相似，不同之处是在Embedding层中加入了每个词的字符级向量表示。,EMB,依靠,每个词的字符级向量表示
模型Embedding层中每个词的向量输入由预训练获得的词向量和CNN获得的字符级向量连接而成，通过双向LSTM和CRF层获得词的标注结果。,模型Embedding层中每个词的向量输入,依靠,预训练获得的词向量
LSTM-CNNs-CRF序列标注模型框架如图4-7所示。,LSTM-CNNs-CRF序列标注模型,依靠,依靠LSTM、CNNs和CRF
在CoNLL-2003命名实体识别数据集上，该模型获得了91.2%的F1值。,CoNLL-2003命名实体识别数据集,依靠,模型
图4-6获取词语字符级向量表示的CNN模型[5]图4-7LSTM-CNNs-CRF序列标注模型框架[5]（3）基于注意力机制的神经网络模型。,CNN模型,依靠,获取词语字符级向量表示
在自然语言处理领域，基于注意力机制的神经网络模型最早应用于解决机器翻译问题，注意力机制可以帮助扩展基本的编码器-解码器模型结构，让模型能够获取输入序列中与下一个目标词相关的信息。,基于注意力机制的神经网络模型,依靠,解决机器翻译问题
在命名实体识别问题方面，Marek_Rei等人在COLING2016的论文中提出了基于注意力机制的词向量和字符级向量组合方法[6]。,Marek_Rei,中文名,基于注意力机制的词向量和字符级向量组合方法
在命名实体识别问题方面，Marek_Rei等人在COLING2016的论文中提出了基于注意力机制的词向量和字符级向量组合方法[6]。,Marek_Rei,依靠,基于注意力机制的词向量和字符级向量组合方法
该方法认为除了将词作为句子基本元素学习得到的特征向量，命名实体识别还需要词中的字符级信息。,命名实体识别,依靠,将词作为句子基本元素学习得到的特征向量
因此，该方法除了使用双向LSTM得到词的特征向量，还基于双向LSTM计算词的字符级特征向量。,双向LSTM,依靠,词的字符级特征向量
假设输入词为“big”，该方法将词中的字符看作一个序列，然后通过正、反向的LSTM计算字符序列的最终状态和，两者相连得到词“big”的字符级向量h∗。,向量的字符级向量h∗,中文名,将词中的字符看作一个序列，然后通过正、反向的LSTM计算字符序列的最终状态和，两者相连得到词“big”的字符级向量h∗
假设输入词为“big”，该方法将词中的字符看作一个序列，然后通过正、反向的LSTM计算字符序列的最终状态和，两者相连得到词“big”的字符级向量h∗。,字符级向量h∗,依靠,正、反向的LSTM计算字符序列的最终状态和，两者相连
h∗通过一个非线性层得到m之后，与“big”的词向量x进行加权相加，而两者相加的权重z是通过一个两层的神经网络计算获得的。,h,中文名,通过一个非线性层得到m之后，与“big”的词向量x进行加权相加
h∗通过一个非线性层得到m之后，与“big”的词向量x进行加权相加，而两者相加的权重z是通过一个两层的神经网络计算获得的。,非线性层,依靠,一个两层的神经网络计算获得权重z
在得到句子中每个词的新向量之后，模型使用CRF对句子中的命名实体进行序列标注。,CRF,依靠,序列标注命名实体
注意力机制的引入使得模型可以动态地确定每个词的词向量和字符级向量在最终特征中的重要性，有效地提升了命名识别的效果。,注意力机制,依靠,模型可以动态地确定每个词的词向量和字符级向量在最终特征中的重要性
与基于词向量和字符级向量拼接的方法相比，基于注意力机制的方法在八个数据集上都获得了最好的实验结果。,基于注意力机制的方法,依靠,基于词向量和字符级向量拼接的方法
图4-8基于注意力机制的词向量和字符级向量组合方法[6]4.2.2关系抽取关系抽取是知识抽取的重要子任务之一，面向非结构化文本数据，关系抽取是从文本中抽取出两个或者多个实体之间的语义关系。,基于注意力机制的词向量和字符级向量组合方法,依靠,关系抽取
关系抽取与实体抽取密切相关，一般在识别出文本中的实体后，再抽取实体之间可能存在的关系。,关系抽取,依靠,实体抽取
目前，关系抽取方法可以分为基于模板的关系抽取方法、基于监督学习的关系抽取方法和基于弱监督学习的关系抽取方法。,基于模板的关系抽取方法,中文名,基于模板的关系抽取
目前，关系抽取方法可以分为基于模板的关系抽取方法、基于监督学习的关系抽取方法和基于弱监督学习的关系抽取方法。,关系抽取方法,依靠,基于模板的关系抽取方法
1.基于模板的关系抽取方法早期的实体关系抽取方法大多基于模板匹配实现。,早期的实体关系抽取方法,依靠,基于模板的匹配方法
该类方法基于语言学知识，结合语料的特点，由领域专家手工编写模板，从文本中匹配具有特定关系的实体。,文本中匹配具有特定关系的实体,依靠,领域专家手工编写模板
在小规模、限定领域的实体关系抽取问题上，基于模板的方法能够取得较好的效果。,基于模板的方法,依靠,在小规模、限定领域的实体关系抽取问题上取得较好的效果
假设想从文本中自动抽取具有“夫妻”关系的实体，并且观察到包含“夫妻”关系的例句。,抽取“夫妻”关系的实体,依靠,观察包含“夫妻”关系的例句
板：●例句1:[姚明]与妻子[叶莉]还有女儿姚沁蕾并排坐在景区的游览车上，画面十分温馨●例句2:[徐峥]老婆[陶虹]晒新写真可以简单地将上述句子中的实体替换为变量，从而得到如下能够获取“夫妻”关系的模●模板1:[X]与妻子[Y]……●模板2:[X]老婆[Y]……利用上述模板在文本中进行匹配，可以获得新的具有“夫妻”关系的实体。,板,依靠,概念/产品
为了进一步提高模板匹配的准确率，还可以将句法分析的结果加入模板中。,模板匹配,依靠,句法分析
基于模板的关系抽取方法的优点是模板构建简单，可以比较快地在小规模数据集上实现关系抽取系统。,基于模板的关系抽取方法的优点,中文名,模板构建简单，可以比较快地在小规模数据集上实现关系抽取系统。
基于模板的关系抽取方法的优点是模板构建简单，可以比较快地在小规模数据集上实现关系抽取系统。,基于模板的关系抽取方法的优点,依靠,比较快地在小规模数据集上实现关系抽取系统
但是，当数据规模较大时，手工构建模板需要耗费领域专家大量的时间。,构建模板,依靠,领域专家
此外，基于模板的关系抽取系统可移植性较差，当面临另一个领域的关系抽取问题时，需要重新构建模板。,基于模板的关系抽取系统,中文名,可移植性较差
此外，基于模板的关系抽取系统可移植性较差，当面临另一个领域的关系抽取问题时，需要重新构建模板。,基于模板的关系抽取系统,依靠,可移植性较差
最后，由于手工构建的模板数量有限，模板覆盖的范围不够，基于模板的关系抽取系统召回率普遍不高。,基于模板的关系抽取系统,依靠,手工构建的模板
2.基于监督学习的关系抽取方法基于监督学习的关系抽取方法将关系抽取转化为分类问题，在大量标注数据的基础上，训练有监督学习模型进行关系抽取。,基于监督学习的关系抽取方法,中文名,将关系抽取转化为分类问题
2.基于监督学习的关系抽取方法基于监督学习的关系抽取方法将关系抽取转化为分类问题，在大量标注数据的基础上，训练有监督学习模型进行关系抽取。,基于监督学习的关系抽取方法,依靠,将关系抽取转化为分类问题
利用监督学习方法进行关系抽取的一般步骤包括：预定义关系的类型；人工标注数据；设计关系识别所需的特征，一般根据实体所在句子的上下文计算获得；选择分类模型（如支持向量机、神经网络和朴素贝叶斯等），基于标注数据训练模型；对训练的模型进行评估。,利用监督学习方法进行关系抽取,中文名,预定义关系的类型
利用监督学习方法进行关系抽取的一般步骤包括：预定义关系的类型；人工标注数据；设计关系识别所需的特征，一般根据实体所在句子的上下文计算获得；选择分类模型（如支持向量机、神经网络和朴素贝叶斯等），基于标注数据训练模型；对训练的模型进行评估。,监督学习方法进行关系抽取,依靠,预定义关系的类型
利用监督学习方法进行关系抽取的一般步骤包括：预定义关系的类型；人工标注数据；设计关系识别所需的特征，一般根据实体所在句子的上下文计算获得；选择分类模型（如支持向量机、神经网络和朴素贝叶斯等），基于标注数据训练模型；对训练的模型进行评估。,监督学习方法进行关系抽取,依靠,人工标注数据
利用监督学习方法进行关系抽取的一般步骤包括：预定义关系的类型；人工标注数据；设计关系识别所需的特征，一般根据实体所在句子的上下文计算获得；选择分类模型（如支持向量机、神经网络和朴素贝叶斯等），基于标注数据训练模型；对训练的模型进行评估。,监督学习方法进行关系抽取,依靠,设计关系识别所需的特征
利用监督学习方法进行关系抽取的一般步骤包括：预定义关系的类型；人工标注数据；设计关系识别所需的特征，一般根据实体所在句子的上下文计算获得；选择分类模型（如支持向量机、神经网络和朴素贝叶斯等），基于标注数据训练模型；对训练的模型进行评估。,监督学习方法进行关系抽取,依靠,选择分类模型
利用监督学习方法进行关系抽取的一般步骤包括：预定义关系的类型；人工标注数据；设计关系识别所需的特征，一般根据实体所在句子的上下文计算获得；选择分类模型（如支持向量机、神经网络和朴素贝叶斯等），基于标注数据训练模型；对训练的模型进行评估。,监督学习方法进行关系抽取,依靠,对训练的模型进行评估
根据计算特征的复杂性，可以将常用的特征分为轻量级、中等量级和重量级三大类。,常用的特征,依靠,计算特征的复杂性
轻量级特征主要是基于实体和词的特征，例如句子中实体前后的词、实体的类型以及实体之间的距离等。,轻量级特征,依靠,实体和词的特征
中等量级特征主要是基于句子中语块序列的特征。,中等量级特征,依靠,句子中语块序列的特征
重量级特征一般包括实体间的依存关系路径、实体间依存树结构的距离以及其他特定的结构信息。,重量级特征,中文名,实体间的依存关系路径
重量级特征一般包括实体间的依存关系路径、实体间依存树结构的距离以及其他特定的结构信息。,重量级特征,依靠,实体间的依存关系路径
重量级特征一般包括实体间的依存关系路径、实体间依存树结构的距离以及其他特定的结构信息。,重量级特征,依靠,实体间依存树结构的距离
"例如，对于句子“Forward_[motion]_of_the_vehicle_through_the_air_caused_a_[suction]_on_thetube”，轻量级的特征可以是实体[motion]和[suction]、实体间的词road{of,the,vehicle,through,the,air,caused,a}等；重量级的特征可以包括依存树中的路径“caused→nsubj→实体1”“caused→dobj→实体2”等。",轻量级特征,中文名,实体[motion]和[suction]
"例如，对于句子“Forward_[motion]_of_the_vehicle_through_the_air_caused_a_[suction]_on_thetube”，轻量级的特征可以是实体[motion]和[suction]、实体间的词road{of,the,vehicle,through,the,air,caused,a}等；重量级的特征可以包括依存树中的路径“caused→nsubj→实体1”“caused→dobj→实体2”等。",重量级特征,中文名,依存树中的路径“caused→nsubj→实体1”“caused→dobj→实体2”等
"例如，对于句子“Forward_[motion]_of_the_vehicle_through_the_air_caused_a_[suction]_on_thetube”，轻量级的特征可以是实体[motion]和[suction]、实体间的词road{of,the,vehicle,through,the,air,caused,a}等；重量级的特征可以包括依存树中的路径“caused→nsubj→实体1”“caused→dobj→实体2”等。",轻量级的特征,依靠,实体[motion]和[suction]
"例如，对于句子“Forward_[motion]_of_the_vehicle_through_the_air_caused_a_[suction]_on_thetube”，轻量级的特征可以是实体[motion]和[suction]、实体间的词road{of,the,vehicle,through,the,air,caused,a}等；重量级的特征可以包括依存树中的路径“caused→nsubj→实体1”“caused→dobj→实体2”等。",重量级的特征,依靠,包括依存树中的路径“caused→nsubj→实体1”“caused→dobj→实体2”等。
draft传统的基于监督学习的关系抽取是一种依赖特征工程的方法，近年来有多个基于深度学习的关系抽取模型被研究者们提出。,基于监督学习的关系抽取,中文名,基于监督学习的关系抽取
draft传统的基于监督学习的关系抽取是一种依赖特征工程的方法，近年来有多个基于深度学习的关系抽取模型被研究者们提出。,基于监督学习的关系抽取,依靠,依赖特征工程的方法
深度学习的方法不需要人工构建各种特征，其输入一般只包括句子中的词及其位置的向量表示。,深度学习的方法,依靠,输入只包括句子中的词及其位置的向量表示
目前，已有的基于深度学习的关系抽取方法主要包括流水线方法和联合抽取方法两大类。,基于深度学习的关系抽取方法,依靠,流水线方法
目前，已有的基于深度学习的关系抽取方法主要包括流水线方法和联合抽取方法两大类。,基于深度学习的关系抽取方法,依靠,联合抽取方法
流水线方法将识别实体和关系抽取作为两个分离的过程进行处理，两者不会相互影响；关系抽取在实体抽取结果的基础上进行，因此关系抽取的结果也依赖于实体抽取的结果。,关系抽取,中文名,在实体抽取结果的基础上进行
流水线方法将识别实体和关系抽取作为两个分离的过程进行处理，两者不会相互影响；关系抽取在实体抽取结果的基础上进行，因此关系抽取的结果也依赖于实体抽取的结果。,关系抽取,依靠,实体抽取的结果
联合抽取方法将实体抽取和关系抽取相结合，在统一的模型中共同优化；联合抽取方法可以避免流水线方法存在的错误积累问题。,联合抽取方法,依靠,实体抽取和关系抽取相结合
（1）基于深度学习的流水线关系抽取方法●CR-CNN模型[7]。,基于深度学习的流水线关系抽取方法,依靠,CR-CNN模型[7]
给定输入的句子，CR-CNN模型首先将句子中的词映射到长度为dw的低维向量，每个词的向量包含了词向量和位置向量两部分。,CR-CNN模型,依靠,将句子中的词映射到长度为dw的低维向量
然后，模型对固定大小滑动窗口中的词的向量进行卷积操作，为每个窗口生成新的长度为dc的特征向量；对所有的窗口特征向量求最大值，模型最终得到整个句子的向量表示dx。,模型,中文名,对固定大小滑动窗口中的词的向量进行卷积操作
然后，模型对固定大小滑动窗口中的词的向量进行卷积操作，为每个窗口生成新的长度为dc的特征向量；对所有的窗口特征向量求最大值，模型最终得到整个句子的向量表示dx。,模型,中文名,为每个窗口生成新的长度为dc的特征向量
然后，模型对固定大小滑动窗口中的词的向量进行卷积操作，为每个窗口生成新的长度为dc的特征向量；对所有的窗口特征向量求最大值，模型最终得到整个句子的向量表示dx。,模型,中文名,对所有的窗口特征向量求最大值
然后，模型对固定大小滑动窗口中的词的向量进行卷积操作，为每个窗口生成新的长度为dc的特征向量；对所有的窗口特征向量求最大值，模型最终得到整个句子的向量表示dx。,模型,中文名,得到整个句子的向量表示dx
然后，模型对固定大小滑动窗口中的词的向量进行卷积操作，为每个窗口生成新的长度为dc的特征向量；对所有的窗口特征向量求最大值，模型最终得到整个句子的向量表示dx。,模型的向量表示dx,依靠,为每个窗口生成新的长度为dc的特征向量
然后，模型对固定大小滑动窗口中的词的向量进行卷积操作，为每个窗口生成新的长度为dc的特征向量；对所有的窗口特征向量求最大值，模型最终得到整个句子的向量表示dx。,模型,依靠,对固定大小滑动窗口中的词的向量进行卷积操作
在进行关系分类时，CR-CNN模型计算句子向量和每个关系类型向量的点积，得到实体具有每种预定义关系的分值。,CR-CNN模型,中文名,计算句子向量和每个关系类型向量的点积
在进行关系分类时，CR-CNN模型计算句子向量和每个关系类型向量的点积，得到实体具有每种预定义关系的分值。,CR-CNN模型,依靠,计算句子向量和每个关系类型向量的点积
CR-CNN模型在SemEval-2010_Task_8数据集上获得了84.1%的F1值，这个结果优于当时最好的非深度学习方法。,CR-CNN模型,依靠,非深度学习方法获得较好的F1值
图4-9CR-CNN模型[7]●Attention_CNNs模型[8]。,CR-CNN模型,依靠,Attention_CNNs模型[8]
Wang等人提出的多层注意力卷积神经网络（Multi-levelAttention_CNN），将注意力机制引入到神经网络中，对反映实体关系更重要的词语赋予更大的权重，借助改进后的目标函数提高关系提取的效果。,多层注意力卷积神经网络,中文名,Multi-levelAttention_CNN
Wang等人提出的多层注意力卷积神经网络（Multi-levelAttention_CNN），将注意力机制引入到神经网络中，对反映实体关系更重要的词语赋予更大的权重，借助改进后的目标函数提高关系提取的效果。,多层注意力卷积神经网络,依靠,注意力机制
其模型的结构如图4-10所示，在输入层，模型引入了词与实体相关的注意力，同时还在池化和混合层引入了针对目标关系类别的注意力。,模型的输入层,依靠,词与实体相关的注意力
其模型的结构如图4-10所示，在输入层，模型引入了词与实体相关的注意力，同时还在池化和混合层引入了针对目标关系类别的注意力。,模型的输入层,依靠,针对目标关系类别的注意力
在SemEval-2010_Task_8数据集上，该模型获得了88%的F1值。,SemEval-2010_Task_8数据集上的文本分类模型,依靠,获得较高的F1值
●Attention_BLSTM模型[9]。,Attention_BLSTM模型,依靠,依靠注意力机制和双向长短期记忆网络
Attention_BLSTM模型如图4-11所示，它包含两个LSTM网络，从正向和反向处理输入的句子，从而得到每个词考虑左边和右边序列背景的状态向量；词的两个状态向量通过元素级求和产生词的向量表示。,Attention_BLSTM模型,依靠,两个LSTM网络
在双向LSTM产生的词向量基础上，该模型通过注意力层组合词的向量产生句子向量，进而基于句子向量将关系分类。,句子向量的产生,依靠,双向LSTM产生的词向量
注意力层首先计算每个状态向量的权重，然后计算所有状态向量的加权和得到句子的向量表示。,注意力层,依靠,计算每个状态向量的权重
实验结果表明，增加注意力层可以有效地提升关系分类的结果。,增加注意力层,依靠,提升关系分类的结果
图4-10Attention_CNNs模型的结构[8]图4-11Attention_BLSTM模型[9]在关系抽取问题方面，还有许多其他属于流水线方法的深度学习模型。,Attention_CNNs模型,依靠,图4-10
在流水线关系抽取方法中，实体抽取和关系抽取两个过程是分离的。,实体抽取,依靠,关系抽取
联合关系抽取方法则是将实体抽取和关系抽取相结合，图4-13展示的是一个实体抽取和关系抽取的联合模型[13]。,联合关系抽取方法,依靠,实体抽取和关系抽取相结合
该模型主要由三个表示层组成：词嵌入层（嵌入层）、基于单词序列的LSTM-RNN层（序列层）以及基于依赖性子树的LSTM-RNN层（依存关系层）。,嵌入层,中文名,词嵌入层
该模型主要由三个表示层组成：词嵌入层（嵌入层）、基于单词序列的LSTM-RNN层（序列层）以及基于依赖性子树的LSTM-RNN层（依存关系层）。,序列层,中文名,基于单词序列的LSTM-RNN层
该模型主要由三个表示层组成：词嵌入层（嵌入层）、基于单词序列的LSTM-RNN层（序列层）以及基于依赖性子树的LSTM-RNN层（依存关系层）。,依存关系层,中文名,基于依赖性子树的LSTM-RNN层
该模型主要由三个表示层组成：词嵌入层（嵌入层）、基于单词序列的LSTM-RNN层（序列层）以及基于依赖性子树的LSTM-RNN层（依存关系层）。,嵌入层,依靠,词嵌入层
该模型主要由三个表示层组成：词嵌入层（嵌入层）、基于单词序列的LSTM-RNN层（序列层）以及基于依赖性子树的LSTM-RNN层（依存关系层）。,序列层,依靠,基于单词序列的LSTM-RNN层
该模型主要由三个表示层组成：词嵌入层（嵌入层）、基于单词序列的LSTM-RNN层（序列层）以及基于依赖性子树的LSTM-RNN层（依存关系层）。,依存关系层,依靠,基于依赖性子树的LSTM-RNN层
在解码过程中，模型在序列层上构建从左到右的实体识别，并实现依存关系层上的关系分类，其中每个基于子树的LSTM-RNN对应于两个被识别实体之间的候选关系。,模型,中文名,从左到右的实体识别
在解码过程中，模型在序列层上构建从左到右的实体识别，并实现依存关系层上的关系分类，其中每个基于子树的LSTM-RNN对应于两个被识别实体之间的候选关系。,模型,中文名,依存关系层上的关系分类
在解码过程中，模型在序列层上构建从左到右的实体识别，并实现依存关系层上的关系分类，其中每个基于子树的LSTM-RNN对应于两个被识别实体之间的候选关系。,模型的实体识别,依靠,依存关系层上的关系分类
在对整个模型结构进行解码之后，模型参数通过基于时间的反向传播进行更新。,模型参数的更新,依靠,基于时间的反向传播
在依存关系层堆叠在序列层上，因此嵌入层和序列层被实体识别和关系分类任务共享，共享参数受实体和关系标签的共同影响。,嵌入层,依靠,实体识别和关系分类任务
该联合模型在SemEval-2010_Task8数据集上获得了84.4%的F1值；将WordNet作为外部知识后，该模型可以获得85.6%的F1值。,联合模型,中文名,在SemEval-2010_Task8数据集上获得F1值
该联合模型在SemEval-2010_Task8数据集上获得了84.4%的F1值；将WordNet作为外部知识后，该模型可以获得85.6%的F1值。,WordNet,中文名,外部知识后模型可以获得F1值
该联合模型在SemEval-2010_Task8数据集上获得了84.4%的F1值；将WordNet作为外部知识后，该模型可以获得85.6%的F1值。,联合模型,依靠,WordNet
当训练语料不足时，弱监督学习方法可以只利用少量的标注数据进行模型学习。,弱监督学习方法,依靠,少量的标注数据模型学习
基于弱监督学习的关系抽取方法主要包括远程监督方法和Bootstrapping方法。,基于弱监督学习的关系抽取方法,依靠,远程监督方法
基于弱监督学习的关系抽取方法主要包括远程监督方法和Bootstrapping方法。,基于弱监督学习的关系抽取方法,依靠,Bootstrapping方法
（1）远程监督方法。,远程监督方法,依靠,人工智能
远程监督方法通过将知识图谱与非结构化文本对齐的方式自动构建大量的训练数据，减少模型对人工标注数据的依赖，增强模型的跨领域适应能力。,远程监督方法,依靠,非结构化文本对齐的方式自动构建大量的训练数据，减少模型对人工标注数据的依赖，增强模型的跨领域适应能力
远程监督方法的基本假设是如果两个实体在知识图谱中存在某种关系，则包含两个实体的句子均表达了这种关系。,远程监督方法,依靠,如果两个实体在知识图谱中存在某种关系
例如，在某知识图谱中存在实体关系创始人（乔布斯，苹果公司），则包含实体乔布斯和苹果公司的句子“乔布斯是苹果公司的联合创始人和CEO”则可被用作关系创始人的训练正例。,实体关系创始人,中文名,乔布斯，苹果公司
例如，在某知识图谱中存在实体关系创始人（乔布斯，苹果公司），则包含实体乔布斯和苹果公司的句子“乔布斯是苹果公司的联合创始人和CEO”则可被用作关系创始人的训练正例。,实体关系创始人,依靠,实体乔布斯
因此，远程监督关系抽取方法的一般步骤为：●从知识图谱中抽取存在目标关系的实体对；●从非结构化文本中抽取含有实体对的句子作为训练样例；●训练监督学习模型进行关系抽取。,远程监督关系抽取方法,中文名,从知识图谱中抽取存在目标关系的实体对
因此，远程监督关系抽取方法的一般步骤为：●从知识图谱中抽取存在目标关系的实体对；●从非结构化文本中抽取含有实体对的句子作为训练样例；●训练监督学习模型进行关系抽取。,远程监督关系抽取方法,中文名,从非结构化文本中抽取含有实体对的句子作为训练样例
因此，远程监督关系抽取方法的一般步骤为：●从知识图谱中抽取存在目标关系的实体对；●从非结构化文本中抽取含有实体对的句子作为训练样例；●训练监督学习模型进行关系抽取。,远程监督关系抽取方法,中文名,训练监督学习模型进行关系抽取
因此，远程监督关系抽取方法的一般步骤为：●从知识图谱中抽取存在目标关系的实体对；●从非结构化文本中抽取含有实体对的句子作为训练样例；●训练监督学习模型进行关系抽取。,远程监督关系抽取方法,依靠,从知识图谱中抽取存在目标关系的实体对
因此，远程监督关系抽取方法的一般步骤为：●从知识图谱中抽取存在目标关系的实体对；●从非结构化文本中抽取含有实体对的句子作为训练样例；●训练监督学习模型进行关系抽取。,远程监督关系抽取方法,依靠,从非结构化文本中抽取含有实体对的句子作为训练样例
因此，远程监督关系抽取方法的一般步骤为：●从知识图谱中抽取存在目标关系的实体对；●从非结构化文本中抽取含有实体对的句子作为训练样例；●训练监督学习模型进行关系抽取。,远程监督关系抽取方法,依靠,训练监督学习模型进行关系抽取
远程监督关系抽取方法可以利用丰富的知识图谱信息获取训练数据，有效地减少了人工标注的工作量。,远程监督关系抽取方法,依靠,丰富的知识图谱信息获取训练数据
但是，基于远程监督的假设，大量噪声会被引入到训练数据中，从而引发语义漂移的现象。,语义漂移,依靠,远程监督的假设
为了改进远程监督实体关系抽取方法，一些研究围绕如何克服训练数据中的噪声问题展开。,改进远程监督实体关系抽取方法,依靠,克服训练数据中的噪声问题
最近，多示例学习、采用注意力机制的深度学习模型以及强化学习等模型被用来解决样例错误标注的问题，取得了较好的效果。,多示例学习,依靠,解决样例错误标注的问题
下面介绍两个具有代表性的模型。,概念/产品,依靠,本体
Guoliang_Ji等人在发表于AAAI2017的论文中提出了基于句子级注意力和实体描述的神经网络关系抽取模型APCNNs[14]。,APCNNs,中文名,基于句子级注意力和实体描述的神经网络关系抽取模型
Guoliang_Ji等人在发表于AAAI2017的论文中提出了基于句子级注意力和实体描述的神经网络关系抽取模型APCNNs[14]。,APCNNs,依靠,句子级注意力
Guoliang_Ji等人在发表于AAAI2017的论文中提出了基于句子级注意力和实体描述的神经网络关系抽取模型APCNNs[14]。,APCNNs,依靠,实体描述的神经网络关系抽取模型
模型结构如图4-14所示，图4-14（a）是PCNNs（Piecewise_Convolutional_Neural_Networks）模型，用于提取单一句子的特征向量；其输入是词向量和位置向量，通过卷积和池化操作，得到句子的向量表示。,PCNNs,依靠,卷积和池化操作
关系的分类是基于包特征上的Softmax分类器实现的。,关系的分类,依靠,基于包特征上的Softmax分类器
APCNNs模型实际采用了多示例学习的策略，将同一关系的样例句子组成样例包，关系分类是基于样例包的特征进行的。,APCNNs模型,依靠,多示例学习的策略
实验结果表明，该模型可以有效地提高远程监督关系抽取的准确率。,远程监督关系抽取,依靠,模型
图4-14APCNNs模型[14]在采用多示例学习策略时，有可能出现整个样例包都包含大量噪声的情况。,APCNNs模型,依靠,多示例学习策略
针对这一问题，Jun_Feng等人提出了基于强化学习的关系分类模型CNN-RL[15]。,CNN-RL,依靠,基于强化学习的关系分类模型
CNN-RL模型框架如图4-15所示，模型有两个重要模块：样例选择器和关系分类器。,样例选择器,依靠,关系分类器
样例选择器负责从样例包中选择高质量的句子，然后由关系分类器从句子级特征对关系进行分类。,样例选择器,依靠,从样例包中选择高质量的句子
样例选择器负责从样例包中选择高质量的句子，然后由关系分类器从句子级特征对关系进行分类。,关系分类器,依靠,从句子级特征对关系进行分类
整个模型采用强化学习的方式，样例选择器基于一个随机策略，在考虑当前句子的选择状态情况下选择样例句子；关系分类器利用卷积神经网络对句子中的实体关系进行分类，并向样例选择器反馈，帮助其改进样例选择策略。,样例选择器,中文名,基于一个随机策略在考虑当前句子的选择状态情况下选择样例句子
整个模型采用强化学习的方式，样例选择器基于一个随机策略，在考虑当前句子的选择状态情况下选择样例句子；关系分类器利用卷积神经网络对句子中的实体关系进行分类，并向样例选择器反馈，帮助其改进样例选择策略。,样例选择器,依靠,一个随机策略
整个模型采用强化学习的方式，样例选择器基于一个随机策略，在考虑当前句子的选择状态情况下选择样例句子；关系分类器利用卷积神经网络对句子中的实体关系进行分类，并向样例选择器反馈，帮助其改进样例选择策略。,关系分类器,依靠,卷积神经网络
在实验对比中，该模型获得了比句子级卷积神经网络和样例包级关系分类模型更好的结果。,句子级卷积神经网络,依靠,概念/产品
在实验对比中，该模型获得了比句子级卷积神经网络和样例包级关系分类模型更好的结果。,样例包级关系分类模型,依靠,概念/产品
图4-15CNN-RL模型[15]（2）Bootstrapping方法。,CNN-RL模型,依靠,Bootstrapping方法
Bootstrapping方法利用少量的实例作为初始种子集合，然后在种子集合上学习获得关系抽取的模板，再利用模板抽取更多的实例，加入种子集合中。,Bootstrapping方法,依靠,少量的实例作为初始种子集合
通过不断地迭代，Bootstrapping方法可以从文本中抽取关系的大量实例。,Bootstrapping,依靠,从文本中抽取关系的大量实例
有很多实体关系抽取系统都采用了Bootstrapping方法。,实体关系抽取系统,依靠,Bootstrapping方法
Brin等人[16]构建的DIPER利用少量实体对作为种子，从Web上大量非结构化文本中抽取新的实例，同时学习新的抽取模板，迭代地获取实体关系，是较早使用Bootstrapping方法的系统。,DIPER,中文名,自底向上抽取实体及其关系
Brin等人[16]构建的DIPER利用少量实体对作为种子，从Web上大量非结构化文本中抽取新的实例，同时学习新的抽取模板，迭代地获取实体关系，是较早使用Bootstrapping方法的系统。,DIPER,依靠,Bootstrapping方法
Agichtein等人[17]设计实现了Snowball关系抽取系统，该系统在DIPER系统基础上提出了模板生成和关系抽取的新策略。,Agichtein等人,依靠,Snowball关系抽取系统
在关系抽取过程中，Snowball可以自动评价新实例的可信度，并保留最可靠的实例加入种子集合。,Snowball,依靠,评价新实例的可信度
在关系抽取过程中，Snowball可以自动评价新实例的可信度，并保留最可靠的实例加入种子集合。,Snowball,依靠,保留最可靠的实例加入种子集合
Etzioni等人[18]构建了KnowItAll抽取系统，从Web文本中抽取非特定领域的事实信息，该系统关系抽取的准确率能达到90%。,KnowItAll,依靠,从Web文本中抽取非特定领域的事实信息
此后，一些基于Bootstrapping的系统加入了更合理的模板描述、限制条件和评分策略，进一步提高了关系抽取的准确率。,基于Bootstrapping的系统,依靠,更合理的模板描述、限制条件和评分策略
例如NELL系统[19]，它以初始本体和少量种子作为输入，从大规模的Web文本中学习，并对学习到的内容进行打分来提升系统性能。,NELL系统,依靠,从大规模的Web文本中学习，并对学习到的内容进行打分来提升系统性能
Bootstrapping方法的优点是关系抽取系统构建成本低，适合大规模的关系抽取任务，并且具备发现新关系的能力。,Bootstrapping方法的优点,中文名,构建成本低
Bootstrapping方法的优点是关系抽取系统构建成本低，适合大规模的关系抽取任务，并且具备发现新关系的能力。,Bootstrapping方法的优点,中文名,适合大规模的关系抽取任务
Bootstrapping方法的优点是关系抽取系统构建成本低，适合大规模的关系抽取任务，并且具备发现新关系的能力。,Bootstrapping方法的优点,中文名,发现新关系的能力
Bootstrapping方法的优点是关系抽取系统构建成本低，适合大规模的关系抽取任务，并且具备发现新关系的能力。,Bootstrapping方法的优点,依靠,构建成本低
Bootstrapping方法的优点是关系抽取系统构建成本低，适合大规模的关系抽取任务，并且具备发现新关系的能力。,Bootstrapping方法的优点,依靠,适合大规模的关系抽取任务
Bootstrapping方法的优点是关系抽取系统构建成本低，适合大规模的关系抽取任务，并且具备发现新关系的能力。,Bootstrapping方法的优点,依靠,发现新关系的能力
但是，Bootstrapping方法也存在不足之处，包括对初始种子较为敏感、存在语义漂移问题、结果准确率较低等。,Bootstrapping方法的不足之处,依靠,对初始种子较为敏感
但是，Bootstrapping方法也存在不足之处，包括对初始种子较为敏感、存在语义漂移问题、结果准确率较低等。,Bootstrapping方法的不足之处,依靠,存在语义漂移问题
但是，Bootstrapping方法也存在不足之处，包括对初始种子较为敏感、存在语义漂移问题、结果准确率较低等。,Bootstrapping方法的不足之处,依靠,结果准确率较低
4.2.3事件抽取事件是指发生的事情，通常具有时间、地点、参与者等属性。,事件,中文名,发生的事情
4.2.3事件抽取事件是指发生的事情，通常具有时间、地点、参与者等属性。,事件,依靠,时间、地点、参与者等属性
事件的发生可能是因为一个动作的产生或者系统状态的改变。,事件的发生,依靠,一个动作的产生
事件的发生可能是因为一个动作的产生或者系统状态的改变。,事件的发生,依靠,系统状态的改变
事件抽取是指从自然语言文本中抽取出用户感兴趣的事件信息，并以结构化的形式呈现出来，例如事件发生的时间、地点、发生原因、参与者等。,事件抽取,中文名,从自然语言文本中抽取出用户感兴趣的事件信息，并以结构化的形式呈现出来
事件抽取是指从自然语言文本中抽取出用户感兴趣的事件信息，并以结构化的形式呈现出来，例如事件发生的时间、地点、发生原因、参与者等。,事件抽取,依靠,从自然语言文本中抽取出用户感兴趣的事件信息，并以结构化的形式呈现出来
基于一段苹果公司举办产品发布会的新闻报道，可以通过事件抽取方法自动获取报道事件的结构化信息，包括事件类型、涉及公司、发生时间及地点、所发布的产品。,事件抽取,依靠,基于一段苹果公司举办产品发布会的新闻报道
一般地，事件抽取任务包含的子任务有：●识别事件触发词及事件类型；●抽取事件元素的同时判断其角色；●抽出描述事件的词组或句子；●事件属性标注；●事件共指消解。,事件抽取,中文名,识别事件触发词及事件类型；抽取事件元素的同时判断其角色；抽出描述事件的词组或句子；事件属性标注；事件共指消解
一般地，事件抽取任务包含的子任务有：●识别事件触发词及事件类型；●抽取事件元素的同时判断其角色；●抽出描述事件的词组或句子；●事件属性标注；●事件共指消解。,事件抽取任务的子任务,依靠,识别事件触发词及事件类型
一般地，事件抽取任务包含的子任务有：●识别事件触发词及事件类型；●抽取事件元素的同时判断其角色；●抽出描述事件的词组或句子；●事件属性标注；●事件共指消解。,事件抽取任务的子任务,依靠,抽取事件元素的同时判断其角色
一般地，事件抽取任务包含的子任务有：●识别事件触发词及事件类型；●抽取事件元素的同时判断其角色；●抽出描述事件的词组或句子；●事件属性标注；●事件共指消解。,事件抽取任务的子任务,依靠,抽出描述事件的词组或句子
一般地，事件抽取任务包含的子任务有：●识别事件触发词及事件类型；●抽取事件元素的同时判断其角色；●抽出描述事件的词组或句子；●事件属性标注；●事件共指消解。,事件抽取任务的子任务,依靠,事件属性标注
一般地，事件抽取任务包含的子任务有：●识别事件触发词及事件类型；●抽取事件元素的同时判断其角色；●抽出描述事件的词组或句子；●事件属性标注；●事件共指消解。,事件抽取任务的子任务,依靠,事件共指消解
图4-16事件抽取示例已有的事件抽取方法可以分为流水线方法和联合抽取方法两大类。,已有的事件抽取方法,依靠,流水线方法
图4-16事件抽取示例已有的事件抽取方法可以分为流水线方法和联合抽取方法两大类。,已有的事件抽取方法,依靠,联合抽取方法
1.事件抽取的流水线方法流水线方法将事件抽取任务分解为一系列基于分类的子任务，包括事件识别、元素抽取、属性分类和可报告性判别；每一个子任务由一个机器学习分类器负责实施。,事件抽取的流水线方法,中文名,事件识别、元素抽取、属性分类和可报告性判别
1.事件抽取的流水线方法流水线方法将事件抽取任务分解为一系列基于分类的子任务，包括事件识别、元素抽取、属性分类和可报告性判别；每一个子任务由一个机器学习分类器负责实施。,事件抽取的流水线方法,依靠,事件识别
1.事件抽取的流水线方法流水线方法将事件抽取任务分解为一系列基于分类的子任务，包括事件识别、元素抽取、属性分类和可报告性判别；每一个子任务由一个机器学习分类器负责实施。,事件抽取的流水线方法,依靠,元素抽取
1.事件抽取的流水线方法流水线方法将事件抽取任务分解为一系列基于分类的子任务，包括事件识别、元素抽取、属性分类和可报告性判别；每一个子任务由一个机器学习分类器负责实施。,事件抽取的流水线方法,依靠,属性分类
1.事件抽取的流水线方法流水线方法将事件抽取任务分解为一系列基于分类的子任务，包括事件识别、元素抽取、属性分类和可报告性判别；每一个子任务由一个机器学习分类器负责实施。,事件抽取的流水线方法,依靠,可报告性判别
一个基本的事件抽取流水线需要的分类器包括：（1）事件触发词分类器。,事件触发词分类器,依靠,一个基本的事件抽取流水线
判断词汇是否为事件触发词，并基于触发词信息对事件类别进行分类。,判断词汇是否为事件触发词,依靠,基于触发词信息对事件类别进行分类。
（2）元素分类器。,元素分类器,依靠,概念/产品
判断词组是否为事件的元素。,判断词组是否为事件的元素,依靠,概念/产品
（3）元素角色分类器。,元素角色分类器,依靠,概念/产品
判定事件元素的角色类别。,判定事件元素的角色类别,依靠,判定事件元素的角色
（4）属性分类器。,属性分类器,依靠,概念/产品
判定事件的属性。,判定事件的属性,依靠,抽取事件三元组
（5）可报告性分类器。,可报告性分类器,依靠,分类
判定是否存在值得报告的事件实例。,值得报告的事件实例的判定,依靠,判定是否存在值得报告的事件实例。
表4-2列出了在事件抽取过程中，触发词分类和元素分类常用的分类特征。,触发词分类常用的分类特征,依靠,分类特征
各个阶段的分类器可以采用机器学习算法中的不同分类器，例如最大熵模型、支持向量机等。,各个阶段的分类器,依靠,机器学习算法
表4-2触发词分类和元素分类常用的分类特征2.事件的联合抽取方法事件抽取的流水线方法在每个子任务阶段都有可能存在误差，这种误差会从前面的环节逐步传播到后面的环节，从而导致误差不断累积，使得事件抽取的性能急剧衰减。,事件的联合抽取方法,中文名,事件抽取的流水线方法
表4-2触发词分类和元素分类常用的分类特征2.事件的联合抽取方法事件抽取的流水线方法在每个子任务阶段都有可能存在误差，这种误差会从前面的环节逐步传播到后面的环节，从而导致误差不断累积，使得事件抽取的性能急剧衰减。,事件的联合抽取方法,依靠,在每个子任务阶段都有可能存在误差
为了解决这一问题，一些研究工作提出了事件的联合抽取方法。,事件的联合抽取方法,依靠,为了解决这一问题
在联合抽取方法中，事件的所有相关信息会通过一个模型同时抽取出来。,联合抽取方法,依靠,事件的所有相关信息通过一个模型同时抽取出来
一般地，联合事件抽取方法可以采用联合推断或联合建模的方法，如图4-17所示。,联合事件抽取方法,依靠,联合推断
一般地，联合事件抽取方法可以采用联合推断或联合建模的方法，如图4-17所示。,联合事件抽取方法,依靠,联合建模
联合推断方法首先建立事件抽取子任务的模型，然后将各个模型的目标函数进行组合，形成联合推断的目标函数；通过对联合目标函数进行优化，获得事件抽取各个子任务的结果。,联合推断方法,中文名,建立事件抽取子任务的模型，然后将各个模型的目标函数进行组合，形成联合推断的目标函数；通过对联合目标函数进行优化，获得事件抽取各个子任务的结果。
联合推断方法首先建立事件抽取子任务的模型，然后将各个模型的目标函数进行组合，形成联合推断的目标函数；通过对联合目标函数进行优化，获得事件抽取各个子任务的结果。,联合推断方法,依靠,建立事件抽取子任务的模型
联合推断方法首先建立事件抽取子任务的模型，然后将各个模型的目标函数进行组合，形成联合推断的目标函数；通过对联合目标函数进行优化，获得事件抽取各个子任务的结果。,联合推断方法,依靠,将各个模型的目标函数进行组合
联合建模的方法在充分分析子任务间的关系后，基于概率图模型进行联合建模，获得事件抽取的总体结果。,联合建模,依靠,充分分析子任务间的关系
具有代表性的联合建模方法是Qi_Li等人在ACL2013论文中提出的联合事件抽取模型[20]。,联合建模方法,依靠,Qi_Li等人
该模型将事件触发词、元素抽取的局部特征和捕获任务之间关联的结构特征结合进行事件抽取。,事件抽取,依靠,事件触发词、元素抽取的局部特征和捕获任务之间关联的结构特征
在图4-18所示的事件触发词和事件元素示例中，“fired”是袭击（Attack）事件的触发词，但是由于该词本身具有歧义性，流水线方法中的局部分类器很容易将其错误分类；但是，如果考虑到“tank”很可能是袭击事件的工具（Instrument）元素，那么就比较容易判断“fired”触发的是袭击事件。,“fired”,中文名,触发
在图4-18所示的事件触发词和事件元素示例中，“fired”是袭击（Attack）事件的触发词，但是由于该词本身具有歧义性，流水线方法中的局部分类器很容易将其错误分类；但是，如果考虑到“tank”很可能是袭击事件的工具（Instrument）元素，那么就比较容易判断“fired”触发的是袭击事件。,“fired”,依靠,袭击（Attack）事件的触发词
在图4-18所示的事件触发词和事件元素示例中，“fired”是袭击（Attack）事件的触发词，但是由于该词本身具有歧义性，流水线方法中的局部分类器很容易将其错误分类；但是，如果考虑到“tank”很可能是袭击事件的工具（Instrument）元素，那么就比较容易判断“fired”触发的是袭击事件。,“tank”,依靠,袭击事件的工具（Instrument）元素
此外，在流水线方法中，局部的分类器也不能捕获“fired”和“died”之间的依赖关系。,局部的分类器,依靠,捕获“fired”和“died”之间的依赖关系
为了克服局部分类器的不足，新的联合抽取模型在使用大量局部特征的基础上，增加了若干全局特征。,联合抽取模型,依靠,大量局部特征
为了克服局部分类器的不足，新的联合抽取模型在使用大量局部特征的基础上，增加了若干全局特征。,联合抽取模型,依靠,若干全局特征
例如，在图4-18的句子中，事件死亡（Die）和事件（Attack）的提及“died”和“fired”共享了三个参数；基于这种情况，可以定义形如图4-19所示的事件抽取全局特征。,事件抽取全局特征,中文名,基于共享参数定义
例如，在图4-18的句子中，事件死亡（Die）和事件（Attack）的提及“died”和“fired”共享了三个参数；基于这种情况，可以定义形如图4-19所示的事件抽取全局特征。,事件死亡（Die）和事件（Attack）的提及“died”和“fired”,依靠,形如图4-19所示的事件抽取全局特征
这类全局特征可以从整体的结构中学习得到，从而使用全局的信息来提升局部的预测。,全局特征,依靠,整体的结构
联合抽取模型将事件抽取问题转换成结构预测问题，并使用集束搜索方法进行求解。,联合抽取模型,依靠,集束搜索方法进行求解
传统的事件抽取方法通常需要借助外部的自然语言处理工具和大量的人工设计的特征；与之相比，深度学习方法具有以下优势：●减少了对外部工具的依赖，甚至不依赖外部工具，可以构建端到端的系统；●使用词向量作为输入，词向量蕴涵了丰富的语义信息；●神经网络具有自动提取句子特征的能力，避免了人工设计特征的烦琐工作。,深度学习方法的优势,中文名,减少对外部工具的依赖甚至不依赖外部工具构建端到端的系统
传统的事件抽取方法通常需要借助外部的自然语言处理工具和大量的人工设计的特征；与之相比，深度学习方法具有以下优势：●减少了对外部工具的依赖，甚至不依赖外部工具，可以构建端到端的系统；●使用词向量作为输入，词向量蕴涵了丰富的语义信息；●神经网络具有自动提取句子特征的能力，避免了人工设计特征的烦琐工作。,深度学习方法的优势,中文名,使用词向量作为输入词向量蕴涵了丰富的语义信息
传统的事件抽取方法通常需要借助外部的自然语言处理工具和大量的人工设计的特征；与之相比，深度学习方法具有以下优势：●减少了对外部工具的依赖，甚至不依赖外部工具，可以构建端到端的系统；●使用词向量作为输入，词向量蕴涵了丰富的语义信息；●神经网络具有自动提取句子特征的能力，避免了人工设计特征的烦琐工作。,深度学习方法的优势,中文名,自动提取句子特征的能力避免了人工设计特征的烦琐工作
传统的事件抽取方法通常需要借助外部的自然语言处理工具和大量的人工设计的特征；与之相比，深度学习方法具有以下优势：●减少了对外部工具的依赖，甚至不依赖外部工具，可以构建端到端的系统；●使用词向量作为输入，词向量蕴涵了丰富的语义信息；●神经网络具有自动提取句子特征的能力，避免了人工设计特征的烦琐工作。,深度学习方法的优点,依靠,减少对外部工具的依赖甚至不依赖外部工具构建端到端的系统
传统的事件抽取方法通常需要借助外部的自然语言处理工具和大量的人工设计的特征；与之相比，深度学习方法具有以下优势：●减少了对外部工具的依赖，甚至不依赖外部工具，可以构建端到端的系统；●使用词向量作为输入，词向量蕴涵了丰富的语义信息；●神经网络具有自动提取句子特征的能力，避免了人工设计特征的烦琐工作。,深度学习方法的优点,依靠,使用词向量作为输入
传统的事件抽取方法通常需要借助外部的自然语言处理工具和大量的人工设计的特征；与之相比，深度学习方法具有以下优势：●减少了对外部工具的依赖，甚至不依赖外部工具，可以构建端到端的系统；●使用词向量作为输入，词向量蕴涵了丰富的语义信息；●神经网络具有自动提取句子特征的能力，避免了人工设计特征的烦琐工作。,深度学习方法的优点,依靠,自动提取句子特征的能力避免了人工设计特征的烦琐工作
图4-20展示了一个基于动态多池化卷积神经网络的事件抽取模型。,基于动态多池化卷积神经网络的事件抽取模型,依靠,基于动态多池化卷积神经网络的事件抽取
该模型由YuboChen等人于2015年发表在ACL会议上[21]。,该模型,依靠,YuboChen等人
模型总体包含词向量学习、词汇级特征抽取、句子级特征抽取和分类器输出四个部分。,模型总体,依靠,词向量学习
其中，词向量学习通过无监督方式学习词的向量表示；词汇级特征抽取基于词的向量表示获取事件抽取相关的词汇线索；句子级特征抽取通过动态多池化卷积神经网络获取句子的语义组合特征；分类器输出产生事件元素的角色类别。,词向量学习,中文名,无监督方式学习词的向量表示
其中，词向量学习通过无监督方式学习词的向量表示；词汇级特征抽取基于词的向量表示获取事件抽取相关的词汇线索；句子级特征抽取通过动态多池化卷积神经网络获取句子的语义组合特征；分类器输出产生事件元素的角色类别。,词汇级特征抽取,中文名,基于词的向量表示获取事件抽取相关的词汇线索
其中，词向量学习通过无监督方式学习词的向量表示；词汇级特征抽取基于词的向量表示获取事件抽取相关的词汇线索；句子级特征抽取通过动态多池化卷积神经网络获取句子的语义组合特征；分类器输出产生事件元素的角色类别。,句子级特征抽取,中文名,动态多池化卷积神经网络获取句子的语义组合特征
其中，词向量学习通过无监督方式学习词的向量表示；词汇级特征抽取基于词的向量表示获取事件抽取相关的词汇线索；句子级特征抽取通过动态多池化卷积神经网络获取句子的语义组合特征；分类器输出产生事件元素的角色类别。,分类器输出,中文名,产生事件元素的角色类别
其中，词向量学习通过无监督方式学习词的向量表示；词汇级特征抽取基于词的向量表示获取事件抽取相关的词汇线索；句子级特征抽取通过动态多池化卷积神经网络获取句子的语义组合特征；分类器输出产生事件元素的角色类别。,词汇级特征抽取,依靠,无监督方式学习词的向量表示
其中，词向量学习通过无监督方式学习词的向量表示；词汇级特征抽取基于词的向量表示获取事件抽取相关的词汇线索；句子级特征抽取通过动态多池化卷积神经网络获取句子的语义组合特征；分类器输出产生事件元素的角色类别。,句子级特征抽取,依靠,动态多池化卷积神经网络
在CNN方法的结果。,SPO,依靠,抽取三元组
ACE2005英文数据集上的实验表明，该模型获得了优于传统方法和其他图4-20基于动态多池化卷积神经网络的事件抽取模型4.3面向结构化数据的知识抽取垂直领域的知识往往来源于支撑企业业务系统的关系数据库，因此，从数据库这种结构化数据中抽取知识也是一类重要的知识抽取方法。,面向结构化数据的知识抽取,依靠,从数据库这种结构化数据中抽取知识
在该领域，已经有一些标准和工具支持将数据库数据转化为RDF数据、OWL本体等。,标准,依靠,将数据库数据转化为RDF数据、OWL本体
W3C的RDB2RDF工作组于2012年发布了两个推荐的RDB2RDF映射语言：DM（Direct_Mapping，直接映射）和R2RML。,R2RML,中文名,直接映射
W3C的RDB2RDF工作组于2012年发布了两个推荐的RDB2RDF映射语言：DM（Direct_Mapping，直接映射）和R2RML。,R2RML,依靠,W3C的RDB2RDF工作组
DM和R2ML映射语言用于定义关系数据库中的数据如何转换为RDF数据的各种规则，具体包括URI的生成、RDF类和属性的定义、空节点的处理、数据间关联关系的表达等。,DM和R2ML映射语言,中文名,DM和R2ML映射语言
DM和R2ML映射语言用于定义关系数据库中的数据如何转换为RDF数据的各种规则，具体包括URI的生成、RDF类和属性的定义、空节点的处理、数据间关联关系的表达等。,DM和R2ML映射语言,依靠,规则
4.3.1直接映射直接映射规范定义了一个从关系数据库到RDF图数据的简单转换，为定义和比较更复杂的转换提供了基础。,直接映射,依靠,规范
它也可用于实现RDF图或定义虚拟图，可以通过SPARQL查询或通过RDF图API访问。,SPARQL查询,依靠,实现RDF图或定义虚拟图
它也可用于实现RDF图或定义虚拟图，可以通过SPARQL查询或通过RDF图API访问。,RDF图API,依靠,实现RDF图或定义虚拟图
直接映射将关系数据库表结构和数据直接转换为RDF图，关系数据库的数据结构直接反映在RDF图中。,直接映射,依靠,关系数据库表结构和数据直接转换为RDF图
直接映射的基本规则包括：●数据库中的表映射为RDF类；●数据库中表的列映射为RDF属性；●数据库表中每一行映射为一个资源或实体，创建IRI；●数据库表中每个单元格的值映射为一个文字值（Literal_Value）；如果单元格的值对应一个外键，则将其替换为外键值指向的资源或实体的IRI。,直接映射的基本规则,中文名,数据库表中每个单元格的值映射为一个文字值（Literal_Value）
直接映射的基本规则包括：●数据库中的表映射为RDF类；●数据库中表的列映射为RDF属性；●数据库表中每一行映射为一个资源或实体，创建IRI；●数据库表中每个单元格的值映射为一个文字值（Literal_Value）；如果单元格的值对应一个外键，则将其替换为外键值指向的资源或实体的IRI。,直接映射的基本规则,依靠,数据库表的行映射为一个资源或实体，创建IRI
下面给出一个简单的例子，解释直接映射的基本思路。,直接映射,依靠,概念/产品依靠映射实现概念/产品之间的映射
首先，假设通过SQL语句创建图4-21中的两个数据库表。,创建图4-21中的两个数据库表,依靠,SQL语句
"创建数据库表的SQL语句如下：基于直接映射标准，上述两个表可以输出如下的RDF数据：图4-21数据库表在直接映射过程中，数据库表中的每一行（例如People表中的<7,“Bob”,18>）产生了一组具有共同主语（subject）的三元组。",基于直接映射标准,中文名,创建数据库表的SQL语句
"创建数据库表的SQL语句如下：基于直接映射标准，上述两个表可以输出如下的RDF数据：图4-21数据库表在直接映射过程中，数据库表中的每一行（例如People表中的<7,“Bob”,18>）产生了一组具有共同主语（subject）的三元组。",数据库表的SQL语句,依靠,基于直接映射标准
主语是由IRI前缀和表名（People）、主键列名（ID）、主键值（7）串联而成的IRI。,主语,中文名,IRI前缀和表名（People）、主键列名（ID）、主键值（7）串联而成的IRI
主语是由IRI前缀和表名（People）、主键列名（ID）、主键值（7）串联而成的IRI。,主语,依靠,IRI前缀和表名（People）、主键列名（ID）、主键值（7）
每列的谓词是由IRI前缀和表名、列名连接形成的IRI。,谓词,依靠,IRI前缀和表名、列名连接形成
这些值是从列值的词汇形式形成的RDF文字。,列值的词汇形式形成的RDF文字,依靠,形成
每个外键都会生成一个三元组，其谓词由外键列名、引用表和引用的列名组成。,外键,依靠,外键列名、引用表和引用的列名组成
这些三元组的宾语是被引用三元组的行标识符（例如<Addresses/ID=18>）。,三元组的宾语,依靠,被引用三元组的行标识符（例如<Addresses/ID=18>）。
直接映射不会为NULL值生成三元组。,直接映射,依靠,为NULL值生成三元组
4.3.2R2RMLR2RML映射语言是一种用于表示从关系数据库到RDF数据集的自定义映射的语言。,R2RML映射语言,依靠,用于表示从关系数据库到RDF数据集的自定义映射的语言
这种映射提供了在RDF数据模型下查看现有关系型数据的能力，并且可以基于用户自定义的结构和目标词汇表示原有的关系型数据。,映射,依靠,在RDF数据模型下查看现有关系型数据的能力
在数据库的直接映射中，生成的RDF图的结构直接反映了数据库的结构，目标RDF词汇直接反映数据库模式元素的名称，结构和目标词汇都不能改变。,RDF图的结构,依靠,数据库的结构
在数据库的直接映射中，生成的RDF图的结构直接反映了数据库的结构，目标RDF词汇直接反映数据库模式元素的名称，结构和目标词汇都不能改变。,目标RDF词汇,依靠,数据库模式元素的名称
然而，通过使用R2RML，用户可以在关系数据上灵活定制视图。,R2RML,依靠,在关系数据上灵活定制视图
每个R2RML映射都针对特定的数据库模式和目标词汇量身定制。,R2RML映射,依靠,特定的数据库模式和目标词汇量身定制
R2RML映射的输入是符合该模式的关系数据库，输出是采用目标词汇表中谓词和类型描述的RDF数据集。,R2RML映射的输入,依靠,符合该模式的关系数据库
R2RML映射的输入是符合该模式的关系数据库，输出是采用目标词汇表中谓词和类型描述的RDF数据集。,R2RML映射的输出,依靠,采用目标词汇表中谓词和类型描述的RDF数据集
R2RML映射是通过逻辑表（Logic_Tables）从数据库中检索数据的。,R2RML映射,依靠,逻辑表（Logic_Tables）从数据库中检索数据
一个逻辑表可以是数据库中的一个表、视图或有效的SQL语句查询。,一个逻辑表,依靠,数据库中的一个表
一个逻辑表可以是数据库中的一个表、视图或有效的SQL语句查询。,一个逻辑表,依靠,视图
一个逻辑表可以是数据库中的一个表、视图或有效的SQL语句查询。,一个逻辑表,依靠,有效的SQL语句查询
每个逻辑表通过三元组映射（Triples_Map）映射至RDF数据，而三元组映射是可以将逻辑表中每一行映射为若干RDF三元组的规则。,三元组映射,依靠,将逻辑表中每一行映射为若干RDF三元组的规则
“逻辑表”突破了关系数据库表的物理结构的限制，为不改变数据库原有的结构而灵活地按需生成RDF数据奠定了基础。,“逻辑表”,依靠,不改变数据库原有的结构而灵活地按需生成RDF数据
三元组映射的规则主要包括两个部分，一个主语映射和多个谓词-宾语映射。,三元组映射的规则,依靠,主语映射
主语映射从逻辑表生成所有RDF三元组中的主语，通常使用基于数据库表中的主键生成的IRI表示。,主语映射,依靠,基于数据库表中的主键生成IRI表示
谓词-宾语映射则包含了谓词映射和宾语映射，其过程与主语映射相似。,谓词-宾语映射,依靠,谓词映射和宾语映射
图4-22中给出了一个示例数据库，其包含两个表，分别是雇用表和部门表。,雇用表,依靠,部门表
用于DEPT表数据转换的R2RML映射文档如下所示。,R2RML映射文档,依靠,用于DEPT表数据转换
此外，为了生成谓词ex:department的三元组，需要将EMP和DEPT表进行连接，可以通过定义下面的映射实现。,EMP表的映射,依靠,将EMP和DEPT表进行连接
"4.3.3相关工具目前，有许多工具支持以访问知识图谱的形式直接访问关系数据库，可以直接使用语句查询数据库中的信息；这类工具也常被称为基于本体的数据库访问SPARQL（Ontology_Based_Database_Access,OBDA）系统。",SPARQL,中文名,基于本体的数据库访问
"4.3.3相关工具目前，有许多工具支持以访问知识图谱的形式直接访问关系数据库，可以直接使用语句查询数据库中的信息；这类工具也常被称为基于本体的数据库访问SPARQL（Ontology_Based_Database_Access,OBDA）系统。",SPARQL,依靠,基于本体的数据库访问
这里介绍几种重要的OBDA系统，表4-3列出了这些系统的主要特性。,OBDA系统,依靠,表4-3列出了这些系统的主要特性
表4-3OBDA系统的主要特性对比（1）D2RQ[22]。,D2RQ,依靠,OBDA系统的主要特性对比
D2RQ是较早开发和发布的OBDA系统，它可以将关系数据库以RDF形式发布，其平台框架如图4-23所示。,D2RQ,依靠,将关系数据库以RDF形式发布
其中，D2R_Server是一个HTTP_Server，主要功能提供对RDF数据的查询访问接口，供上层的RDF浏览器、SPARQL查询客户端以及HTML浏览器调用。,D2R_Server,中文名,一个HTTP_Server
其中，D2R_Server是一个HTTP_Server，主要功能提供对RDF数据的查询访问接口，供上层的RDF浏览器、SPARQL查询客户端以及HTML浏览器调用。,D2R_Server,依靠,提供对RDF数据的查询访问接口
D2R_Server使用了一种可定制的D2RQ映射文件将关系数据库内容映射为RDF格式，与本章前面介绍的映射语言十分相似。,D2R_Server,依靠,可定制的D2RQ映射文件
基于D2RQ映射，Web端的请求被重写为SQL查询，这种即时转换允许从大型实时数据库发布RDF，而无须将数据复制到专用的RDF三元组存储中。,基于D2RQ映射,中文名,Web端的请求被重写为SQL查询
基于D2RQ映射，Web端的请求被重写为SQL查询，这种即时转换允许从大型实时数据库发布RDF，而无须将数据复制到专用的RDF三元组存储中。,基于D2RQ映射,依靠,重写为SQL查询
此外，D2RQ系统还部分支持R2RML映射。,D2RQ系统,依靠,部分支持R2RML映射
（2）Mastro[23]。,Mastro,依靠,SPO三元组
Mastro是一个基于Java语言开发的OBDA系统，系统中的本体使用属于DL-Lite轻量级描述逻辑系列的语言定义，通过数据库和本体元素之间的映射，用户可以通过SPARQL查询数据库。,Mastro,中文名,基于Java语言开发的OBDA系统
Mastro是一个基于Java语言开发的OBDA系统，系统中的本体使用属于DL-Lite轻量级描述逻辑系列的语言定义，通过数据库和本体元素之间的映射，用户可以通过SPARQL查询数据库。,Mastro,依靠,通过数据库和本体元素之间的映射，用户可以通过SPARQL查询数据库
Mastro数据源管理器支持与最流行的商业和非商业DBMS的交互。,Mastro数据源管理器,依靠,与最流行的商业和非商业DBMS的交互
除此之外，还为Oracle、DB2、SQLServer、MySQL和PostgreSQL提供支持。,SPO,依靠,为Oracle、DB2、SQLServer、MySQL和PostgreSQL提供支持
图4-23D2RQ平台框架[22]图4-24Mastro系统结构[23]（3）Ultrawrap[24]。,D2RQ平台,依靠,本体查询
Ultrawrap是一个商业化系统，其系统结构如图4-25所示，主要包含编译器和服务器两部分。,Ultrawrap,依靠,编译器
Ultrawrap是一个商业化系统，其系统结构如图4-25所示，主要包含编译器和服务器两部分。,Ultrawrap,依靠,服务器
其中，编译器负责建立数据库到RDF和OWL的映射；服务器负责在数据库上执行SPARQL查询。,编译器,依靠,建立数据库到RDF和OWL的映射
其中，编译器负责建立数据库到RDF和OWL的映射；服务器负责在数据库上执行SPARQL查询。,服务器,依靠,在数据库上执行SPARQL查询
Ultrawrap在执行SPARQL查询时可以获得与SQL语句查询相同的速度，它支持R2RML和D2RQ映射，并为用户提供图形界面个性化定制映射。,Ultrawrap,依靠,R2RML和D2RQ映射
图4-25Ultrawrap系统结构（4）Morph-RDB[25]。,Morph-RDB,依靠,Ultrawrap
Morph-RDB是由马德里理工大学本体工程组开发的RDB2RDF引擎，遵循R2RML规范。,Morph-RDB,依靠,马德里理工大学本体工程组
Morph-RDB支持两种操作模式：数据升级（从关系数据库中的数据生成RDF实例）和查询转换（SPARQL到SQL）。,Morph-RDB,依靠,数据升级
Morph-RDB采用各种优化技术来生成高效的SQL查询，例如自连接消除和子查询消除。,Morph-RDB,依靠,自连接消除
Morph-RDB采用各种优化技术来生成高效的SQL查询，例如自连接消除和子查询消除。,Morph-RDB,依靠,子查询消除
目前，Morph-RDB支持MySQL、PostgreSQL、H2、CSV文件和MonetDB等数据源。,Morph-RDB,依靠,MySQL
目前，Morph-RDB支持MySQL、PostgreSQL、H2、CSV文件和MonetDB等数据源。,Morph-RDB,依靠,PostgreSQL
目前，Morph-RDB支持MySQL、PostgreSQL、H2、CSV文件和MonetDB等数据源。,Morph-RDB,依靠,H2
目前，Morph-RDB支持MySQL、PostgreSQL、H2、CSV文件和MonetDB等数据源。,Morph-RDB,依靠,CSV文件
目前，Morph-RDB支持MySQL、PostgreSQL、H2、CSV文件和MonetDB等数据源。,Morph-RDB,依靠,MonetDB
（5）Ontop[26]。,Ontop,依靠,知识问答系统
Ontop是一个将关系数据库作为虚拟的RDF图进行SPARQL查询的工具。,Ontop,依靠,将关系数据库作为虚拟的RDF图进行SPARQL查询
Ontop由Bozen-Bolzano自由大学开发，是基于Apache许可证的开源工具。,Ontop,依靠,Bozen-Bolzano自由大学开发
通过将本体中的词汇（类和属性）通过映射链接到数据源，Ontop系统将关系数据库转换为虚拟的RDF图。,Ontop,依靠,将本体中的词汇通过映射链接到数据源
Ontop支持R2RML映射，它可以将SPARQL查询翻译为关系数据库中的SQL查询，从而实现在数据库上的SPARQL查询。,Ontop,依靠,R2RML映射
自万维网出现以来，半结构化数据越来越丰富，全文文档和数据库不再是唯一的数据形式，因此半结构化数据也成为知识获取的重要来源。,半结构化数据,依靠,知识获取
目前，百科类数据、网页数据是可被用于知识获取的重要半结构化数据，本节将介绍面向此类数据的知识抽取方法。,面向此类数据的知识抽取方法,依靠,百科类数据
目前，百科类数据、网页数据是可被用于知识获取的重要半结构化数据，本节将介绍面向此类数据的知识抽取方法。,面向此类数据的知识抽取方法,依靠,网页数据
4.4.1面向百科类数据的知识抽取以维基百科为代表的百科类数据是典型的半结构化数据。,面向百科类数据的知识抽取,依靠,维基百科为代表的百科类数据
图4-27维基百科词条页面结构因为词条包含丰富的半结构化数据，并且其中的信息具有较高的准确度，维基百科已经成为构建大规模知识图谱的重要数据来源。,维基百科,依靠,构建大规模知识图谱
目前，基于维基百科已经构建起多个知识图谱，包括DBpedia[27]和Yago[28]等。,知识图谱,依靠,维基百科
随着中文百科站点的发展，如百度百科、互动百科，一些大规模的中文知识图谱也陆续基于百科数据被构建出来，包括Zhishi.me[29]、XLore[30]和CN-DBpedia[31]等。,中文知识图谱,依靠,构建中文知识图谱
在基于百科数据构建知识图谱的过程中，关键问题是如何准确地从百科数据中抽取结构化语义信息。,从百科数据中抽取结构化语义信息,依靠,基于百科数据构建知识图谱
在基于百科数据构建的知识图谱中，DBpedia是较早发布、具有代表性的知识图谱，下面对它的构建方法进行介绍。,DBpedia,依靠,概念/产品
DBpedia是一个大规模的多语言百科知识图谱，是维基百科的机构化版本。,DBpedia,依靠,维基百科的机构化版本
DBpedia采用固定模式对维基百科中的实体信息进行抽取，在Linking_Open_Data原则的指导下，将其以关联数据的形式在Web上发布与共享。,DBpedia,依靠,固定模式对维基百科中的实体信息进行抽取
得益于维基百科的数据规模，DBpedia是目前最大的跨领域知识图谱之一。,DBpedia,依靠,维基百科的数据规模
截至2019年2月，DBpedia英文版描述了458万个实体，其中有422万个实体被准确地在一个本体中进行分类，其中包括144.5万个人物、73.5万个地点、41.1万件作品、24.1万个组织、25.1万个物种和6000种疾病。,DBpedia英文版,依靠,在一个本体中进行分类
此外，DBpedia还提供DBpedia数据集包含超过了125种语言的本地化版本，共包含了3830万个事物。,DBpedia,依靠,DBpedia数据集
完整的3800万个来自125种不同语言的标签和摘要，2520万个图片链接和2980万个外部网页链接。,三元组,依靠,完整的3800万个来自125种不同语言的标签和摘要，2520万个图片链接和2980万个外部网页链接。
DBpedia通过大约5000万个RDF链接与其他链接数据集连接，使其成为LOD数据集的重要核心。,DBpedia,依靠,其他链接数据集
总体上，DBpedia包含约30亿条RDF三元组，其中5.8亿条是从维基百科的英文版中提取的，24.6亿条是从其他语言版本中提取的。,DBpedia,依靠,从维基百科的英文版中提取的RDF三元组
总体上，DBpedia包含约30亿条RDF三元组，其中5.8亿条是从维基百科的英文版中提取的，24.6亿条是从其他语言版本中提取的。,DBpedia,依靠,从其他语言版本中提取的RDF三元组
根据抽样评测，DBpedia中RDF三元组的正确率达88%。,DBpedia中RDF三元组的正确率,依靠,抽样评测
框架的主要组成部分是：页面集合，包含本地及远程的维基百科文章数据；目标数据，存储或序列化提取的RDF三元组；将特定类型的维基标记转换为三元组的提取器；支持提取器的解析器，其作用是确定数据类型，在不同单元之间转换值并将标记分解成列表；提取作业，负责将页面集合、提取器和目标数据分组到一个工作流程中；知识提取管理器，负责管理将维基百科文章传递给提取器并将其输出传递到目标数据的过程。,框架的组成部分,依靠,提取作业
框架的主要组成部分是：页面集合，包含本地及远程的维基百科文章数据；目标数据，存储或序列化提取的RDF三元组；将特定类型的维基标记转换为三元组的提取器；支持提取器的解析器，其作用是确定数据类型，在不同单元之间转换值并将标记分解成列表；提取作业，负责将页面集合、提取器和目标数据分组到一个工作流程中；知识提取管理器，负责管理将维基百科文章传递给提取器并将其输出传递到目标数据的过程。,框架的组成部分,依靠,知识提取管理器
图4-28DBpedia知识抽取的总体框架[27]DBpedia使用了多种知识提取器从维基百科中获取结构化数据，具体包括：●标签（Labels）：抽取维基百科词条的标题，并将其定义为实体的标签；●摘要（Abstracts）：抽取维基百科词条页面的第一段文字，将其定义为实体的短摘要；抽取词条目录前最长500字的长摘要。,DBpedia,依靠,标签抽取维基百科词条的标题
●跨语言链接（Inter-language_Links）：抽取词条页面指向其他语言版本的跨语言链接；联；●图片（Images）：提取指向图片的链接；●重定向（Redirects）：抽取维基百科词条的重定向链接，建立其与同义词条的关●消歧（Disambiguation）：从维基百科消歧页面抽取有歧义的词条链接；●外部链接（External_Links）：抽取词条正文指向维基百科外部的链接；●页面链接（Pagelinks）：抽取词条正文指向维基百科内部的链接；●主页（Homepages）：抽取诸如公司、机构等实体的主页链接；●分类（Categories）：抽取词条所属的分类；●地理坐标（Geo-Coordinates）：抽取词条页面中存在的地理位置的经纬度坐标。,抽取链接,依靠,跨语言链接
●跨语言链接（Inter-language_Links）：抽取词条页面指向其他语言版本的跨语言链接；联；●图片（Images）：提取指向图片的链接；●重定向（Redirects）：抽取维基百科词条的重定向链接，建立其与同义词条的关●消歧（Disambiguation）：从维基百科消歧页面抽取有歧义的词条链接；●外部链接（External_Links）：抽取词条正文指向维基百科外部的链接；●页面链接（Pagelinks）：抽取词条正文指向维基百科内部的链接；●主页（Homepages）：抽取诸如公司、机构等实体的主页链接；●分类（Categories）：抽取词条所属的分类；●地理坐标（Geo-Coordinates）：抽取词条页面中存在的地理位置的经纬度坐标。,抽取链接,依靠,图片
●跨语言链接（Inter-language_Links）：抽取词条页面指向其他语言版本的跨语言链接；联；●图片（Images）：提取指向图片的链接；●重定向（Redirects）：抽取维基百科词条的重定向链接，建立其与同义词条的关●消歧（Disambiguation）：从维基百科消歧页面抽取有歧义的词条链接；●外部链接（External_Links）：抽取词条正文指向维基百科外部的链接；●页面链接（Pagelinks）：抽取词条正文指向维基百科内部的链接；●主页（Homepages）：抽取诸如公司、机构等实体的主页链接；●分类（Categories）：抽取词条所属的分类；●地理坐标（Geo-Coordinates）：抽取词条页面中存在的地理位置的经纬度坐标。,抽取链接,依靠,重定向
●跨语言链接（Inter-language_Links）：抽取词条页面指向其他语言版本的跨语言链接；联；●图片（Images）：提取指向图片的链接；●重定向（Redirects）：抽取维基百科词条的重定向链接，建立其与同义词条的关●消歧（Disambiguation）：从维基百科消歧页面抽取有歧义的词条链接；●外部链接（External_Links）：抽取词条正文指向维基百科外部的链接；●页面链接（Pagelinks）：抽取词条正文指向维基百科内部的链接；●主页（Homepages）：抽取诸如公司、机构等实体的主页链接；●分类（Categories）：抽取词条所属的分类；●地理坐标（Geo-Coordinates）：抽取词条页面中存在的地理位置的经纬度坐标。,抽取链接,依靠,消歧
●跨语言链接（Inter-language_Links）：抽取词条页面指向其他语言版本的跨语言链接；联；●图片（Images）：提取指向图片的链接；●重定向（Redirects）：抽取维基百科词条的重定向链接，建立其与同义词条的关●消歧（Disambiguation）：从维基百科消歧页面抽取有歧义的词条链接；●外部链接（External_Links）：抽取词条正文指向维基百科外部的链接；●页面链接（Pagelinks）：抽取词条正文指向维基百科内部的链接；●主页（Homepages）：抽取诸如公司、机构等实体的主页链接；●分类（Categories）：抽取词条所属的分类；●地理坐标（Geo-Coordinates）：抽取词条页面中存在的地理位置的经纬度坐标。,抽取链接,依靠,外部链接
●跨语言链接（Inter-language_Links）：抽取词条页面指向其他语言版本的跨语言链接；联；●图片（Images）：提取指向图片的链接；●重定向（Redirects）：抽取维基百科词条的重定向链接，建立其与同义词条的关●消歧（Disambiguation）：从维基百科消歧页面抽取有歧义的词条链接；●外部链接（External_Links）：抽取词条正文指向维基百科外部的链接；●页面链接（Pagelinks）：抽取词条正文指向维基百科内部的链接；●主页（Homepages）：抽取诸如公司、机构等实体的主页链接；●分类（Categories）：抽取词条所属的分类；●地理坐标（Geo-Coordinates）：抽取词条页面中存在的地理位置的经纬度坐标。,抽取链接,依靠,页面链接
●信息框（infobox）：从词条页面的信息框中抽取实体的结构化信息。,信息框,依靠,从词条页面的信息框中抽取实体的结构化信息
在上述抽取器中，信息框抽取从维基百科中取获得大量的实体属性和实体关系，是DBpedia中最有价值的信息之一。,信息框抽取,依靠,维基百科
信息框抽取有两种形式，一种为一般抽取，另一种为基于映射的抽取。,信息框抽取,依靠,一般抽取
信息框的一般抽取直接将信息框中的信息转换为RDF三元组。,信息框的一般抽取,依靠,将信息框中的信息转换为RDF三元组
三元组的主语由DBpedia的URI前缀和词条名称相连组成，谓语由信息框属性URI前缀和属性名相连组成，宾语则基于属性值创建，可以是实体的URI或者数据类型的值。,三元组,中文名,主语组成
三元组的主语由DBpedia的URI前缀和词条名称相连组成，谓语由信息框属性URI前缀和属性名相连组成，宾语则基于属性值创建，可以是实体的URI或者数据类型的值。,三元组的主语,依靠,DBpedia的URI前缀和词条名称相连组成
三元组的主语由DBpedia的URI前缀和词条名称相连组成，谓语由信息框属性URI前缀和属性名相连组成，宾语则基于属性值创建，可以是实体的URI或者数据类型的值。,谓语,依靠,信息框属性URI前缀和属性名相连组成
三元组的主语由DBpedia的URI前缀和词条名称相连组成，谓语由信息框属性URI前缀和属性名相连组成，宾语则基于属性值创建，可以是实体的URI或者数据类型的值。,宾语,依靠,基于属性值创建
然而，这种抽取方式对于维基百科信息框中存在的属性名和信息框模板同义异名问题不作处理，因此抽取出的三元组存在数据不一致的问题。,抽取方式,依靠,维基百科信息框中存在的属性名和信息框模板同义异名问题
为了处理该类问题，DBpedia使用了基于映射的信息框抽取方法；该方法首先将信息框的模板、属性映射到人工定义的本体中的类型和属性，然后采用本体中的词汇描述抽取出的结构化信息，获得的三元组数据质量更高。,信息框抽取方法,依靠,基于映射
图4-29信息框示例[27]4.4.2面向Web网页的知识抽取互联网中的网页含有丰富的数据，与普通文本数据相比，网页也具有一定的结构，因此也被视为是一种半结构化的数据。,面向Web网页的知识抽取,中文名,互联网中的网页含有丰富的数据
图4-29信息框示例[27]4.4.2面向Web网页的知识抽取互联网中的网页含有丰富的数据，与普通文本数据相比，网页也具有一定的结构，因此也被视为是一种半结构化的数据。,面向Web网页的知识抽取,依靠,结构
从页面的HTML代码中可以看到，产品的名称、价格等具体信息可以通过HTML中的标记区分获取到。,HTML中的标记,依靠,区分获取到产品的名称、价格等信息
图4-30某电商网站产品搜索结果页面及其HTML代码从网页中获取结构化信息一般通过包装器实现，图4-31展示了基于包装器抽取网页信息的框架。,包装器,依靠,从网页中获取结构化信息
包装器是能够将数据从HTML网页中抽取出来，并将它们还原为结构化数据的软件程序。,包装器,依靠,能够将数据从HTML网页中抽取出来，并将它们还原为结构化数据的软件程序
包装器的生成方法有三大类：手工方法、包装器归纳方法和自动抽取方法。,包装器的生成方法,依靠,手工方法
包装器的生成方法有三大类：手工方法、包装器归纳方法和自动抽取方法。,包装器的生成方法,依靠,包装器归纳方法
包装器的生成方法有三大类：手工方法、包装器归纳方法和自动抽取方法。,包装器的生成方法,依靠,自动抽取方法
图4-31基于包装器抽取网页信息的框架1.手工方法手工方法是通过人工分析构建包装器信息抽取的规则。,基于包装器抽取网页信息的框架,依靠,手工方法
手工方法需要查看网页结构和代码，在人工分析的基础上，手工编写出适合当前网站的抽取表达式；表达式的形式一般可以是XPath表达式、CSS选择器的表达式等。,手工方法,中文名,在人工分析的基础上，手工编写出适合当前网站的抽取表达式
手工方法需要查看网页结构和代码，在人工分析的基础上，手工编写出适合当前网站的抽取表达式；表达式的形式一般可以是XPath表达式、CSS选择器的表达式等。,手工方法的抽取表达式,依靠,查看网页结构和代码
XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。,XPath,依靠,用来确定XML文档中某部分位置的语言
借助它可以获取网页中元素的位置，从而获取需要的信息。,SPO,依靠,获取网页中元素的位置
在图4-30的例子中，如果要获取产品价格信息，则可以定义如下XPath进行抽取：CSS选择器是通过CSS元素实现对网页中元素的定位，并获取元素信息的。,CSS选择器,中文名,通过CSS元素实现对网页中元素的定位，并获取元素信息
在图4-30的例子中，如果要获取产品价格信息，则可以定义如下XPath进行抽取：CSS选择器是通过CSS元素实现对网页中元素的定位，并获取元素信息的。,CSS选择器,依靠,通过CSS元素实现对网页中元素的定位，并获取元素信息
分析图4-30中的搜索结果页面，价格信息的CSS选择器表达式为：2.包装器归纳方法包装器归纳方法是基于有监督学习方法从已标注的训练样例集合中学习信息抽取的规则，然后对相同模板的其他网页进行数据抽取的方法。,包装器归纳方法,中文名,基于有监督学习方法从已标注的训练样例集合中学习信息抽取的规则，然后对相同模板的其他网页进行数据抽取的方法
分析图4-30中的搜索结果页面，价格信息的CSS选择器表达式为：2.包装器归纳方法包装器归纳方法是基于有监督学习方法从已标注的训练样例集合中学习信息抽取的规则，然后对相同模板的其他网页进行数据抽取的方法。,包装器归纳方法,依靠,基于有监督学习方法从已标注的训练样例集合中学习信息抽取的规则
典型的包装器归纳流程包括以下步骤：网页清洗、网页标注、包装器空间生成、包装器评估。,包装器归纳流程,依靠,网页清洗
典型的包装器归纳流程包括以下步骤：网页清洗、网页标注、包装器空间生成、包装器评估。,包装器归纳流程,依靠,网页标注
典型的包装器归纳流程包括以下步骤：网页清洗、网页标注、包装器空间生成、包装器评估。,包装器归纳流程,依靠,包装器空间生成
典型的包装器归纳流程包括以下步骤：网页清洗、网页标注、包装器空间生成、包装器评估。,包装器归纳流程,依靠,包装器评估
（1）网页清洗。,网页清洗,依靠,信息抽取
纠正和清理网页不规范的HTML、XML标记，可采用TIDY类工具。,TIDY类工具,依靠,纠正和清理网页不规范的HTML、XML标记
（2）网页标注。,网页标注,依靠,概念/产品
在网页上标注需要抽取的数据，标注过程一般是给网页中的某个位置打上特殊的标签，表明此处是需要抽取的数据。,标注过程,依靠,给网页中的某个位置打上特殊的标签
例如，在图4-30的例子中，如果需要抽取页面上“华为P10”产品的信息和价格，则可以在产品信息和价格所在的标签里打上一个特殊的标记作为标注。,标注,中文名,特殊的标记作为标注
例如，在图4-30的例子中，如果需要抽取页面上“华为P10”产品的信息和价格，则可以在产品信息和价格所在的标签里打上一个特殊的标记作为标注。,标注,依靠,产品信息和价格所在的标签里打上一个特殊的标记作为标注
（3）包装器空间生成。,包装器空间生成,依靠,概念/产品
基于标注的数据生成XPath集合空间，对生成的集合进行归纳，从而形成若干个子集。,生成XPath集合空间,依靠,对生成的集合进行归纳
归纳的目标是使子集中的XPath能够覆盖尽可能多的已标注数据项，使其具有一定的泛化能力。,归纳,中文名,使子集中的XPath能够覆盖尽可能多的已标注数据项，使其具有一定的泛化能力。
归纳的目标是使子集中的XPath能够覆盖尽可能多的已标注数据项，使其具有一定的泛化能力。,归纳,依靠,使子集中的XPath能够覆盖尽可能多的已标注数据项，使其具有一定的泛化能力。
（4）包装器评估。,包装器评估,依靠,概念/产品
包装器可以通过准确率和召回率进行评估。,包装器,依靠,准确率和召回率
使用待评估包装器对训练数据中的网页进行标注，将包装器输出的与人工标注的相同项的数量表示为N；准确率是N除以包装器输出标注的总数量，而召回率是N除以人工标注数据项的总数量。,准确率,依靠,包装器输出标注的总数量
使用待评估包装器对训练数据中的网页进行标注，将包装器输出的与人工标注的相同项的数量表示为N；准确率是N除以包装器输出标注的总数量，而召回率是N除以人工标注数据项的总数量。,召回率,依靠,人工标注数据项的总数量
准确率和召回率越高，表示包装器的质量越好。,准确率和召回率,依靠,包装器的质量
3.自动抽取方法包装器归纳方法需要大量的人工标注工作，因而不适用对大量站点进行数据的抽取。,自动抽取方法包装器,依靠,大量的人工标注工作
此外，包装器维护的工作量也很大，一旦网站改版，需要重新标注数据，归纳新的包装器。,包装器,中文名,维护工作量
此外，包装器维护的工作量也很大，一旦网站改版，需要重新标注数据，归纳新的包装器。,包装器,依靠,归纳新的包装器
自动抽取方法不需要任何的先验知识和人工标注的数据，可以很好地克服上述问题。,自动抽取方法,依靠,无需任何的先验知识和人工标注的数据
在自动抽取方法中，相似的网页首先通过聚类被分成若干组，通过挖掘同一组中相似网页的重复模式，可以生成适用于该组网页的包装器。,相似的网页,依靠,聚类
在应用包装器进行数据抽取时，首先将需要抽取的页面划分到先前生成的网页组，然后应用该组对应的包装器进行数据抽取。,应用包装器进行数据抽取,依靠,先前生成的网页组
上述三种Web页面的信息抽取方法各有优点和缺点，表4-5对它们进行了对比。,信息抽取,中文名,表4-5对它们进行了对比
上述三种Web页面的信息抽取方法各有优点和缺点，表4-5对它们进行了对比。,概念/产品,依靠,信息抽取
表4-5Web页面的信息抽取方法对比4.5知识挖掘知识挖掘是从已有的实体及实体关系出发挖掘新的知识，具体包括知识内容挖掘和知识结构挖掘。,知识内容挖掘,依靠,已有的实体及实体关系
4.5.1知识内容挖掘：实体链接实体链接是指将文本中的实体指称（Mention）链向其在给定知识库中目标实体的过程。,实体链接,中文名,将文本中的实体指称链向其在给定知识库中目标实体的过程
4.5.1知识内容挖掘：实体链接实体链接是指将文本中的实体指称（Mention）链向其在给定知识库中目标实体的过程。,实体链接,依靠,将文本中的实体指称链向其在给定知识库中目标实体
实体链接可以将文本数据转化为有实体标注的形式，建立文本与知识库的联系，可以为进一步文本分析和处理提供基础。,实体链接,依靠,将文本数据转化为有实体标注的形式，建立文本与知识库的联系
通过实体链接，文本中的实体指称与其在知识库中对应的实体建立了链接。,文本中的实体指称,依靠,知识库中对应的实体建立链接
实体链接的基本流程如图4-33所示，包括实体指称识别、候选实体生成和候选实体消歧三个步骤，每个步骤都可以采用不同的技术和方法。,实体链接,依靠,实体指称识别
实体链接的基本流程如图4-33所示，包括实体指称识别、候选实体生成和候选实体消歧三个步骤，每个步骤都可以采用不同的技术和方法。,实体链接,依靠,候选实体生成
实体链接的基本流程如图4-33所示，包括实体指称识别、候选实体生成和候选实体消歧三个步骤，每个步骤都可以采用不同的技术和方法。,实体链接,依靠,候选实体消歧
图4-32实体链接示例图4-33实体链接的基本流程1.实体指称识别实体链接的第一步是要识别出文本中的实体指称，例如从图4-32给出的文本中识别[乔丹]、[美国]、[NBA]等。,实体链接,中文名,识别出文本中的实体指称
图4-32实体链接示例图4-33实体链接的基本流程1.实体指称识别实体链接的第一步是要识别出文本中的实体指称，例如从图4-32给出的文本中识别[乔丹]、[美国]、[NBA]等。,实体链接,依靠,实体指称识别
该步骤主要通过命名实体识别技术或者词典匹配技术实现。,命名实体识别技术,依靠,该步骤
命名实体识别技术在本章前面已经介绍过；词典匹配技术需要首先构建问题领域的实体指称词典，通过直接与文本的匹配识别指称。,命名实体识别技术,依靠,文本的匹配识别指称
2.候选实体生成候选实体生成是确定文本中的实体指称可能指向的实体集合。,候选实体生成,依靠,确定文本中的实体指称可能指向的实体集合
例如，上述例子中实体指称[乔丹]可以指代知识库中的多个实体，如[篮球运动员迈克尔乔丹]、[足球运动员迈克尔乔丹]、[运动品牌飞人乔丹]等。,实体指称,中文名,乔丹
例如，上述例子中实体指称[乔丹]可以指代知识库中的多个实体，如[篮球运动员迈克尔乔丹]、[足球运动员迈克尔乔丹]、[运动品牌飞人乔丹]等。,实体指称,依靠,知识库中的多个实体
生成实体指称的候选实体有以下三种方法：（1）表层名字扩展。,生成实体指称的候选实体,依靠,表层名字扩展
某些实体提及是缩略词或其全名的一部分，因此可以通过表层名字扩展技术，从实体提及出现的相关文档中识别其他可能的扩展变体（例如全名）。,表层名字扩展技术,中文名,从实体提及出现的相关文档中识别其他可能的扩展变体
某些实体提及是缩略词或其全名的一部分，因此可以通过表层名字扩展技术，从实体提及出现的相关文档中识别其他可能的扩展变体（例如全名）。,表层名字扩展技术,依靠,从实体提及出现的相关文档中识别其他可能的扩展变体
然后，可以利用这些扩展形式形成实体提及的候选实体集合。,实体提及的候选实体集合,依靠,利用这些扩展形式形成
表层名字扩展可以采用启发式的模式匹配方法实现。,表层名字扩展,依靠,启发式的模式匹配方法
例如，常用的模式是提取实体提及邻近括号中的缩写作为扩展结果；例如“University_of_Illinois_at_Urbana-Champaign（UIUC）”“Hewlett-Packard（HP）”等。,模式,中文名,提取实体提及邻近括号中的缩写作为扩展结果
例如，常用的模式是提取实体提及邻近括号中的缩写作为扩展结果；例如“University_of_Illinois_at_Urbana-Champaign（UIUC）”“Hewlett-Packard（HP）”等。,模式,依靠,提取实体提及邻近括号中的缩写作为扩展结果
除了使用模式匹配的方法，也有一些方法通过有监督学习的技术从文本中抽取复杂的实体名称缩写。,从文本中抽取复杂的实体名称缩写,依靠,模式匹配的方法
除了使用模式匹配的方法，也有一些方法通过有监督学习的技术从文本中抽取复杂的实体名称缩写。,从文本中抽取复杂的实体名称缩写,依靠,有监督学习的技术
（2）基于搜索引擎的方法。,基于搜索引擎的方法,依靠,基于搜索引擎
将实体提及和上下文文字提交至搜索引擎，可以根据搜索引擎返回的检索结果生成候选实体。,搜索引擎,依靠,根据搜索引擎返回的检索结果生成候选实体
例如，可以将实体指称作为搜索关键词提交至谷歌搜索引擎，并将其返回结果中的维基百科页面作为候选实体。,实体指称,依靠,谷歌搜索引擎
此外，维基百科自有的搜索功能也可以用于生成候选实体。,维基百科搜索功能,依靠,生成候选实体
（3）构建查询实体引用表。,查询实体引用表,依靠,构建
很多实体链接系统都基于维基百科数据构建查询实体引用表，建立实体提及与候选实体的对应关系。,实体链接系统,依靠,构建查询实体引用表，建立实体提及与候选实体的对应关系
实体引用表示例如表4-5所示，它可以看作是一个<键-值>映射；一个键可以对应一个或多个值。,实体引用,依靠,一个键可以对应一个或多个值
在完成引用表构建后，可以通过实体提及直接从表中获得其候选实体。,引用表的构建,依靠,实体提及
为了构建查询实体引用表，常用的方法是基于维基百科中的词条页面、重定向页面、消歧页面、词条正文超链接等抽取实体提及与实体的对应关系。,抽取实体提及与实体的对应关系,中文名,构建查询实体引用表
为了构建查询实体引用表，常用的方法是基于维基百科中的词条页面、重定向页面、消歧页面、词条正文超链接等抽取实体提及与实体的对应关系。,查询实体引用表,依靠,基于维基百科中的词条页面、重定向页面、消歧页面、词条正文超链接等抽取实体提及与实体的对应关系
维基百科词条页面描述的对象通常被当作知识库中的实体，词条页面的标题即为实体提及；重定向页面的标题可以作为其所指向词条实体的提及；消歧页面标题可作为实体提及，其对应的实体是页面中列出的词条实体。,维基百科词条页面描述的对象,中文名,实体
维基百科词条页面描述的对象通常被当作知识库中的实体，词条页面的标题即为实体提及；重定向页面的标题可以作为其所指向词条实体的提及；消歧页面标题可作为实体提及，其对应的实体是页面中列出的词条实体。,维基百科词条页面描述的对象,依靠,知识库中的实体
维基百科页面中的链接是以[[实体|实体提及]]的格式标记的，因此处理所有的链接可以提取实体和实体提及的对应关系。,维基百科页面中的链接,依靠,提取实体和实体提及的对应关系
表4-5实体引用表示例3.候选实体消歧在确定文本中的实体指称和它们的候选实体后，实体链接系统需要为每一个实体指称确定其指向的实体，这一步骤被称为候选实体消歧。,候选实体消歧,中文名,为每一个实体指称确定其指向的实体
表4-5实体引用表示例3.候选实体消歧在确定文本中的实体指称和它们的候选实体后，实体链接系统需要为每一个实体指称确定其指向的实体，这一步骤被称为候选实体消歧。,候选实体消歧,依靠,为每一个实体指称确定其指向的实体
一般地，候选实体消歧被作为排序问题进行求解；即给定实体提及，对它的候选实体按照链接可能性由大到小进行排序。,候选实体消歧,中文名,排序问题
一般地，候选实体消歧被作为排序问题进行求解；即给定实体提及，对它的候选实体按照链接可能性由大到小进行排序。,候选实体消歧,依靠,排序问题进行求解
总体上，候选实体消歧方法包括基于图的方法、基于概率生成模型的方法、基于主题模型的方法和基于深度学习的方法等。,候选实体消歧方法,依靠,基于图的方法
总体上，候选实体消歧方法包括基于图的方法、基于概率生成模型的方法、基于主题模型的方法和基于深度学习的方法等。,候选实体消歧方法,依靠,基于概率生成模型的方法
总体上，候选实体消歧方法包括基于图的方法、基于概率生成模型的方法、基于主题模型的方法和基于深度学习的方法等。,候选实体消歧方法,依靠,基于主题模型的方法
总体上，候选实体消歧方法包括基于图的方法、基于概率生成模型的方法、基于主题模型的方法和基于深度学习的方法等。,候选实体消歧方法,依靠,基于深度学习的方法
下面介绍每类方法中具有代表性的工作。,每类方法中具有代表性的工作,依靠,介绍
[32]（1）基于图的方法。,基于图的方法,依靠,图
基于图的方法将实体指称、实体以及它们之间的关系通过图的形式表示出来，然后在图上对实体指称之间、候选实体之间、实体指称与候选实体之间的关联关系进行协同推理。,基于图的方法,中文名,在图上对实体指称之间、候选实体之间、实体指称与候选实体之间的关联关系进行协同推理
基于图的方法将实体指称、实体以及它们之间的关系通过图的形式表示出来，然后在图上对实体指称之间、候选实体之间、实体指称与候选实体之间的关联关系进行协同推理。,基于图的方法,依靠,实体指称、实体以及它们之间的关系
该类方法比较具有代表性的是Han等人较早提出的基于参照图（Referent_Graph）协同实体链接方法[33]。,基于参照图（Referent_Graph）协同实体链接方法,依靠,Han等人较早提出的基于参照图（Referent_Graph）协同实体链接方法
Han等人提出在候选实体消歧时，首先建立如图4-34所示的参照图，图中对实体、提及-实体、实体-实体的关系进行了表示；图中实体提及和实体间的加权边表示了它们的局部依赖性；实体和实体间的加权边代表实体间的语义相关度，为进行全局协同的实体消歧提供了基础。,实体提及,依靠,实体-实体的关系
Han等人提出在候选实体消歧时，首先建立如图4-34所示的参照图，图中对实体、提及-实体、实体-实体的关系进行了表示；图中实体提及和实体间的加权边表示了它们的局部依赖性；实体和实体间的加权边代表实体间的语义相关度，为进行全局协同的实体消歧提供了基础。,实体,依靠,实体间的加权边
在计算了实体提及的初始重要性度量等人的方法将其作为实体消歧的初始依据并在参照图上进行传递，该过程与后，Han_PageRank算法中节点rank值的传递与更新方式类似。,Han_PageRank算法,中文名,节点rank值的传递与更新方式类似
在计算了实体提及的初始重要性度量等人的方法将其作为实体消歧的初始依据并在参照图上进行传递，该过程与后，Han_PageRank算法中节点rank值的传递与更新方式类似。,Han_PageRank算法,依靠,节点rank值的传递与更新方式
最后，基于实体消歧依据的传递结果，计算一个结合局部相容度和全局依赖性的实体消歧目标函数，为每个实体提及确定能使目标函数最大化的目标实体，从而得到实体消歧结果。,实体消歧目标函数,中文名,使目标函数最大化的目标实体
最后，基于实体消歧依据的传递结果，计算一个结合局部相容度和全局依赖性的实体消歧目标函数，为每个实体提及确定能使目标函数最大化的目标实体，从而得到实体消歧结果。,实体消歧目标函数,依靠,局部相容度
最后，基于实体消歧依据的传递结果，计算一个结合局部相容度和全局依赖性的实体消歧目标函数，为每个实体提及确定能使目标函数最大化的目标实体，从而得到实体消歧结果。,实体消歧目标函数,依靠,全局依赖性
采用基于图的方法进行候选实体消歧的实体链接系统还有文献[34]、文献[35]等。,实体链接系统,依靠,基于图的方法
图4-34参照图[33]（2）基于概率生成模型的方法。,基于概率生成模型的方法,依靠,基于概率生成模型的方法依靠概率生成模型
基于概率生成模型对实体提及和实体的联合概率进行建模，可以通过模型的推理求解实体消歧问题。,基于概率生成模型,依靠,对实体提及和实体的联合概率进行建模
在Han等人[36]提出的实体-提及概率生成模型中，实体提及被作为生成样本进行建模，其生成过程如图4-35所示。,实体-提及概率生成模型,依靠,生成样本进行建模
图4-35实体提及生成过程示例[36]首先，模型依据实体的概率分布P（e）选择实体提及对应的实体，如例子中的[Michael_Jeffrey_Jordan]和[Michael_I.Jordan]；然后，模型依据给定实体e实体名称的条件概率P（s|e）选择实体提及的名称，如例子中的[Jordan]和[Michael_Jordan]；最后，模型依据给定实体e上下文的条件概率P（c|e）输出实体提及的上下文。,模型,依靠,依据实体的概率分布P（e）选择实体提及对应的实体
"根据上述实体提及的生成过程，实体和提及的联合概率可以定义为P（m,e）=P（s,c,e）=P（e）P（s|e）P（c|e）在该方法中，P（e）对应了实体的流行度，P（s|e）对应了实体名称知识，P（c|e）对应了上下文知识。",实体联合概率,中文名,"P（m,e）"
当给定实体提及m时，候选实体消歧通过以下式子实现（3）基于主题模型的方法。,候选实体消歧,依靠,基于主题模型的方法
Han等人认为，在同一个文本中出现的实体应该与文本表述的主题相关。,同一个文本中出现的实体,依靠,与文本表述的主题相关
基于该思想，他们提出了实体-主题模型，可以对实体在文本中的相容度、实体与话题的一致性进行联合建模，从而提升实体链接的结果。,实体-主题模型,中文名,对实体在文本中的相容度、实体与话题的一致性进行联合建模
基于该思想，他们提出了实体-主题模型，可以对实体在文本中的相容度、实体与话题的一致性进行联合建模，从而提升实体链接的结果。,实体-主题模型,依靠,对实体在文本中的相容度、实体与话题的一致性进行联合建模
实体-主题模型如图4-36所示，给定主题数量T、实体数量E、实体名称数量K和词的数量V，实体-主题模型通过如下过程生成关于主题、实体名称和实体上下文的全局知识。,实体-主题模型,依靠,给定主题数量T、实体数量E、实体名称数量K和词的数量V，生成关于主题、实体名称和实体上下文的全局知识
通过吉布斯抽样算法，可以基于实体-主题模型推断获得实体消歧所需的决策信息。,实体-主题模型推断,依靠,基于实体-主题模型推断获得实体消歧所需的决策信息
图4-36实体-主题模型[37]（4）基于深度学习的方法。,实体-主题模型,依靠,基于深度学习的方法
在候选实体消歧过程中，准确计算实体的相关度十分重要。,准确计算实体的相关度,依靠,候选实体消歧过程
因为在利用上下文中信息或进行协同实体消歧时，都需要评价实体与实体的相关度。,评价实体与实体的相关度,依靠,利用上下文中信息或进行协同实体消歧
Huang等人[38]提出了一个基于深度神经网络的实体语义相关度计算模型，如图4-37所示。,基于深度神经网络的实体语义相关度计算模型,依靠,实体语义相关度计算
基于词袋和独热表示的输入经过词散列层进行降维，然后经过多层神经网络的非线性变换，得到语义层上实体的表示；两个实体的相关度被定义为它们语义层表示向量的余弦相似度。,语义层上实体的表示,中文名,向量的余弦相似度
基于词袋和独热表示的输入经过词散列层进行降维，然后经过多层神经网络的非线性变换，得到语义层上实体的表示；两个实体的相关度被定义为它们语义层表示向量的余弦相似度。,两个实体的相关度,依靠,它们语义层表示向量的余弦相似度
"图4-37实体提及生成过程示例[38]4.5.2知识结构挖掘：规则挖掘1.归纳逻辑程序设计归纳逻辑程序设计（Inductive_Logic_Programming,ILP）是以一阶逻辑归纳为理论基础，并以一阶逻辑为表达语言的符号规则学习算法[39]。",归纳逻辑程序设计,中文名,以一阶逻辑归纳为理论基础，并以一阶逻辑为表达语言的符号规则学习算法
"图4-37实体提及生成过程示例[38]4.5.2知识结构挖掘：规则挖掘1.归纳逻辑程序设计归纳逻辑程序设计（Inductive_Logic_Programming,ILP）是以一阶逻辑归纳为理论基础，并以一阶逻辑为表达语言的符号规则学习算法[39]。",归纳逻辑程序设计,依靠,一阶逻辑归纳
知识图谱中的实体关系可看作是二元谓词描述的事实，因此也可通过ILP方法从知识图谱中学习一阶逻辑规则。,实体关系,依靠,二元谓词描述的事实
"给定背景知识和目标谓词（知识图谱中即为关系）,ILP系统可以学习获得描述目标谓词的逻辑规则集合。",ILP系统,依靠,学习获得描述目标谓词的逻辑规则集合
FOIL[40]是早期具有代表性的ILP系统，它采用顺序覆盖的策略逐条学习逻辑规则，在学习每条规则时，FOIL采用了基于信息熵的评价函数引导搜索过程，归纳学习一阶规则。,FOIL,中文名,早期具有代表性的ILP系统
FOIL[40]是早期具有代表性的ILP系统，它采用顺序覆盖的策略逐条学习逻辑规则，在学习每条规则时，FOIL采用了基于信息熵的评价函数引导搜索过程，归纳学习一阶规则。,FOIL,依靠,顺序覆盖的策略逐条学习逻辑规则
下面通过一个例子介绍FOIL的规则学习过程。,FOIL的规则学习过程,依靠,概念/产品
设有规则学习问题如表4-6所示。,规则学习问题,依靠,概念/产品
背景知识描述了某一家庭的成员关系，规则学习的目标谓词为daughter，该目标谓词有若干正例和反例事实。,规则学习的目标谓词,依靠,背景知识描述了某一家庭的成员关系
"FOIL在规则学习过程中，从空规则daughter（X,Y）←开始，逐一将可用谓词加入规则体进行考察，按照预定标准评估规则的优劣并选取最优规则；持续谓词的添加直至规则只覆盖正例而不覆盖任何反例。",FOIL,中文名,正向实例过滤
"FOIL在规则学习过程中，从空规则daughter（X,Y）←开始，逐一将可用谓词加入规则体进行考察，按照预定标准评估规则的优劣并选取最优规则；持续谓词的添加直至规则只覆盖正例而不覆盖任何反例。",FOIL,依靠,逐一将可用谓词加入规则体进行考察
表4-7列出了FOIL学习单个规则的过程。,FOIL学习单个规则的过程,依靠,依靠记忆
当获得一个满足上述要求的规则后，FOIL将被该规则覆盖的正例移除，然后基于剩余的正例和反例再重复上述过程获得新的规则，直至所有正例都被移除。,FOIL,中文名,反例正例循环获取规则
当获得一个满足上述要求的规则后，FOIL将被该规则覆盖的正例移除，然后基于剩余的正例和反例再重复上述过程获得新的规则，直至所有正例都被移除。,FOIL,依靠,基于剩余的正例和反例再重复上述过程获得新的规则，直至所有正例都被移除
表4-6规则学习问题表4-7FOIL学习单个规则的过程注：和分别为被规则ri覆盖正例和反例的数量。,FOIL,中文名,泛化逆反集
表4-6规则学习问题表4-7FOIL学习单个规则的过程注：和分别为被规则ri覆盖正例和反例的数量。,FOIL,依靠,抽取概念或实体
在扩展规则体的每一步，FOIL选择使得规则FOIL_Gain达到最大的谓词加入规则体。,FOIL,依靠,选择使得规则FOIL_Gain达到最大的谓词加入规则体
FOIL_Gain的定义为：式中，Ri为当前待扩展的规则；Li+1为由候选谓词构成的新文字；和分别为被规则Ri覆盖正例和反例的数量；和分别为被新规则Ri+1覆盖正例和反例的数量；为同时被规则Ri和Ri+1覆盖的正例数量。,FOIL_Gain,中文名,形式化同义反演规则获取
FOIL_Gain的定义为：式中，Ri为当前待扩展的规则；Li+1为由候选谓词构成的新文字；和分别为被规则Ri覆盖正例和反例的数量；和分别为被新规则Ri+1覆盖正例和反例的数量；为同时被规则Ri和Ri+1覆盖的正例数量。,FOIL_Gain,依靠,由候选谓词构成的新文字
基于FOIL_Gain评价函数，FOIL在构建规则的每一阶段倾向于选择覆盖较多正例和较少反例的规则。,FOIL_Gain评价函数,依靠,构建规则的每一阶段倾向于选择覆盖较多正例和较少反例的规则
在早期的ILP系统中，还有以Progol[41]为代表的基于逆语义蕴涵的学习方法。,早期的ILP系统,依靠,基于逆语义蕴涵的学习方法
"文献[39,42]对大量ILP方法进行了综述。",ILP方法,依靠,对大量进行综述
多数ILP系统仅适用于小规模的数据集，在较大规模的数据集上运行效率不高。,ILP系统,依靠,小规模的数据集
因此，近年来也有大量研究致力于提高ILP系统的可扩展性，这些工作包括FOIL-D[43]、PILP[44]、QuickFOIL[45]和分布式并行的ILP系统[46]等。,FOIL-D,中文名,FOIL-分布式并行的ILP系统
因此，近年来也有大量研究致力于提高ILP系统的可扩展性，这些工作包括FOIL-D[43]、PILP[44]、QuickFOIL[45]和分布式并行的ILP系统[46]等。,FOIL-D,依靠,可扩展性
因此，近年来也有大量研究致力于提高ILP系统的可扩展性，这些工作包括FOIL-D[43]、PILP[44]、QuickFOIL[45]和分布式并行的ILP系统[46]等。,PILP,依靠,可扩展性
因此，近年来也有大量研究致力于提高ILP系统的可扩展性，这些工作包括FOIL-D[43]、PILP[44]、QuickFOIL[45]和分布式并行的ILP系统[46]等。,QuickFOIL,依靠,可扩展性
因此，近年来也有大量研究致力于提高ILP系统的可扩展性，这些工作包括FOIL-D[43]、PILP[44]、QuickFOIL[45]和分布式并行的ILP系统[46]等。,分布式并行的ILP系统,依靠,可扩展性
最近，针对大规模知识图谱的特点，Galarraga等人研究并提出了AMIE系统[47];AMIE采用关联规则挖掘的方法，并定义了新的支持度和覆盖度度量对搜索空间进行剪枝，并可以在知识图谱不完备的条件下评价规则，在规则质量和学习效率方面都比传统ILP方法有很大的提升。,AMIE,依靠,关联规则挖掘的方法
最近，针对大规模知识图谱的特点，Galarraga等人研究并提出了AMIE系统[47];AMIE采用关联规则挖掘的方法，并定义了新的支持度和覆盖度度量对搜索空间进行剪枝，并可以在知识图谱不完备的条件下评价规则，在规则质量和学习效率方面都比传统ILP方法有很大的提升。,AMIE,依靠,新的支持度和覆盖度度量
在对AMIE多个计算过程进行优化后，Galarraga等人又发布了其升级系统AMIE+[48]，新系统具有更高的计算效率。,AMIE+,依靠,对AMIE多个计算过程进行优化
"2.路径排序算法（PathRankingAlgorithm,PRA）PRA[49,50]是一种将关系路径作为特征的知识图谱链接预测算法，因为其获取的关系路径实际上对应一种霍恩子句，PRA计算的路径特征可以转换为逻辑规则，便于人们发现和理解知识图谱中隐藏的知识。",路径排序算法,中文名,PRA
"2.路径排序算法（PathRankingAlgorithm,PRA）PRA[49,50]是一种将关系路径作为特征的知识图谱链接预测算法，因为其获取的关系路径实际上对应一种霍恩子句，PRA计算的路径特征可以转换为逻辑规则，便于人们发现和理解知识图谱中隐藏的知识。",PRA,依靠,将关系路径作为特征的知识图谱链接预测算法
PRA的基本思想是通过发现连接两个实体的一组关系路径来预测实体间可能存在的某种特定关系。,PRA,依靠,发现连接两个实体的一组关系路径来预测实体间可能存在的某种特定关系
"如图4-38所示，若要预测球员和赛事联盟之间的AlthletePlaysForLeague关系，连接实体HinesWard和NFL的关系路径<AlthletePlaysForTeam,TeamPlaysInLeague>可以作为预测模型的一个重要特征。",连接实体HinesWard和NFL的关系路径,中文名,"<AlthletePlaysForTeam,TeamPlaysInLeague>"
"如图4-38所示，若要预测球员和赛事联盟之间的AlthletePlaysForLeague关系，连接实体HinesWard和NFL的关系路径<AlthletePlaysForTeam,TeamPlaysInLeague>可以作为预测模型的一个重要特征。",连接实体HinesWard和NFL的关系路径,依靠,"<AlthletePlaysForTeam,TeamPlaysInLeague>"
实际上，该关系路径对应着一个常识知识，可以用图4-39中的霍恩子句表示。,关系路径,依靠,一个常识知识
在链接预测过程中，PRA会自动发现有用的关系路径来构建预测模型；PRA具体的工作流程分为三个重要的步骤：特征选择、特征计算和关系分类。,PRA,中文名,自动发现有用的关系路径来构建预测模型的工作流程
在链接预测过程中，PRA会自动发现有用的关系路径来构建预测模型；PRA具体的工作流程分为三个重要的步骤：特征选择、特征计算和关系分类。,PRA,依靠,特征选择
在链接预测过程中，PRA会自动发现有用的关系路径来构建预测模型；PRA具体的工作流程分为三个重要的步骤：特征选择、特征计算和关系分类。,PRA,依靠,特征计算
在链接预测过程中，PRA会自动发现有用的关系路径来构建预测模型；PRA具体的工作流程分为三个重要的步骤：特征选择、特征计算和关系分类。,PRA,依靠,关系分类
图4-38示例知识图谱子图[50]图4-39霍恩子句（1）特征选择。,图4-38示例知识图谱子图,依靠,概念/产品
因为知识图谱中连接特定实体对的关系路径数量可能会很多，特别是当允许的关系路径长度较长时，关系路径的数量将快速增长。,关系路径,依靠,特定实体对
PRA并不使用连接实体对的所有关系路径作为模型的特征，所以第一步会对关系路径进行选择，仅保留对于预测目标关系潜在有用的关系路径。,PRA,中文名,选择关系路径
PRA并不使用连接实体对的所有关系路径作为模型的特征，所以第一步会对关系路径进行选择，仅保留对于预测目标关系潜在有用的关系路径。,PRA,依靠,对关系路径进行选择，仅保留对于预测目标关系潜在有用的关系路径
"为了保证特征选择的效率，PRA使用了基于随机游走的特征选择方法；对于某个关系路径π,PRA基于随机游走计算该路径的准确度（precision）和覆盖度（coverage）。",PRA,中文名,基于随机游走的特征选择方法
"为了保证特征选择的效率，PRA使用了基于随机游走的特征选择方法；对于某个关系路径π,PRA基于随机游走计算该路径的准确度（precision）和覆盖度（coverage）。",PRA,中文名,基于随机游走计算某个路径的准确度（precision）和覆盖度（coverage）
"为了保证特征选择的效率，PRA使用了基于随机游走的特征选择方法；对于某个关系路径π,PRA基于随机游走计算该路径的准确度（precision）和覆盖度（coverage）。",PRA,依靠,基于随机游走计算路径的准确度
"为了保证特征选择的效率，PRA使用了基于随机游走的特征选择方法；对于某个关系路径π,PRA基于随机游走计算该路径的准确度（precision）和覆盖度（coverage）。",PRA,依靠,基于随机游走计算路径的覆盖度
式中，P（si→Gi;π）是以实体si为起点，沿着关系路径π进行随机游走能够抵达目标实体的概率。,P（si→Gi;π）,中文名,以实体si为起点，沿着关系路径π进行随机游走能够抵达目标实体的概率
式中，P（si→Gi;π）是以实体si为起点，沿着关系路径π进行随机游走能够抵达目标实体的概率。,P（si→Gi;π）,依靠,沿着关系路径π进行随机游走能够抵达目标实体的概率
PRA对于准确度和覆盖度都分别设定阈值，只有当两个度量值不小于阈值的关系路径时，才被作为特征保留。,PRA,中文名,概念/产品属性约简
PRA对于准确度和覆盖度都分别设定阈值，只有当两个度量值不小于阈值的关系路径时，才被作为特征保留。,PRA,依靠,关系路径
（2）特征计算。,特征计算,依靠,概念/产品
在选择了有用的关系路径作为特征之后，PRA将为每个实体对计算其特征值。,PRA,依靠,为每个实体对计算其特征值
"给定实体对(h,t)和某一特征路径π,PRA将从实体s为起点沿着关系路径π进行随机游走抵达实体t的概率作为该实体对在关系路径π特征的值。",PRA,依靠,从实体s为起点沿着关系路径π进行随机游走抵达实体t的概率
通过计算实体对在每个特征关系路径上的可达概率，就可以得到该实体对所有特征的值。,实体对在每个特征关系路径上的可达概率,依靠,得到该实体对所有特征的值
（3）关系分类。,关系分类,依靠,概念/产品
基于训练样例（目标关系的正例实体对和反例实体对）和它们的特征，PRA为每个目标关系训练一个分类模型。,PRA,依靠,训练一个分类模型
利用训练完的模型，可以预测知识图谱中任意两个实体间是否存在某特定关系。,知识图谱中任意两个实体间是否存在某特定关系,依靠,利用训练完的模型
关系分类可以使用任何一种分类模型，PRA中使用了逻辑回归分类模型，并取得了较好的效果。,关系分类,依靠,任何一种分类模型
PRA在训练逻辑回归模型的过程中，可以获得关系路径的权重，从而可以对路径的重要性进行排序；而且关系路径具有很好的可解释性。,PRA,依靠,获得关系路径的权重
本实践的相关工具、实验数据及操作说明由OpenKG提供，地址为http://openkg.cn。,本实践,依靠,OpenKG
该框架遵循Apache开源协议。,该框架,依靠,Apache开源协议
4.6.1开源工具的技术架构如图4-41所示为DeepDive的整体处理流程，主要分为数据准备和因子图模型构建两个部分，CNdeepdive在此基础上添加了神经网络模型和增量操作。,DeepDive,中文名,数据准备
4.6.1开源工具的技术架构如图4-41所示为DeepDive的整体处理流程，主要分为数据准备和因子图模型构建两个部分，CNdeepdive在此基础上添加了神经网络模型和增量操作。,DeepDive,中文名,因子图模型构建
4.6.1开源工具的技术架构如图4-41所示为DeepDive的整体处理流程，主要分为数据准备和因子图模型构建两个部分，CNdeepdive在此基础上添加了神经网络模型和增量操作。,CNdeepdive,中文名,添加了神经网络模型
4.6.1开源工具的技术架构如图4-41所示为DeepDive的整体处理流程，主要分为数据准备和因子图模型构建两个部分，CNdeepdive在此基础上添加了神经网络模型和增量操作。,CNdeepdive,中文名,增量操作
4.6.1开源工具的技术架构如图4-41所示为DeepDive的整体处理流程，主要分为数据准备和因子图模型构建两个部分，CNdeepdive在此基础上添加了神经网络模型和增量操作。,DeepDive,依靠,数据准备
4.6.1开源工具的技术架构如图4-41所示为DeepDive的整体处理流程，主要分为数据准备和因子图模型构建两个部分，CNdeepdive在此基础上添加了神经网络模型和增量操作。,DeepDive,依靠,因子图模型构建
在具体应用中，可以选择使用因子图模型或神经网络模型。,因子图模型,依靠,具体应用
图4-41DeepDive的整体处理流程图中浅色字体部分是股权交易关系抽取实例在框架中对应的文件名、命令或需要配置的脚本文件。,股权交易关系抽取,依靠,文件名、命令或需要配置的脚本文件
