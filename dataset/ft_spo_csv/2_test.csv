input,subject,subject_type,relation,object,object_type
表2-5OWL词汇扩展3.OWL版本目前，OWL2是OWL的最新版本，老的OWL版本也被称为OWL1。,OWL,知识图谱,,OWL2,
表2-5OWL词汇扩展3.OWL版本目前，OWL2是OWL的最新版本，老的OWL版本也被称为OWL1。,OWL,知识图谱,,OWL2,
表2-5OWL词汇扩展3.OWL版本目前，OWL2是OWL的最新版本，老的OWL版本也被称为OWL1。,OWL版本,知识图谱,,OWL,
OWL2定义了一些OWL的子语言，通过限制语法使用，使得这些子语言能够更方便地实现，以及服务不同的应用。,OWL2,知识图谱,,OWL的子语言,
OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。,OWL2,知识图谱,,OWL_2_RL,
OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。,OWL2,知识图谱,,OWL_2_QL,
OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。,OWL2,知识图谱,,OWL_2_EL,
OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。,OWL2的子语言,知识图谱,,OWL_2_RL,
OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。,OWL2的子语言,知识图谱,,OWL_2_QL,
OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。,OWL2的子语言,知识图谱,,OWL_2_EL,
OWL_2_QL是OWL2子语言中最为简单的，QL代表Query_Language，所以OWL_2_QL是专为基于本体的查询设计的。,OWL_2_QL,知识图谱,,专为基于本体的查询设计的,
OWL_2_QL是OWL2子语言中最为简单的，QL代表Query_Language，所以OWL_2_QL是专为基于本体的查询设计的。,OWL_2_QL,知识图谱,,Query_Language,
它的查询复杂度是AC0，非常适合大规模处理。,Elasticsearch,知识图谱,,AC0,
它是基于描述逻辑DL-Lite定义的。,OWL_2_QL,知识图谱,,基于描述逻辑DL-Lite,
表2-6给出了OWL_2_QL词汇总结。,OWL_2_QL词汇总结,知识图谱,,表2-6,
表2-6OWL_2_QL词汇总结另外一个能够提供多项式推理的OWL是OWL_2_EL。,OWL_2_QL,知识图谱,,多项式推理的OWL,
表2-6OWL_2_QL词汇总结另外一个能够提供多项式推理的OWL是OWL_2_EL。,OWL_2_EL,知识图谱,,OWL_2_EQ,
与OWL_2_QL不同，OWL_2_EL专为概念术语描述、本体的分类推理而设计，广泛应用在生物医疗领域，如临床医疗术语本体SNOMED_CT。,OWL_2_EL,知识图谱,,概念术语描述本体推理,
OWL_2_EL的分类复杂度是Ptime-Complete，它是基于描述逻辑语言EL++定义的。,OWL_2_EL,知识图谱,,Ptime-Complete,
表2-7给出了OWL_2_QL词汇总结。,OWL_2_QL词汇总结,知识图谱,,表2-7,
表2-7OWL_2_QL词汇总结例如，OWL_2_EL允许表达如下复杂的概念：Female??likes.Movie??hasSon.(Student??attends.CSCourse)指的是所有喜欢电影、儿子是学生且参加计算机课程的女性。,OWL_2_EL,知识图谱,,复杂的概念,
表2-7OWL_2_QL词汇总结例如，OWL_2_EL允许表达如下复杂的概念：Female??likes.Movie??hasSon.(Student??attends.CSCourse)指的是所有喜欢电影、儿子是学生且参加计算机课程的女性。,OWL_2_EL,知识图谱,,OWL_2_EQ,
下面给出一个例子。,SPO,知识图谱,,概念/三元组抽取,
假设有一个本体，包含以下公理：公理1.Apple??beInvestedBy.(Fidelity?BlackStone)：苹果由富达和黑石投资。,Apple,知识图谱,,?beInvestedBy.(Fidelity?BlackStone),
假设有一个本体，包含以下公理：公理1.Apple??beInvestedBy.(Fidelity?BlackStone)：苹果由富达和黑石投资。,Apple,知识图谱,,苹果,
公理2.?beFundedBy.Fidelity?InnovativeCompanies：借助富达融资的公司都是创新企业。,Fidelity,知识图谱,,借助富达融资的公司都是创新企业,
公理3.?beFundedBy.BlackStone?InnovativeCompanies：借助黑石融资的公司都是创新企业。,BlackStone,知识图谱,,借助黑石融资的公司都是创新企业,
公理4.beInvestedBy?beFundedBy：投资即是帮助融资。,beInvestedBy,知识图谱,,beFundedBy,
由公理1可以推出公理5:Apple??beInvestedBy.Fidelity；由公理5和公理4可以推出公???beFundedBy.Fidelity；最后，由公理6和公理2可以推出公理7:Apple理6:Apple_InnovativeCompanies。,公理5,知识图谱,,Apple,
由公理1可以推出公理5:Apple??beInvestedBy.Fidelity；由公理5和公理4可以推出公???beFundedBy.Fidelity；最后，由公理6和公理2可以推出公理7:Apple理6:Apple_InnovativeCompanies。,等价,知识图谱,,Apple??beInvestedBy.Fidelity,
由公理1可以推出公理5:Apple??beInvestedBy.Fidelity；由公理5和公理4可以推出公???beFundedBy.Fidelity；最后，由公理6和公理2可以推出公理7:Apple理6:Apple_InnovativeCompanies。,等价,知识图谱,,由公理5和公理4可以推出公???beFundedBy.Fidelity,
由公理1可以推出公理5:Apple??beInvestedBy.Fidelity；由公理5和公理4可以推出公???beFundedBy.Fidelity；最后，由公理6和公理2可以推出公理7:Apple理6:Apple_InnovativeCompanies。,等价,知识图谱,,由公理6和公理2可以推出公理7:Apple,
还有一个推理复杂度是多项式时间的OWL2子语言叫OWL_2_RL。,OWL_2_RL,知识图谱,,OWL2子语言,
还有一个推理复杂度是多项式时间的OWL2子语言叫OWL_2_RL。,OWL_2_RL,知识图谱,,OWL2推理复杂度是多项式时间的子语言,
OWL_2_RL扩展了RDFS的表达能力，在RDFS的基础上引入属性的特殊特性（函数性、互反性和对称性），允许声明等价性，允许属性的局部约束。,OWL_2_RL,知识图谱,,RDFS的表达能力,
OWL_2_RL扩展了RDFS的表达能力，在RDFS的基础上引入属性的特殊特性（函数性、互反性和对称性），允许声明等价性，允许属性的局部约束。,OWL_2_RL,知识图谱,,OWL_2_RDFS扩展了RDFS的表达能力,
OWL_2_RL_的推理是一种前向链推理，即将推理规则应用到OWL_2_RL本体，得到新的知识，即OWL_2_RL推理是针对实例数据的推理。,OWL_2_RL推理,知识图谱,,前向链推理,
OWL_2_RL_的推理是一种前向链推理，即将推理规则应用到OWL_2_RL本体，得到新的知识，即OWL_2_RL推理是针对实例数据的推理。,OWL_2_RL推理,知识图谱,,前向链推理,
"下面给出两个OWL_2_RL上的推理规则：p_rdfs:domain_x,spo?s_rdf:type_xp_rdfs:range_x,spo?o_rdf:type_x其中，s、p、o、x为变量。",p_rdfs:domain_x,知识图谱,,推理规则的等价,
第一条规则表示如果属性p的定义域是类x，而且实例s和o有关系p（这里把属性与关系看成是一样的），那么实例s是类x的一个元素。,规则,知识图谱,,如果属性p的定义域是类x，而且实例s和o有关系p,
第一条规则表示如果属性p的定义域是类x，而且实例s和o有关系p（这里把属性与关系看成是一样的），那么实例s是类x的一个元素。,如果属性p的定义域是类x,知识图谱,,if the domain of property p is class x,
第一条规则表示如果属性p的定义域是类x，而且实例s和o有关系p（这里把属性与关系看成是一样的），那么实例s是类x的一个元素。,实例s和o有关系p,知识图谱,,instance s and o have relationship p,
第二条规则表示如果属性p的值域是类x，而且实例s和o有关系p，那么实例o是类x的一个元素。,第二条规则,知识图谱,,如果属性p的值域是类x，而且实例s和o有关系p,
第二条规则表示如果属性p的值域是类x，而且实例s和o有关系p，那么实例o是类x的一个元素。,等价,知识图谱,,equivalent,
"例如exp:hasChild_rdfs:domain_exp:Person,exp:Helen_exp:hasChild_exp:Jack，由第一条规则可以推出_exp:Helen_rdf:type_exp:Person。",exp:hasChild_rdfs:domain_exp:Person,知识图谱,,exp:Helen,
"例如exp:hasChild_rdfs:domain_exp:Person,exp:Helen_exp:hasChild_exp:Jack，由第一条规则可以推出_exp:Helen_rdf:type_exp:Person。",exp:hasChild_rdfs:domain_exp:Person,知识图谱,,exp:Helen_exp:hasChild_exp:Jack,
"例如exp:hasChild_rdfs:domain_exp:Person,exp:Helen_exp:hasChild_exp:Jack，由第一条规则可以推出_exp:Helen_rdf:type_exp:Person。",exp:Helen,知识图谱,,英文名,
OWL_2_RL允许的核心词汇有：●rdfs:subClassOf；●rdfs:subPropertyOf；●rdfs:domain；●rdfs:range；●owl:TransitiveProperty；●owl:FunctionalProperty；●owl:sameAs；●owl:equivalentClass；●owl:equivalentProperty；●owl:someValuesFrom；●owl:allValuesFrom。,OWL_2_RL,知识图谱,,OWL_2_RL,
OWL_2_RL的前向链推理复杂度是PTIME完备的，PTIME复杂度是针对实例数据推理得到的结果。,OWL_2_RL的前向链推理复杂度,知识图谱,,PTIME完备,
OWL_2_RL的前向链推理复杂度是PTIME完备的，PTIME复杂度是针对实例数据推理得到的结果。,OWL_2_RL的前向链推理复杂度,知识图谱,,PTIME完备,
2.3.3知识图谱查询语言的表示RDF支持类似数据库的查询语言，叫作SPARQL[1]，它提供了查询RDF数据的标准语法、处理SPARQL查询的规则以及结果返回形式。,SPARQL,知识图谱,,查询RDF数据,
2.3.3知识图谱查询语言的表示RDF支持类似数据库的查询语言，叫作SPARQL[1]，它提供了查询RDF数据的标准语法、处理SPARQL查询的规则以及结果返回形式。,SPARQL,知识图谱,,查询RDF数据的标准语法,
1.SPARQL知识图谱查询基本构成●变量，RDF中的资源，以“?”或者“$”指示；●三元组模板，在WHERE子句中列出关联的三元组模板，之所以称为模板，因为三元组中允许存在变量；●SELECT子句中指示要查询的目标变量。,SPARQL知识图谱查询,知识图谱,,变量、RDF中的资源,
1.SPARQL知识图谱查询基本构成●变量，RDF中的资源，以“?”或者“$”指示；●三元组模板，在WHERE子句中列出关联的三元组模板，之所以称为模板，因为三元组中允许存在变量；●SELECT子句中指示要查询的目标变量。,SPARQL知识图谱查询,知识图谱,,SPARQL knowledge graph query,
下面是一个简单的SPARQL查询例子：这个SPARQL查询指的是查询所有选修CS328课程的学生，PREFIX部分进行命名空间的声明，使得下面查询的书写更为简洁。,SPARQL查询,知识图谱,,查询所有选修CS328课程的学生,
下面是一个简单的SPARQL查询例子：这个SPARQL查询指的是查询所有选修CS328课程的学生，PREFIX部分进行命名空间的声明，使得下面查询的书写更为简洁。,SPARQL查询,知识图谱,,查询所有选修CS328课程的学生,
2.常见的SPARQL查询算子（1）OPTIONAL。,SPARQL查询算子,知识图谱,,常见的,
2.常见的SPARQL查询算子（1）OPTIONAL。,SPARQL查询算子,知识图谱,,OPTIONAL,
可选算子，指的是在这个算子覆盖范围的查询语句是可选的。,可选算子,知识图谱,,在这个算子覆盖范围的查询语句是可选的,
例如：指的是查询所有选修CS328课程的学生姓名，以及他们的邮箱。,查询所有选修CS328课程的学生姓名,知识图谱,,查询学生姓名,
例如：指的是查询所有选修CS328课程的学生姓名，以及他们的邮箱。,查询所有选修CS328课程的学生姓名,知识图谱,,查询学生的邮箱,
OPTIONAL关键字指示如果没有邮箱，则依然返回学生姓名，邮箱处空缺。,OPTIONAL关键字,知识图谱,,没有邮箱则依然返回学生姓名，邮箱处空缺,
OPTIONAL关键字指示如果没有邮箱，则依然返回学生姓名，邮箱处空缺。,OPTIONAL,知识图谱,,等价,
（2）FILTER。,FILTER,知识图谱,,SPO,
过滤算子，指的是这个算子覆盖范围的查询语句可以用来过滤查询结果。,过滤算子,知识图谱,,覆盖范围的查询语句可以用来过滤查询结果,
例如：指的是查询学生姓名、选修课程以及他们的年龄；如果有年龄，则年龄必须大于25岁。,spo,知识图谱,,查询学生姓名、选修课程以及他们的年龄,
（3）UNION。,UNION,知识图谱,,SPO,
并算子，指的是将两个查询的结果合并起来。,并算子,知识图谱,,将两个查询的结果合并起来,
例如：指的是查询选修课程CS328或CS909的学生姓名以及邮件。,SPO,知识图谱,,查询选修课程CS328或CS909的学生姓名以及邮件,
注意，这里的邮件是必须返回的，如果没有邮件值，则不返回这条记录。,邮件,知识图谱,,返回记录,
注意，这里的邮件是必须返回的，如果没有邮件值，则不返回这条记录。,邮件,知识图谱,,email,
需要注意UNION和OPTIONAL的区别。,UNION,知识图谱,,集合运算,
下面给出一个SPARQL查询的例子。,SPARQL查询,知识图谱,,SPO,
给定一个RDF数据集：以及一个SPARQL查询：这个SPARQL查询期望查询所有的收购关系，可以得到查询结果如表2-8所示。,SPARQL查询,知识图谱,,查询所有的收购关系,
给定一个RDF数据集：以及一个SPARQL查询：这个SPARQL查询期望查询所有的收购关系，可以得到查询结果如表2-8所示。,等价,知识图谱,,equivalence,
表2-8查询结果给定论文一个SPARQL查询：这个查询期望查询所有具备关联交易的公司。,查询,知识图谱,,SPARQL查询,
"假设有下面两条规则：hold_share（X,Y）:-control（X,Y）conn_trans（Y,Z）:-hold_share（X,Y）,hold_share（X,Z）第一条规则指的是如果X控制了Y，那么X控股Y；第二条规则指的是如果X同时控股Y和Z，那么Y和Z具备关联交易。",hold_share,知识图谱,,规则,
"假设有下面两条规则：hold_share（X,Y）:-control（X,Y）conn_trans（Y,Z）:-hold_share（X,Y）,hold_share（X,Z）第一条规则指的是如果X控制了Y，那么X控股Y；第二条规则指的是如果X同时控股Y和Z，那么Y和Z具备关联交易。",等价,知识图谱,,"if X controls Y, then X holds Y",
"假设有下面两条规则：hold_share（X,Y）:-control（X,Y）conn_trans（Y,Z）:-hold_share（X,Y）,hold_share（X,Z）第一条规则指的是如果X控制了Y，那么X控股Y；第二条规则指的是如果X同时控股Y和Z，那么Y和Z具备关联交易。",等价,知识图谱,,如果X同时控股Y和Z，那么Y和Z具备关联交易。,
通过查询重写技术，可以得到下面的SPARQL查询：但是这个查询比较复杂，可以通过下面的SPARQL查询简化：在这个查询中，SPARQL允许嵌套查询，即WHERE子句中包含SELECT子句。,SPARQL查询,知识图谱,,嵌套查询,
通过查询重写技术，可以得到下面的SPARQL查询：但是这个查询比较复杂，可以通过下面的SPARQL查询简化：在这个查询中，SPARQL允许嵌套查询，即WHERE子句中包含SELECT子句。,SPARQL查询,知识图谱,,SPARQL查询重写技术,
2.3.4语义Markup表示语言语义网进一步定义了在网页中嵌入语义Markup的方法和表示语言。,语义Markup表示语言,知识图谱,,在网页中嵌入语义Markup的方法和表示语言,
被谷歌知识图谱以及Schema.Org采用的语义Markup语言主要包括JSON-LD、RDFa和HTML5MicroData。,Schema.Org,知识图谱,,语义Markup语言,
1.JSON-LDJSON-LD（JavaScript_Object_Notation_for_Linked_Data）是一种基于JSON表示和传输链接数据的方法。,JSON-LD,知识图谱,,基于JSON表示和传输链接数据的方法,
1.JSON-LDJSON-LD（JavaScript_Object_Notation_for_Linked_Data）是一种基于JSON表示和传输链接数据的方法。,JSON-LD,知识图谱,,JavaScript_Object_Notation_for_Linked_Data,
JSON-LD描述了如何通过JSON表示有向图，以及如何在一个文档中混合表示链接数据及非链接数据。,JSON-LD,知识图谱,,通过JSON表示有向图,
JSON-LD描述了如何通过JSON表示有向图，以及如何在一个文档中混合表示链接数据及非链接数据。,JSON-LD,知识图谱,,在一个文档中混合表示链接数据及非链接数据,
JSON-LD的语法和JSON兼容。,JSON-LD,知识图谱,,JSON,
下面是一个简单的JSON例子：JSON文档表示一个人。,JSON,知识图谱,,一个人,
人们很容易推断这里的含义：“name”是人的名字，“homepage”是其主页，“image”是其某种照片。,name,知识图谱,,人的名字,
人们很容易推断这里的含义：“name”是人的名字，“homepage”是其主页，“image”是其某种照片。,name,知识图谱,,人的名字,
人们很容易推断这里的含义：“name”是人的名字，“homepage”是其主页，“image”是其某种照片。,homepage,知识图谱,,主页,
人们很容易推断这里的含义：“name”是人的名字，“homepage”是其主页，“image”是其某种照片。,image,知识图谱,,某种照片,
当然，机器不理解“name”和“image”这样的术语。,语义网,知识图谱,,属于本体,
JSON-LD通过引入规范的术语表示，例如统一化表示“name”、“homepage”和“image”的URI，使得数据交换和机器理解成为基础。,JSON-LD,知识图谱,,统一的术语表示,
如下所示：可以看出，JSON-LD呈现出语义网技术的风格，它们有着类似的目标：围绕某类知识提供共享的术语。,JSON-LD,知识图谱,,围绕某类知识提供共享的术语,
如下所示：可以看出，JSON-LD呈现出语义网技术的风格，它们有着类似的目标：围绕某类知识提供共享的术语。,语义网技术,知识图谱,,semantic web technology,
例如，每个数据集不应该围绕“name”重复发明概念。,数据集,知识图谱,,“不应该围绕“name”重复发明概念”,
但是，JSON-LD的实现没有选择大部分语义网技术栈（TURTLE/SPARQL/Quad单、不复杂以及面向一般开发人员的方式推进。,JSON-LD的实现,知识图谱,,没有选择大部分语义网技术栈（TURTLE/SPARQL/Quad单、不复杂以及面向一般开发人员的方式推进。,
Stores），而是以简2.RDFaRDFa（Resource_Description_Framework_in_attributes）是一种早期网页语义标记语言。,RDFa,知识图谱,,早期网页语义标记语言,
Stores），而是以简2.RDFaRDFa（Resource_Description_Framework_in_attributes）是一种早期网页语义标记语言。,RDFa,知识图谱,,Resource_Description_Framework_in_attributes,
RDFa也是W3C推荐标准。,RDFa,知识图谱,,W3C推荐标准,
它扩充了XHTML的几个属性，网页制作者可以利用这些属性在网页中添加可供机器读取的资源。,XML,知识图谱,,扩充XHTML的几个属性,
与RDF的对应关系使得RDFa可以将RDF的三元组嵌入在XHTML文档中，它也使得符合标准的使用端可以从RDFa文件中提取出这些RDF三元组。,RDFa,知识图谱,,将RDF的三元组嵌入在XHTML文档中,
与RDF的对应关系使得RDFa可以将RDF的三元组嵌入在XHTML文档中，它也使得符合标准的使用端可以从RDFa文件中提取出这些RDF三元组。,RDFa,知识图谱,,符合标准的使用端可以从RDFa文件中提取出这些RDF三元组,
与RDF的对应关系使得RDFa可以将RDF的三元组嵌入在XHTML文档中，它也使得符合标准的使用端可以从RDFa文件中提取出这些RDF三元组。,RDFa,知识图谱,,将RDF的三元组嵌入在XHTML文档中,
与RDF的对应关系使得RDFa可以将RDF的三元组嵌入在XHTML文档中，它也使得符合标准的使用端可以从RDFa文件中提取出这些RDF三元组。,RDFa,知识图谱,,符合标准的使用端可以从RDFa文件中提取出这些RDF三元组,
RDFa通过引入名字空间的方法，在已有的标签中加入RDFa相应的属性，以便解析支持RDFa技术的浏览器或者搜索引擎，从而达到优化的目的。,RDFa,知识图谱,,在已有的标签中加入RDFa相应的属性，以便解析支持RDFa技术的浏览器或者搜索引擎，从而达到优化的目的。,
上面的代码示例中用到了RDFa属性中的about属性和property属性。,about,知识图谱,,RDFa属性,
这段代码示例说明了一篇文章，然后描述了和这篇文章相关的信息，例如标题、创建者和创建日期，就可以让支持RDFa的机器识别这些属性。,代码示例,知识图谱,,一篇文章,
这段代码示例说明了一篇文章，然后描述了和这篇文章相关的信息，例如标题、创建者和创建日期，就可以让支持RDFa的机器识别这些属性。,一段代码示例,知识图谱,,一段代码示例说明了一篇文章,
RDFa可以从机器可理解的层面优化搜索，提升访问体验以及网页数据的关联。,RDFa,知识图谱,,优化搜索，提升访问体验以及网页数据的关联,
3.HTML5_MicrodataMicrodata（微数据）是在网页标记语言中嵌入机器可读的属性数据。,HTML5_Microdata,知识图谱,,在网页标记语言中嵌入机器可读的属性数据,
微数据使用自定义词汇表、带作用域的键值对给DOM做标记，用户可以自定义微数据词汇表，在自己的网页中嵌入自定义的属性。,微数据,知识图谱,,自定义词汇表、带作用域的键值对给DOM做标记,
微数据使用自定义词汇表、带作用域的键值对给DOM做标记，用户可以自定义微数据词汇表，在自己的网页中嵌入自定义的属性。,微数据,知识图谱,,microdata,
微数据是给那些已经在页面上可见的数据施加额外的语义，当HTML的词汇不够用时，使用微数据可以取得较好的效果。,微数据,知识图谱,,给那些已经在页面上可见的数据施加额外的语义,
下面是一个HTML5Microdata的示例。,HTML5Microdata,知识图谱,,HTML5的语义化数据,
这个例子给出了Person类下一个叫Andy的人的照片和URL地址。,Person,知识图谱,,一个叫Andy的人,
通过HTML5Microdata，浏览器可以很方便地从网页上提取微数据实体、属性及属性值。,HTML5Microdata,知识图谱,,从网页上提取微数据实体、属性及属性值,
2.4常见开放域知识图谱的知识表示方法不同的知识图谱项目都会根据实际的需要选择不同的知识表示框架。,知识图谱,知识图谱,,不同的知识图谱项目都会根据实际的需要选择不同的知识表示框架,
这些框架有不同的描述术语、表达能力、数据格式等方面的考虑，但本质上有相似之处。,框架,知识图谱,,描述术语、表达能力、数据格式等方面的考虑,
这些框架有不同的描述术语、表达能力、数据格式等方面的考虑，但本质上有相似之处。,框架,知识图谱,,frame,
这里以三个最典型的开放域知识图谱（Freebase、Wikidata、ConceptNet）为例，尝试比较不同的知识图谱项目选用的知识表示框架，并总结影响知识表示框架选择的主要因素。,知识图谱,知识图谱,,不同的知识图谱项目选用的知识表示框架,
为便于比较分析，以RDF、OWL的描述术语和表达能力为主要比较对象。,为便于比较分析,知识图谱,,RDF、OWL的描述术语和表达能力,
2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。,FreebaseFreebase的知识表示框架,知识图谱,,对象-Object,
2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。,FreebaseFreebase的知识表示框架,知识图谱,,事实-Facts,
2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。,FreebaseFreebase的知识表示框架,知识图谱,,类型-ID,
2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。,FreebaseFreebase的知识表示框架,知识图谱,,MIDTypes,
2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。,FreebaseFreebase的知识表示框架,知识图谱,,属性-Properties,
2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。,Freebase,知识图谱,,Freebase的知识表示框架,
“Object”代表实体。,“Object”,知识图谱,,实体,
每一个“Object”有唯一的（Machine_ID）。,Object,知识图谱,,每一个“Object”,
每一个“Object”有唯一的（Machine_ID）。,Object,知识图谱,,对象,
一个“Object”可以有一个或多个“Types”。,Object,知识图谱,,一个或多个Types,
“Properties”用来描述“Facts”。,“Properties”,知识图谱,,“Facts”,
例如，“Barack_Obama”是一个Object，并拥有一个唯一的MID:“/m/02mjmr”。,Barack_Obama,知识图谱,,Object,
例如，“Barack_Obama”是一个Object，并拥有一个唯一的MID:“/m/02mjmr”。,“Barack_Obama”,知识图谱,,Object,
这个Object是“/government/us_president”，并有一个称的一个为“/government/us_president/presidency_number”的Property，其数值是“44”。,“/government/us_president”,知识图谱,,/government/us_president,
"Freebase使用复合值类型（Compound_Value_Types,CVT）处理多元关系。",Freebase,知识图谱,,复合值类型处理多元关系,
"Freebase使用复合值类型（Compound_Value_Types,CVT）处理多元关系。",复合值类型,知识图谱,,"Compound_Value_Types,CVT",
type如图2-16所示，示例的CVT描述了关于Obama的任职期限的多元关系“government_position_held”。,government_position_held,知识图谱,,CVT的示例,
这个多元关系包含多个子二元关系：“office_holder”“office_position”“from”“to”等。,多元关系,知识图谱,,多个子二元关系,
一个CVT就是有唯一MID的Object，也可以有多个Types。,CVT,知识图谱,,有唯一MID的Object,
一个CVT就是有唯一MID的Object，也可以有多个Types。,CVT,知识图谱,,多个Types,
为了以示区别，Freebase把所有非CVT的Object也称为“Topic”。,Freebase,知识图谱,,Object,
Wikidata起源于Wikipedia，因此与Wikipedia一样，以页面“Page”为基本的组织单元。,Wikidata,知识图谱,,Wikipedia,
Entities类似于OWL:Things，代指最顶层的对象。,Entities,知识图谱,,OWL:Things,
每一个Entity都有一个独立的维基页面。,Entity,知识图谱,,独立的维基页面,
Entities主要有两类：Items和Properties。,Entities,知识图谱,,Items,
Entities主要有两类：Items和Properties。,Entities,知识图谱,,Properties,
Items类似于RDF中的Instance，代指实例对象。,Items,知识图谱,,实例对象,
Properties和Statements分别等价于RDF中的Property和Statement。,Properties,知识图谱,,RDF中的Property,
通常一个Item的页面还包含多个别名-aliases和多个指向Wikipedia的外部链接-Sitelinks。,一个Item的页面,知识图谱,,别名-aliases,
通常一个Item的页面还包含多个别名-aliases和多个指向Wikipedia的外部链接-Sitelinks。,一个Item的页面,知识图谱,,外部链接-Sitelinks,
通常一个Item的页面还包含多个别名-aliases和多个指向Wikipedia的外部链接-Sitelinks。,别名,知识图谱,,aliases,
通常一个Item的页面还包含多个别名-aliases和多个指向Wikipedia的外部链接-Sitelinks。,外部链接,知识图谱,,Sitelinks,
一个Statement包含一个Property、一个或多个Values、一个或多个Qualifiers、一个或多个References、一个标识重要性程度的Rank。,Statement,知识图谱,,一个Property、一个或多个Values、一个或多个Qualifiers、一个或多个References、一个标识重要性程度的Rank,
一个Statement包含一个Property、一个或多个Values、一个或多个Qualifiers、一个或多个References、一个标识重要性程度的Rank。,Statement,知识图谱,,一个Statement包含一个Property、一个或多个Values、一个或多个Qualifiers、一个或多个References、一个标识重要性程度的Rank,
修饰-Qualifiers用于处理复杂的多元表示。,Qualifiers,知识图谱,,用于处理复杂的多元表示,
如一个陈述“spouse:Jane_Belson”描述了一个二元关系。,spo,知识图谱,,三元组,
如一个陈述“spouse:Jane_Belson”描述了一个二元关系。,等价,知识图谱,,等价,
可以使用Qualifiers给这个陈述增加多个附加信息来刻画多元关系，如“startdate:25_November_1991”and“end_date:11_May_2011”等。,Qualifiers,知识图谱,,刻画多元关系,
可以使用Qualifiers给这个陈述增加多个附加信息来刻画多元关系，如“startdate:25_November_1991”and“end_date:11_May_2011”等。,Qualifiers,知识图谱,,给这个陈述增加多个附加信息来刻画多元关系,
引用-References用于标识每个陈述的来源或出处，如来源于某个维基百科页面等。,引用,知识图谱,,标识每个陈述的来源或出处,
引用也是一种Qualifiers，通常添加到Statements的附加信息中。,引用,知识图谱,,Qualifiers,
Wikidata支持多种数值类型，包括其自有的Item类型、RDF_Literal、URL、媒体类型Commons_Media，以及Time、Globe_coordinates和Quantity三种复杂类型。,Wikidata,知识图谱,,多种数值类型,
Wikidata支持多种数值类型，包括其自有的Item类型、RDF_Literal、URL、媒体类型Commons_Media，以及Time、Globe_coordinates和Quantity三种复杂类型。,Wikidata,知识图谱,,Wikidata,
Wikidata允许给每个Statement增加三种权重：normal（缺省）、preferred和deprecated。,Wikidata,知识图谱,,给每个Statement增加三种权重,
Wikidata允许给每个Statement增加三种权重：normal（缺省）、preferred和deprecated。,Wikidata,知识图谱,,Wikidata,
Wikidata定义了三种Snacks作为Statement的具体描述结构：PropertyValueSnack、PropertyNoValueSnack、PropertySomeValueSnack。,Wikidata,知识图谱,,定义三种Snacks,
Wikidata定义了三种Snacks作为Statement的具体描述结构：PropertyValueSnack、PropertyNoValueSnack、PropertySomeValueSnack。,PropertyValueSnack,知识图谱,,属性值小吃,
Wikidata定义了三种Snacks作为Statement的具体描述结构：PropertyValueSnack、PropertyNoValueSnack、PropertySomeValueSnack。,PropertyNoValueSnack,知识图谱,,属性无值小吃,
Wikidata定义了三种Snacks作为Statement的具体描述结构：PropertyValueSnack、PropertyNoValueSnack、PropertySomeValueSnack。,PropertySomeValueSnack,知识图谱,,属性有些值小吃,
PropertyNoValueSnack类似于OWL中的Negation，表示类似于“Elizabeth_spouse”的知识。,PropertyNoValueSnack,知识图谱,,类似于OWL中的Negation,
"PropertySomeValueSnack类似于OWL中的存在量词someValuesFrom，表示类似于“PopeLinus_had_a_date_of_birth,but_it_is_unknown_to_us”这样的知识。",PropertySomeValueSnack,知识图谱,,类似于OWL中的存在量词someValuesFrom,
"PropertySomeValueSnack类似于OWL中的存在量词someValuesFrom，表示类似于“PopeLinus_had_a_date_of_birth,but_it_is_unknown_to_us”这样的知识。",PropertySomeValueSnack,知识图谱,,someValuesFrom,
of_England_had_no_I_Wikidata的URI机制遵循了Linked_Open_Data的URI原则，采用统一的URI机制：Item，如Q49，或者一个http://www.wikidata.org/entity/<id>。,of_England_had_no_I_Wikidata,知识图谱,,URI机制,
of_England_had_no_I_Wikidata的URI机制遵循了Linked_Open_Data的URI原则，采用统一的URI机制：Item，如Q49，或者一个http://www.wikidata.org/entity/<id>。,of_England_had_no_I_Wikidata,知识图谱,,URI机制,
其中，<id>可以是一个Property，如P234。,P234,知识图谱,,一个Property,
2.4.3ConceptNet5ConceptNet5的知识表示框架主要包含如下要素：概念-Concepts、词-Words、短语-Phrases、断言-Assertions、关系-Relations、边-Edges。,概念-Concepts,知识图谱,,概念概念概念概念,
Concepts由Words或Phrases组成，构成了图谱中的节点。,Concepts,知识图谱,,Words或Phrases,
Concepts由Words或Phrases组成，构成了图谱中的节点。,Concepts,知识图谱,,Words或Phrases,
与其他知识图谱的节点不同，这些Concepts通常是从自然语言文本中提取出来的，更接近自然语言描述，而不是形式化的命名。,Concepts,知识图谱,,从自然语言文本中提取出来的,
与其他知识图谱的节点不同，这些Concepts通常是从自然语言文本中提取出来的，更接近自然语言描述，而不是形式化的命名。,Concepts,知识图谱,,概念,
Assertions描述了Concepts之间的关系，类似于RDF中的Statements。,Assertions,知识图谱,,Concepts之间的关系,
Assertions描述了Concepts之间的关系，类似于RDF中的Statements。,Assertions,知识图谱,,Concepts之间的关系,
Edges类似于RDF中的Property。,Edges,知识图谱,,RDF中的Property,
一个Concepts包含多条边，而一条边可能有多个产生来源。,一个Concepts,知识图谱,,多条边,
一个Concepts包含多条边，而一条边可能有多个产生来源。,一条边,知识图谱,,多个产生来源,
例如，一个“化妆Cause漂亮”的断言可能来源于文本抽取，也可能来源于用户的手工输入。,“化妆Cause漂亮”的断言,知识图谱,,文本抽取,
例如，一个“化妆Cause漂亮”的断言可能来源于文本抽取，也可能来源于用户的手工输入。,“化妆Cause漂亮”的断言,知识图谱,,用户的手工输入,
来源越多，该断言就越可靠。,断言,知识图谱,,来源越多，该断言就越可靠。,
ConceptNet5根据来源的多少和可靠程度计算每个断言的置信度。,ConceptNet5,知识图谱,,根据来源的多少和可靠程度计算每个断言的置信度。,
ConceptNet5根据来源的多少和可靠程度计算每个断言的置信度。,ConceptNet5,知识图谱,,根据来源的多少和可靠程度计算每个断言的置信度。,
ConceptNet5示例如图2-17所示。,ConceptNet5,知识图谱,,概念/产品,
"ConceptNet5中的关系包含21个预定义的、多语言通用的关系，如IsA、UsedFor等，以及从自然语言文本中抽取的更加接近自然语言描述的非形式化的关系，如of,caused_by等。",ConceptNet5,知识图谱,,21个预定义的、多语言通用的关系,
"ConceptNet5中的关系包含21个预定义的、多语言通用的关系，如IsA、UsedFor等，以及从自然语言文本中抽取的更加接近自然语言描述的非形式化的关系，如of,caused_by等。",ConceptNet5,知识图谱,,ConceptNet 5,
on_top图2-17ConceptNet5示例ConceptNet5对URI进行了精心的设计。,ConceptNet5,知识图谱,,对URI进行了精心的设计,
URI同时考虑了类型（如是概念还是关系）、语言、正则化后的概念名称、词性、歧义等因素。,URI,知识图谱,,考虑了类型（如是概念还是关系）、语言、正则化后的概念名称、词性、歧义等因素。,
URI同时考虑了类型（如是概念还是关系）、语言、正则化后的概念名称、词性、歧义等因素。,URI,知识图谱,,Uniform Resource Identifier,
其中，n代指这是一个名词，basement用于区分歧义。,n,知识图谱,,一个名词,
其中，n代指这是一个名词，basement用于区分歧义。,n,知识图谱,,一个名词,
在处理表示“x_is_the_first_argument_of_y”这类多元关系的问题上，ConceptNet5把所有关于某条边的附加信息增加为边的属性，如图2-18所示。,ConceptNet5,知识图谱,,处理表示“x_is_the_first_argument_of_y”这类多元关系的问题,
在处理表示“x_is_the_first_argument_of_y”这类多元关系的问题上，ConceptNet5把所有关于某条边的附加信息增加为边的属性，如图2-18所示。,边的附加信息,知识图谱,,边的属性,
图2-18ConceptNet5的知识表示结构2.5知识图谱的向量表示方法与前面所述的表示方法不同的是，本节要描述的方法是把知识图谱中的实体和关系映射到低维连续的向量空间，而不是使用基于离散符号的表达方式。,ConceptNet5,知识图谱,,概念语义网络的五大版本,
2.5.1知识图谱表示的挑战在前面提到的一些知识图谱的表示方法中，其基础大多是以三元组的方法对知识进行组织。,知识图谱的表示方法,知识图谱,,三元组的方法对知识进行组织,
2.5.1知识图谱表示的挑战在前面提到的一些知识图谱的表示方法中，其基础大多是以三元组的方法对知识进行组织。,知识图谱的表示方法,知识图谱,,三元组的方法对知识进行组织,
在具体的知识库网络中，节点对应着三元组的头实体和尾实体，边对应着三元组的关系。,节点,知识图谱,,在具体的知识库网络中，节点对应着三元组的头实体和尾实体，边对应着三元组的关系,
虽然这种离散的符号化的表达方式可以非常有效地将数据结构化，但是在当前的大规模应用上也面临着巨大的挑战。,离散的符号化的表达方式,知识图谱,,将数据结构化,
知识以基于离散符号的方法进行表达，但这些符号并不能在计算机中表达相应语义层面的信息，也不能进行语义计算，对下游的一些应用并不友好。,知识,知识图谱,,基于离散符号的方法进行表达,
数据具有一定的稀疏性，现实中的知识图谱无论是实体还是关系都有长尾分布的情况，也就是某一个实体或关系具有极少的实例样本，这种现象会影响某些应用的准确率。,长尾分布,知识图谱,,long tail distribution,
从上面的问题可以看出，对于当前的数据量较大的知识图谱、变化各异的应用来说，需要改进传统的表示方法。,知识图谱,知识图谱,,改进传统的表示方法,
2.5.2词的向量表示方法在介绍有关知识图谱的向量表示方法之前，在此先介绍词的表示方法。,词的向量表示方法,知识图谱,,"介绍有关知识图谱的向量表示方法之前,在此先介绍词的表示方法。",
2.5.2词的向量表示方法在介绍有关知识图谱的向量表示方法之前，在此先介绍词的表示方法。,词的向量表示方法,知识图谱,,词的表示方法,
在自然语言处理领域中，因为离散符号化的词语并不能蕴涵语义信息，所以将词映射到向量空间，这不仅有利于进行相应的计算，在映射的过程中也能使相关的向量蕴涵一定的语义。,向量空间映射,知识图谱,,将词映射到向量空间,
在自然语言处理领域中，因为离散符号化的词语并不能蕴涵语义信息，所以将词映射到向量空间，这不仅有利于进行相应的计算，在映射的过程中也能使相关的向量蕴涵一定的语义。,向量空间,知识图谱,,vector space,
知识图谱中的向量表示方法也在此次有所借鉴。,知识图谱中的向量表示方法,知识图谱,,此次,
1.独热编码传统的独热编码（One-Hot_Encoding）方法是将一个词表示成一个很长的向量，该向量的维度是整个词表的大小。,向量的维度,知识图谱,,传统的独热编码（One-Hot_Encoding）方法,
1.独热编码传统的独热编码（One-Hot_Encoding）方法是将一个词表示成一个很长的向量，该向量的维度是整个词表的大小。,向量的维度,知识图谱,,整个词表的大小,
对于某一个具体的词，在其独热表示的向量中，除了表示该词编号的维度为1，其余都为0。,向量的向量,知识图谱,,独热表示的向量,
对于某一个具体的词，在其独热表示的向量中，除了表示该词编号的维度为1，其余都为0。,向量的向量,知识图谱,,独热表示的向量,
如图2-19所示，假如词Rome的编号为1，则在其独热编码中，仅有维度1是1，其余都是0。,词Rome,知识图谱,,独热编码,
如图2-19所示，假如词Rome的编号为1，则在其独热编码中，仅有维度1是1，其余都是0。,独热编码,知识图谱,,one-hot-encoding,
这种表示方法虽然简单，但是可以看出其并没有编码语义层面的信息，稀疏性非常强，当整个词典非常大时，编码出向量的维度也会很大。,向量的维度,知识图谱,,稀疏性非常强,
这种表示方法虽然简单，但是可以看出其并没有编码语义层面的信息，稀疏性非常强，当整个词典非常大时，编码出向量的维度也会很大。,向量的维度,知识图谱,,编码出向量的维度,
"2.词袋模型词袋模型（Bag-of-Words,BoW）是一种对文本中词的表示方法。",词袋模型,知识图谱,,对文本中词的表示方法,
"2.词袋模型词袋模型（Bag-of-Words,BoW）是一种对文本中词的表示方法。",词袋模型,知识图谱,,"Bag-of-Words,BoW",
该方法将文本想象成一个装词的袋子，不考虑词之间的上下文关系，不关心词在袋子中存放的顺序，仅记录每个词在该文本（词袋）中出现的次数。,文本向量,知识图谱,,词袋法,
该方法将文本想象成一个装词的袋子，不考虑词之间的上下文关系，不关心词在袋子中存放的顺序，仅记录每个词在该文本（词袋）中出现的次数。,文本向量,知识图谱,,text vector,
具体的方法是先收集所有文本的可见词汇并组成一个词典，再对所有词进行编号，对于每个文本，可以使用一个表示每个词出现次数的向量来表示，该向量的每一个维度的数字表示该维度所指代的词在该文本中出现的次数。,向量的维度,知识图谱,,表示每个词出现次数的向量,
具体的方法是先收集所有文本的可见词汇并组成一个词典，再对所有词进行编号，对于每个文本，可以使用一个表示每个词出现次数的向量来表示，该向量的每一个维度的数字表示该维度所指代的词在该文本中出现的次数。,向量,知识图谱,,向量的每一个维度的数字表示该维度所指代的词在该文本中出现的次数,
如图2-20所示，在文本doc1中，Rome出现32次，Paris出现14次，France出现0次。,Rome,知识图谱,,doc1,
如图2-20所示，在文本doc1中，Rome出现32次，Paris出现14次，France出现0次。,Rome,知识图谱,,罗马,
如图2-20所示，在文本doc1中，Rome出现32次，Paris出现14次，France出现0次。,Paris,知识图谱,,巴黎,
图2-19独热编码示例1图2-20独热编码示例23.词向量上面对词的表示方法并没有考虑语义层面的信息，为了更多地表示词与词之间的语义相似程度，提出词的分布式表示，也就是基于上下文的稠密向量表示法，通常称为词向量或词嵌入（Word_Embedding）。,词向量,知识图谱,,基于上下文的稠密向量表示法,
图2-19独热编码示例1图2-20独热编码示例23.词向量上面对词的表示方法并没有考虑语义层面的信息，为了更多地表示词与词之间的语义相似程度，提出词的分布式表示，也就是基于上下文的稠密向量表示法，通常称为词向量或词嵌入（Word_Embedding）。,词向量,知识图谱,,基于上下文的稠密向量表示法,
产生词向量的手段主要有三种：●Count-based。,产生词向量的手段,知识图谱,,Count-based,
基于计数的方法，简单说就是记录文本中词的出现次数。,基于计数的方法,知识图谱,,记录文本中词的出现次数,
●Predictive。,Predictive,知识图谱,,概念/产品,
基于预测的方法，既可以通过上下文预测中心词，也可以通过中心词预测上下文。,基于预测的方法,知识图谱,,通过上下文预测中心词或中心词预测上下文,
基于预测的方法，既可以通过上下文预测中心词，也可以通过中心词预测上下文。,基于预测的方法,知识图谱,,based on prediction method,
●Task-based。,SPO,知识图谱,,Task-based,
基于任务的，也就是通过任务驱动的方法。,基于任务的,知识图谱,,通过任务驱动的方法,
通过对词向量在具体任务上的表现效果对词向量进行学习。,词向量,知识图谱,,具体任务上的表现效果,
对词向量的产生方法到现在为止有较多的研究，在本章中并不展开讨论，下面简单介绍经典的开源工具word2vec[8]中包含的CBoW和Skip-gram两个模型。,word2vec,知识图谱,,word2vec,
CBoW也就是连续词袋模型（Continuous_Bag-of-Words），和之前提到的BoW相似之处在于该模型也不用考虑词序的信息。,CBoW,知识图谱,,连续词袋模型,
CBoW也就是连续词袋模型（Continuous_Bag-of-Words），和之前提到的BoW相似之处在于该模型也不用考虑词序的信息。,CBoW,知识图谱,,连续词袋模型,
其主要思想是，用上下文预测中心词，从而训练出的词向量包含了一定的上下文信息。,上下文预测中心词,知识图谱,,用上下文预测中心词，从而训练出的词向量包含了一定的上下文信息,
其主要思想是，用上下文预测中心词，从而训练出的词向量包含了一定的上下文信息。,上下文预测中心词,知识图谱,,用上下文预测中心词,
"如图2-21（a）所示，其中wn是中心词，wn?2,wn?1,wn+1,wn+2为该中心词的上下文的词。",wn,知识图谱,,中心词,
"如图2-21（a）所示，其中wn是中心词，wn?2,wn?1,wn+1,wn+2为该中心词的上下文的词。",等价,知识图谱,,等价,
将上下文词的独热表示与词向量矩阵E相乘，提取相应的词向量并求和得到投影层，然后再经过一个Softmax层最终得到输出，输出的每一维表达的就是词表中每个词作为该上下文的中心词的概率。,投影层,知识图谱,,将上下文词的独热表示与词向量矩阵E相乘,
将上下文词的独热表示与词向量矩阵E相乘，提取相应的词向量并求和得到投影层，然后再经过一个Softmax层最终得到输出，输出的每一维表达的就是词表中每个词作为该上下文的中心词的概率。,投影层,知识图谱,,projection layer,
将上下文词的独热表示与词向量矩阵E相乘，提取相应的词向量并求和得到投影层，然后再经过一个Softmax层最终得到输出，输出的每一维表达的就是词表中每个词作为该上下文的中心词的概率。,输出,知识图谱,,output,
整个模型在训练的过程就像是一个窗口在训练语料上进行滑动，所以被称为连续词袋模型。,连续词袋模型,知识图谱,,窗口在训练语料上进行滑动,
整个模型在训练的过程就像是一个窗口在训练语料上进行滑动，所以被称为连续词袋模型。,连续词袋模型,知识图谱,,continuous bag of words model,
Skip-gram的思想与CBoW恰恰相反，其考虑用中心词来预测上下文词。,Skip-gram,知识图谱,,用中心词来预测上下文词,
Skip-gram的思想与CBoW恰恰相反，其考虑用中心词来预测上下文词。,Skip-gram,知识图谱,,Skip-gram的思想与CBoW恰恰相反，其考虑用中心词来预测上下文词。,
如图2-21（b）所示，先通过中心词的独热表示从词向量矩阵中得到中心词的词向量得到投影层，然后经过一层Softmax得到输出，输出的每一维中代表某个词作为输入中心词的上下文出现的概率。,向量矩阵中得到中心词的词向量,知识图谱,,投影层,
如图2-21（b）所示，先通过中心词的独热表示从词向量矩阵中得到中心词的词向量得到投影层，然后经过一层Softmax得到输出，输出的每一维中代表某个词作为输入中心词的上下文出现的概率。,投影层,知识图谱,,projection layer,
图2-21CBoW模型在训练好的词向量中可以发现一些词的词向量在连续空间中的一些关系，如图2-22所示。,CBoW模型,知识图谱,,在训练好的词向量中可以发现一些词的词向量在连续空间中的一些关系,
图2-21CBoW模型在训练好的词向量中可以发现一些词的词向量在连续空间中的一些关系，如图2-22所示。,CBoW,知识图谱,,连续空间中的一些关系,
vec（Rome）?vec（Italy）≈vec（Paris）?vec（France）可以看出，Roma和Italy之间有is-capital-of的关系，而这种关系恰好也在Paris和France之间出现。,Roma,知识图谱,,Rome,
vec（Rome）?vec（Italy）≈vec（Paris）?vec（France）可以看出，Roma和Italy之间有is-capital-of的关系，而这种关系恰好也在Paris和France之间出现。,Italy,知识图谱,,is-capital-of,
vec（Rome）?vec（Italy）≈vec（Paris）?vec（France）可以看出，Roma和Italy之间有is-capital-of的关系，而这种关系恰好也在Paris和France之间出现。,Paris,知识图谱,,France,
通过两对在语义上关系相同的词向量相减可以得出相近的结果，可以猜想出Roma和Italy的词向量通过简单的相减运算，得到了一种类似is-capital-of关系的连续向量，而这种关系的向量可以近似地平移到其他具有类似关系的两个词向量之间。,Roma,知识图谱,,is-capital-of关系,
通过两对在语义上关系相同的词向量相减可以得出相近的结果，可以猜想出Roma和Italy的词向量通过简单的相减运算，得到了一种类似is-capital-of关系的连续向量，而这种关系的向量可以近似地平移到其他具有类似关系的两个词向量之间。,Roma,知识图谱,,意大利,
这也说明了经过训练带有一定语义层面信息的词向量具有一定的空间平移性。,向量,知识图谱,,空间平移性,
这也说明了经过训练带有一定语义层面信息的词向量具有一定的空间平移性。,空间平移性,知识图谱,,spatial shiftness,
"图2-22词向量在连续空间中的关系上面所说的两个词之间的关系，恰好可以简单地理解成知识图谱中的关系（relation）、（Rome,is-capital-of,Italy）和（Paris,is-capital-of,France），可以看作是知识图谱中的三元组（triple），这对知识图谱的向量表示产生了一定的启发。",图2-22词向量在连续空间中的关系,知识图谱,,知识图谱中的关系,
"图2-22词向量在连续空间中的关系上面所说的两个词之间的关系，恰好可以简单地理解成知识图谱中的关系（relation）、（Rome,is-capital-of,Italy）和（Paris,is-capital-of,France），可以看作是知识图谱中的三元组（triple），这对知识图谱的向量表示产生了一定的启发。",关系,知识图谱,,relation,
"图2-22词向量在连续空间中的关系上面所说的两个词之间的关系，恰好可以简单地理解成知识图谱中的关系（relation）、（Rome,is-capital-of,Italy）和（Paris,is-capital-of,France），可以看作是知识图谱中的三元组（triple），这对知识图谱的向量表示产生了一定的启发。",三元组,知识图谱,,triple,
2.5.3知识图谱嵌入的概念为了解决前面提到的知识图谱表示的挑战，在词向量的启发下，研究者考虑如何将知识图谱中的实体和关系映射到连续的向量空间，并包含一些语义层面的信息，可以使得在下游任务中更加方便地操作知识图谱，例如问答任务[9]、关系抽取[10]等。,知识图谱嵌入的概念,知识图谱,,解决前面提到的知识图谱表示的挑战,
2.5.3知识图谱嵌入的概念为了解决前面提到的知识图谱表示的挑战，在词向量的启发下，研究者考虑如何将知识图谱中的实体和关系映射到连续的向量空间，并包含一些语义层面的信息，可以使得在下游任务中更加方便地操作知识图谱，例如问答任务[9]、关系抽取[10]等。,知识图谱嵌入的概念,知识图谱,,embedding of knowledge graph,
对于计算机来说，连续向量的表达可以蕴涵更多的语义，更容易被计算机理解和操作。,连续向量的表达,知识图谱,,计算机中语义的表达,
把这种将知识图谱中包括实体和关系的内容映射到连续向量空间方法的研究领域称为知识图谱嵌入（Knowledge_Graph（Representation_Learning）、知识表示学习。,知识图谱嵌入,知识图谱,,知识表示学习,
把这种将知识图谱中包括实体和关系的内容映射到连续向量空间方法的研究领域称为知识图谱嵌入（Knowledge_Graph（Representation_Learning）、知识表示学习。,知识图谱嵌入,知识图谱,,Knowledge_Graph（Representation_Learning）,
Embedding）、知识图谱的向量表示、知识图谱的表示学习类似于词向量，知识图谱嵌入也是通过机器学习的方法对模型进行学习，与独热编码、词袋模型的最大区别在于，知识图谱嵌入方法的训练需要基于监督学习。,知识图谱嵌入,知识图谱,,机器学习的方法对模型进行学习,
Embedding）、知识图谱的向量表示、知识图谱的表示学习类似于词向量，知识图谱嵌入也是通过机器学习的方法对模型进行学习，与独热编码、词袋模型的最大区别在于，知识图谱嵌入方法的训练需要基于监督学习。,知识图谱嵌入,知识图谱,,knowledge graph embedding,
在训练的过程中，可以学习一定的语义层信息，词向量具有的空间平移性也简单地说明了这点。,词向量,知识图谱,,学习一定的语义层信息,
"类似于词向量，经典的知识图谱嵌入模型TransE的设计思想就是，如果一个三元组（h,r,t）成立，那么它们需要符合h+r≈t关系，例如：vec（Rome）+vec（is?capital?of）≈vec（Italy）所以，在知识图谱嵌入的学习过程中，不同的模型从不同的角度把相应的语义信息嵌入知识图谱的向量表示中，如图2-23所示。",TransE,知识图谱,,transE,
图2-23语义信息嵌入知识图谱的向量表示中2.5.4知识图谱嵌入的优点研究者将目光从传统的知识图谱表示方法转移到知识图谱的嵌入方法，是因为与之前的方法相比，用向量表达实体和关系的知识图谱嵌入方法有很多优点。,知识图谱嵌入的优点,知识图谱,,用向量表达实体和关系的知识图谱嵌入方法,
使用向量的表达方式可以提高应用时的计算效率，当把知识图谱的内容映射到向量空间时，相应的算法可以使用数值计算，所以计算的效率也会同时提高。,向量的表达方式,知识图谱,,提高应用时的计算效率,
使用向量的表达方式可以提高应用时的计算效率，当把知识图谱的内容映射到向量空间时，相应的算法可以使用数值计算，所以计算的效率也会同时提高。,向量的表达方式,知识图谱,,向量空间,
用向量表示后，知识图谱将更加适用于当前流行的机器学习算法，例如神经网络等方法。,知识图谱,知识图谱,,向量表示,
因为下游应用输入的并不再是符号，所以可以考虑的方法也不会仅局限于图算法。,图算法,知识图谱,,下游应用输入,
因为下游应用输入的并不再是符号，所以可以考虑的方法也不会仅局限于图算法。,下游应用,知识图谱,,input,
将知识图谱嵌入作为下游应用的预训练向量输入，使得输入的信息不再是孤立的不包含语义信息的符号，而是已经经过一次训练，并且包含一定信息的向量。,知识图谱嵌入,知识图谱,,下游应用的预训练向量输入,
将知识图谱嵌入作为下游应用的预训练向量输入，使得输入的信息不再是孤立的不包含语义信息的符号，而是已经经过一次训练，并且包含一定信息的向量。,知识图谱嵌入,知识图谱,,knowledge graph embedding,
如上所述，知识图谱的嵌入方法可以提高计算的效率，增加下游应用的多样性，并可以作为预训练，为下游模型提供语义支持，所以对其展开的研究具有很大的应用价值和前景。,知识图谱的嵌入方法,知识图谱,,"提高计算的效率,增加下游应用的多样性,并可以作为预训练,为下游模型提供语义支持,所以对其展开的研究具有很大的应用价值和前景。",
如上所述，知识图谱的嵌入方法可以提高计算的效率，增加下游应用的多样性，并可以作为预训练，为下游模型提供语义支持，所以对其展开的研究具有很大的应用价值和前景。,知识图谱的嵌入方法,知识图谱,,embedding methods of knowledge graph,
2.5.5知识图谱嵌入的主要方法多数知识图谱嵌入模型主要依靠知识图谱中可以直接观察到的信息对模型进行训练，也就是说，根据知识图谱中所有已知的三元组训练模型。,知识图谱嵌入模型,知识图谱,,根据知识图谱中所有已知的三元组训练模型,
对于这类方法，常常只需训练出来的实体表示和矩阵表示满足被用来训练的三元组即可，但是这样的结果往往并不能完全满足所有的下游任务。,用来训练的三元组,知识图谱,,被用来训练,
对于这类方法，常常只需训练出来的实体表示和矩阵表示满足被用来训练的三元组即可，但是这样的结果往往并不能完全满足所有的下游任务。,概念/方法,知识图谱,,等价,
所以，当前也有很多的研究者开始关注怎么利用一些除知识图谱之外的额外信息训练知识图谱嵌入。,知识图谱嵌入,知识图谱,,利用一些除知识图谱之外的额外信息训练知识图谱嵌入,
所以，当前也有很多的研究者开始关注怎么利用一些除知识图谱之外的额外信息训练知识图谱嵌入。,知识图谱嵌入,知识图谱,,knowledge graph embedding,
这些额外的信息包括实体类型（Entity_Types）、关系路径（Relation_Paths）等。,额外的信息,知识图谱,,实体类型,
这些额外的信息包括实体类型（Entity_Types）、关系路径（Relation_Paths）等。,额外的信息,知识图谱,,关系路径,
这些额外的信息包括实体类型（Entity_Types）、关系路径（Relation_Paths）等。,等价,知识图谱,,Entity_Types,
这些额外的信息包括实体类型（Entity_Types）、关系路径（Relation_Paths）等。,等价,知识图谱,,Relation_Paths,
根据有关知识图谱嵌入的综述[11]，将知识图谱嵌入的方法分类介绍如下。,知识图谱嵌入,知识图谱,,将知识图谱嵌入的方法分类,
1.转移距离模型转移距离模型（Translational_Distance_Model）的主要思想是将衡量向量化后的知识图谱中三元组的合理性问题，转化成衡量头实体和尾实体的距离问题。,转移距离模型,知识图谱,,衡量向量化后的知识图谱中三元组的合理性问题,
1.转移距离模型转移距离模型（Translational_Distance_Model）的主要思想是将衡量向量化后的知识图谱中三元组的合理性问题，转化成衡量头实体和尾实体的距离问题。,转移距离模型,知识图谱,,Translational_Distance_Model,
这一方法的重点是如何设计得分函数，得分函数常常被设计成利用关系把头实体转移到尾实体的合理性的函数。,得分函数,知识图谱,,设计得分函数,
这一方法的重点是如何设计得分函数，得分函数常常被设计成利用关系把头实体转移到尾实体的合理性的函数。,得分函数,知识图谱,,evaluate the rationality of transferring the head entity to the tail entity,
受词向量的启发，由词与词在向量空间的语义层面关系，可以拓展到知识图谱中头实体和尾实体在向量空间的关系。,头实体,知识图谱,,词与词在向量空间的语义层面关系,
也就是说，同样可以考虑把知识图谱中的头实体和尾实体映射到向量空间中，且它们之间的联系也可以考虑成三元组中的关系。,头实体,知识图谱,,向量空间,
也就是说，同样可以考虑把知识图谱中的头实体和尾实体映射到向量空间中，且它们之间的联系也可以考虑成三元组中的关系。,头实体,知识图谱,,head entity,
也就是说，同样可以考虑把知识图谱中的头实体和尾实体映射到向量空间中，且它们之间的联系也可以考虑成三元组中的关系。,尾实体,知识图谱,,tail entity,
"TransE[12]便是受到了词向量中平移不变性的启发，在TransE中，把实体和关系都表示为向量，对于某一个具体的关系（head,relation,tail），把关系的向量表示解释成头实体的向量到尾实体的向量的转移向量（Translation_vector）。",TransE,知识图谱,,向量的转移向量,
"TransE[12]便是受到了词向量中平移不变性的启发，在TransE中，把实体和关系都表示为向量，对于某一个具体的关系（head,relation,tail），把关系的向量表示解释成头实体的向量到尾实体的向量的转移向量（Translation_vector）。",TransE,知识图谱,,Translation_vector,
也就是说，如果在一个知识图谱中，某一个三元组成立，则它的实体和关系需要满足关系head+relation≈tail。,三元组,知识图谱,,关系head+relation≈tail,
也就是说，如果在一个知识图谱中，某一个三元组成立，则它的实体和关系需要满足关系head+relation≈tail。,关系head,知识图谱,,关系头,
也就是说，如果在一个知识图谱中，某一个三元组成立，则它的实体和关系需要满足关系head+relation≈tail。,关系tail,知识图谱,,关系尾,
2.语义匹配模型相比于转移距离模型，语义匹配模型（Semantic_Matching_Models），更注重挖掘向量化后的实体和关系的潜在语义。,语义匹配模型,知识图谱,,转移距离模型,
2.语义匹配模型相比于转移距离模型，语义匹配模型（Semantic_Matching_Models），更注重挖掘向量化后的实体和关系的潜在语义。,语义匹配模型,知识图谱,,Semantic_Matching_Models,
该方向的模型主要是RESCAL[13]以及它的延伸模型。,该方向的模型,知识图谱,,RESCAL以及它的延伸模型,
RESCAL模型的核心思想是将整个知识图谱编码为一个三维张量，由这个张量分解出一个核心张量和一个因子矩阵，核心张量中每个二维矩阵切片代表一种关系，因子矩阵中每一行代表一个实体。,核心张量,知识图谱,,将整个知识图谱编码为一个三维张量,
由核心张量和因子矩阵还原的结果被看作对应三元组成立的概率，如果概率大于某个阈值，则对应三元组正确；否则不正确。,由核心张量和因子矩阵还原的结果,知识图谱,,对应三元组成立的概率,
由核心张量和因子矩阵还原的结果被看作对应三元组成立的概率，如果概率大于某个阈值，则对应三元组正确；否则不正确。,对应三元组成立的概率,知识图谱,,对应三元组成立的概率,
其得分函数可以写成DistMul[14]通过限制Mr为对角矩阵简化RESCAL模型，也就是说其限制Mr=diag（r）。,DistMul,知识图谱,,得分函数,
其得分函数可以写成DistMul[14]通过限制Mr为对角矩阵简化RESCAL模型，也就是说其限制Mr=diag（r）。,DistMul,知识图谱,,得分函数,
但因为是对角矩阵，所以存在h?diag（r）t=t?diag（r）h，也就是说这种简化的模型只天然地假设所有关系是对称的，显然这是不合理的。,对角矩阵,知识图谱,,diagonal matrix,
ComplEx[15]模型考虑到复数的乘法不满足交换律，所以在该模型中实体和关系的向量表示不再依赖实数而是放在了复数域，从而其得分函数不具有对称性。,ComplEx模型,知识图谱,,不考虑交换律,
ComplEx[15]模型考虑到复数的乘法不满足交换律，所以在该模型中实体和关系的向量表示不再依赖实数而是放在了复数域，从而其得分函数不具有对称性。,ComplEx模型,知识图谱,,ComplEx模型,
也就是说，对于非对称的关系，将三元组中的头实体和尾实体调换位置后可以得到不同的分数。,非对称的关系,知识图谱,,三元组中的头实体和尾实体调换位置后可以得到不同的分数,
也就是说，对于非对称的关系，将三元组中的头实体和尾实体调换位置后可以得到不同的分数。,非对称的关系,知识图谱,,将三元组中的头实体和尾实体调换位置后可以得到不同的分数,
3.考虑附加信息的模型除了仅仅依靠知识库中的三元组构造知识图谱嵌入的模型，还有一些模型考虑额外的附加信息进行提升。,考虑附加信息的模型,知识图谱,,仅仅依靠知识库中的三元组构造知识图谱嵌入的模型,
3.考虑附加信息的模型除了仅仅依靠知识库中的三元组构造知识图谱嵌入的模型，还有一些模型考虑额外的附加信息进行提升。,考虑附加信息的模型,知识图谱,,考虑附加信息的模型,
实体类型是一种容易考虑的额外信息。,实体类型,知识图谱,,容易考虑的额外信息,
在知识库中，一般会给每个实体设定一定的类别，例如Rome具有city的属性、Italy具有country的属性。,Rome,知识图谱,,city,
在知识库中，一般会给每个实体设定一定的类别，例如Rome具有city的属性、Italy具有country的属性。,Italy,知识图谱,,country,
"最简单的考虑实体类型的方法是在知识图谱中设立类似于IsA这样的可以表示实体属性的关系，例如（Rome,IsA,city）（Italy,IsA,Country）这样的三元组。",IsA,知识图谱,,实体类型的考虑方法,
"最简单的考虑实体类型的方法是在知识图谱中设立类似于IsA这样的可以表示实体属性的关系，例如（Rome,IsA,city）（Italy,IsA,Country）这样的三元组。",等价,知识图谱,,IsA,
当训练知识图谱嵌入的时候，考虑这样的三元组就可以将属性信息考虑到向量表示中。,三元组,知识图谱,,将属性信息考虑到向量表示中,
也有一些方法[16]考虑相同类型的实体需要在向量表示上更加接近。,方法,知识图谱,,考虑相同类型的实体需要在向量表示上更加接近,
"关系路径也可以称为实体之间的多跳关系（Multi-hop_Relationships），一般就是指可以连接两个实体的关系链，例如（Rome,is?capital?of,Italy）（Italy,is?country?of,Europe）.从Rome到Europe的关系路径就是一条is?capital?of→is?country?of关系链。",关系路径,知识图谱,,实体之间的多跳关系,
"关系路径也可以称为实体之间的多跳关系（Multi-hop_Relationships），一般就是指可以连接两个实体的关系链，例如（Rome,is?capital?of,Italy）（Italy,is?country?of,Europe）.从Rome到Europe的关系路径就是一条is?capital?of→is?country?of关系链。",关系路径,知识图谱,,Multi-hop_Relationships,
当前很多方法也尝试考虑关系路径来提升嵌入模型，这里的关键问题是考虑如何用相同的向量表达方式来表达路径。,向量表达方式,知识图谱,,考虑如何用相同的向量表达方式来表达路径,
当前很多方法也尝试考虑关系路径来提升嵌入模型，这里的关键问题是考虑如何用相同的向量表达方式来表达路径。,等价,知识图谱,,等价,
在基于路径的TransE，也就是PTransE[17]中，考虑了相加、相乘和RNN三种用关系表达关系路径的方法：p=r1+r2+?+rlp=r1?r2???rlci=f（W[ci?1;ri]）.在基于RNN的方法中，令c1=r1并且一直遍历路径中的关系，直到最终p=cn。,相加,知识图谱,,p=r1+r2+?+rl,
在基于路径的TransE，也就是PTransE[17]中，考虑了相加、相乘和RNN三种用关系表达关系路径的方法：p=r1+r2+?+rlp=r1?r2???rlci=f（W[ci?1;ri]）.在基于RNN的方法中，令c1=r1并且一直遍历路径中的关系，直到最终p=cn。,相乘,知识图谱,,p=r1?r2???rl,
在基于路径的TransE，也就是PTransE[17]中，考虑了相加、相乘和RNN三种用关系表达关系路径的方法：p=r1+r2+?+rlp=r1?r2???rlci=f（W[ci?1;ri]）.在基于RNN的方法中，令c1=r1并且一直遍历路径中的关系，直到最终p=cn。,RNN,知识图谱,,c=f（W[ci?1;ri]）.在基于RNN的方法中，令c1=r1并且一直遍历路径中的关系，直到最终p=cn。,
对于某一个知识库中存在的三元组，其两个实体间的关系路径p需要和原本两个实体间关系的向量表示相接近。,知识库中存在的三元组,知识图谱,,两个实体间的关系路径p,
对于某一个知识库中存在的三元组，其两个实体间的关系路径p需要和原本两个实体间关系的向量表示相接近。,关系路径,知识图谱,,p需要和原本两个实体间关系的向量表示相接近,
文本描述（Textual_Descriptions）指的是在一些知识图谱中，对实体有一些简要的文本描述，如图2-24所示，这些描述本身具有一定的语义信息，对提高嵌入的质量有一定的提升。,文本描述,知识图谱,,对实体有一些简要的文本描述,
除了某些知识库本身具有的文本描述，也可以使用外部的文本信息和语料库。,知识问答系统,知识图谱,,外部的文本信息和语料库,
除了某些知识库本身具有的文本描述，也可以使用外部的文本信息和语料库。,知识库,知识图谱,,knowledge base,
Wang[18]提出了一种在知识图谱嵌入的过程中使用文本信息的联合模型，该模型分三个部分：知识模型、文本模型和对齐模型。,Wang[18],知识图谱,,在知识图谱嵌入的过程中使用文本信息的联合模型,
Wang[18]提出了一种在知识图谱嵌入的过程中使用文本信息的联合模型，该模型分三个部分：知识模型、文本模型和对齐模型。,知识模型,知识图谱,,知识模型的联合模型,
其中，知识模型对知识图谱中的实体和关系做嵌入，这是一个TransE的变种；文本模型对语料库中词语进行向量化，这是一个Skip-gram模型的变种；对齐模型用来保证知识图谱中的实体和关系与单词的嵌入在同一个空间中。,知识模型,知识图谱,,TransE的变种,
其中，知识模型对知识图谱中的实体和关系做嵌入，这是一个TransE的变种；文本模型对语料库中词语进行向量化，这是一个Skip-gram模型的变种；对齐模型用来保证知识图谱中的实体和关系与单词的嵌入在同一个空间中。,文本模型,知识图谱,,Skip-gram模型的变种,
联合模型在训练时降低来自三个子模型的损失之和。,联合模型,知识图谱,,降低来自三个子模型的损失之和,
"图2-24文本描述示例逻辑规则（Logical_Rules）也是常被用来考虑的附加信息，这里讨论的重点主要是霍恩子句，例如简单规则?x,y:IsDirectorOf（x,y）?BeDirectedBy（y,x）说明了两个不同的关系之间的关系。",Logical_Rules,知识图谱,,附加信息,
"图2-24文本描述示例逻辑规则（Logical_Rules）也是常被用来考虑的附加信息，这里讨论的重点主要是霍恩子句，例如简单规则?x,y:IsDirectorOf（x,y）?BeDirectedBy（y,x）说明了两个不同的关系之间的关系。",等价,知识图谱,,equivalence,
"图2-24文本描述示例逻辑规则（Logical_Rules）也是常被用来考虑的附加信息，这里讨论的重点主要是霍恩子句，例如简单规则?x,y:IsDirectorOf（x,y）?BeDirectedBy（y,x）说明了两个不同的关系之间的关系。",英文名,知识图谱,,equivalence,
Guo[19]提出了一种以规则为指导的知识图谱嵌入方法，其中提出的软规则（Soft_rule）指的是使用AMIE+规则学习方法在知识图谱中挖掘的带有置信度的规则，该方法的整体框架是一个迭代的过程，其中包含两个部分，称为软标签预测阶段（Soft_Label_Prediction）和嵌入修正阶段（Embedding_Rectification）。,软标签预测阶段,知识图谱,,Soft_Label_Prediction,
Guo[19]提出了一种以规则为指导的知识图谱嵌入方法，其中提出的软规则（Soft_rule）指的是使用AMIE+规则学习方法在知识图谱中挖掘的带有置信度的规则，该方法的整体框架是一个迭代的过程，其中包含两个部分，称为软标签预测阶段（Soft_Label_Prediction）和嵌入修正阶段（Embedding_Rectification）。,嵌入修正阶段,知识图谱,,Embedding_Rectification,
简单来说，就是讲规则学习和知识图谱嵌入学习互相迭代，最后使得知识图谱嵌入可以融入一定的规则信息。,知识图谱嵌入,知识图谱,,规则学习和知识图谱嵌入学习,
简单来说，就是讲规则学习和知识图谱嵌入学习互相迭代，最后使得知识图谱嵌入可以融入一定的规则信息。,知识图谱嵌入,知识图谱,,knowledge graph embedding,
2.5.6知识图谱嵌入的应用在知识图谱嵌入的发展中，也有很多的相关应用一起发展起来，它们和知识图谱嵌入之间有着相辅相成的关系。,知识图谱嵌入的应用,知识图谱,,知识图谱嵌入的发展,
2.5.6知识图谱嵌入的应用在知识图谱嵌入的发展中，也有很多的相关应用一起发展起来，它们和知识图谱嵌入之间有着相辅相成的关系。,知识图谱嵌入的应用,知识图谱,,application of knowledge graph embedding,
本小节将简单介绍一些典型的应用。,本小节,知识图谱,,典型的应用,
1.链接预测链接预测（Link_Prediction）指通过一个已知的实体和关系预测另一个实体，或者通过两个实体预测关系。,链接预测,知识图谱,,链接预测（Link_Prediction）,
1.链接预测链接预测（Link_Prediction）指通过一个已知的实体和关系预测另一个实体，或者通过两个实体预测关系。,链接预测,知识图谱,,Link_Prediction,
"简单来说，也就是（h,r,?）,（?,r,t）,（h,?,t）三种知识图谱的补全任务，被称为链接预测。",链接预测,知识图谱,,知识图谱的补全任务,
"简单来说，也就是（h,r,?）,（?,r,t）,（h,?,t）三种知识图谱的补全任务，被称为链接预测。",链接预测,知识图谱,,link prediction,
当知识图谱的嵌入被学习完成后，知识图谱嵌入就可以通过排序完成。,知识图谱嵌入,知识图谱,,排序,
当知识图谱的嵌入被学习完成后，知识图谱嵌入就可以通过排序完成。,知识图谱嵌入,知识图谱,,knowledge graph embedding,
"例如需要链接预测(Roma,is-capital-of,?)，可以将知识图谱中的每个实体都放在尾实体的位置上，并且放入相应的知识图谱嵌入模型的得分函数中，计算不同实体作为该三元组的尾实体的得分，也就是该三元组的合理性，得分最高的实体会被作为链接预测的结果。",链接预测,知识图谱,,知识图谱中每个实体都放在尾实体的位置上并且放入相应的知识图谱嵌入模型的得分函数中计算不同实体作为该三元组的尾实体的得分,
"例如需要链接预测(Roma,is-capital-of,?)，可以将知识图谱中的每个实体都放在尾实体的位置上，并且放入相应的知识图谱嵌入模型的得分函数中，计算不同实体作为该三元组的尾实体的得分，也就是该三元组的合理性，得分最高的实体会被作为链接预测的结果。",链接预测,知识图谱,,link-prediction,
链接预测也常被用于评测知识图谱嵌入。,链接预测,知识图谱,,评测知识图谱嵌入,
一般来说，会用链接预测的正确答案的排序评估某种嵌入模型在链接预测上的能力，比较常见的参数有平均等级（Mean_Rank）、平均倒数等级（Mean_Reciprocal_Rank）和命中前n（Hist@n）。,评估某种嵌入模型在链接预测上的能力,知识图谱,,比较常见的参数,
一般来说，会用链接预测的正确答案的排序评估某种嵌入模型在链接预测上的能力，比较常见的参数有平均等级（Mean_Rank）、平均倒数等级（Mean_Reciprocal_Rank）和命中前n（Hist@n）。,平均等级,知识图谱,,Mean_Rank,
一般来说，会用链接预测的正确答案的排序评估某种嵌入模型在链接预测上的能力，比较常见的参数有平均等级（Mean_Rank）、平均倒数等级（Mean_Reciprocal_Rank）和命中前n（Hist@n）。,平均倒数等级,知识图谱,,Mean_Reciprocal_Rank,
一般来说，会用链接预测的正确答案的排序评估某种嵌入模型在链接预测上的能力，比较常见的参数有平均等级（Mean_Rank）、平均倒数等级（Mean_Reciprocal_Rank）和命中前n（Hist@n）。,命中前n,知识图谱,,Hist@n,
2.三元组分类三元组分类（Triple_Classification）指的是给定一个完整的三元组，判断三元组的真假。,三元组分类,知识图谱,,给定一个完整的三元组，判断三元组的真假,
2.三元组分类三元组分类（Triple_Classification）指的是给定一个完整的三元组，判断三元组的真假。,三元组分类,知识图谱,,Triple_Classification,
这对于训练过的知识图谱向量来说非常简单，只需要把三元组各个部分的向量表达带入相应的知识图谱嵌入的得分函数，三元组的得分越高，其合理性和真实性越高。,知识图谱向量,知识图谱,,计算三元组的得分,
这对于训练过的知识图谱向量来说非常简单，只需要把三元组各个部分的向量表达带入相应的知识图谱嵌入的得分函数，三元组的得分越高，其合理性和真实性越高。,知识图谱向量,知识图谱,,knowldge graph embedding,
3.实体对齐实体对齐（Entity_Resolution）也称为实体解析，任务是验证两个实体是否指代或者引用的是同一个事物或对象。,实体对齐,知识图谱,,实体解析,
该任务可以删除同一个知识库中冗余的实体，也可以在知识库融合的时候从异构的数据源中找到相同的实体。,删除同一个知识库中冗余的实体,知识图谱,,该任务,
该任务可以删除同一个知识库中冗余的实体，也可以在知识库融合的时候从异构的数据源中找到相同的实体。,删除同一个知识库中冗余的实体,知识图谱,,delete redundant entities in the same knowledge base,
该任务可以删除同一个知识库中冗余的实体，也可以在知识库融合的时候从异构的数据源中找到相同的实体。,从异构的数据源中找到相同的实体,知识图谱,,find the same entity from heterogeneous data sources,
"一种方法是，如果需要确定x、y两个实体指代同一个对象有多大可能，则使用知识图谱嵌入的得分函数对三元组(x,EqualTo,y)打分，但这种方法的前提是需要在知识库中存在EqualTo关系。",知识图谱嵌入的得分函数,知识图谱,,判断x、y两个实体指代同一个对象有多大可能,
"一种方法是，如果需要确定x、y两个实体指代同一个对象有多大可能，则使用知识图谱嵌入的得分函数对三元组(x,EqualTo,y)打分，但这种方法的前提是需要在知识库中存在EqualTo关系。",等价,知识图谱,,equalTo,
也有研究者提出完全根据实体的向量表示判断，例如设计一些实体之间的相似度函数来判断两个实体的相似程度，再进行对齐。,向量表示判断,知识图谱,,根据实体的向量表示判断,
也有研究者提出完全根据实体的向量表示判断，例如设计一些实体之间的相似度函数来判断两个实体的相似程度，再进行对齐。,根据实体的向量表示判断,知识图谱,,根据实体的向量表示判断,
"4.问答系统利用知识图谱完成问答系统是该任务的一个研究方向，该任务的重心是对某一个具体的通过自然语言表达的问题，使用知识图谱中的三元组对其进行回答，如下：A:Where_is_the_capital_of_Italy？Q:Rome（Rome,is-capital-of,Italy）A:Who_is_the_president_of_USA？Q:Donald_Trump（Donald_Trump,is-president-of,USA）文献[9]介绍了一种借助知识图谱嵌入完成该问题的方法。",问答系统,知识图谱,,answer_a_specific_question_using_knowledge_graph,
简单来说就是设计一种得分函数，使问题的向量表示和其正确答案的向量表示得分较高。,得分函数,知识图谱,,设计一种得分函数,
简单来说就是设计一种得分函数，使问题的向量表示和其正确答案的向量表示得分较高。,得分函数,知识图谱,,"design a scoring function, make the problem vector representation and its correct answer vector representation score higher",
"S（q,a）是被设计出来的得分函数S（q,a）=（Wφ（q））?（Wψ（a））.式中，W为包含词语、实体和关系的向量表示的矩阵；φ（q）为词语出现的稀疏向量；ψ（a）为实体和关系出现的稀疏向量。","S（q,a）",知识图谱,,"得分函数S（q,a）",
"S（q,a）是被设计出来的得分函数S（q,a）=（Wφ（q））?（Wψ（a））.式中，W为包含词语、实体和关系的向量表示的矩阵；φ（q）为词语出现的稀疏向量；ψ（a）为实体和关系出现的稀疏向量。","S（q,a）",知识图谱,,"被设计出来的得分函数S（q,a）=（Wφ（q））?（Wψ（a））",
简单来说，Wφ（q）和Wψ（a）可以分别表示问题和答案的向量表示。,Wφ（q）,知识图谱,,向量表示问题的向量表示,
简单来说，Wφ（q）和Wψ（a）可以分别表示问题和答案的向量表示。,Wφ,知识图谱,,向量表示问题的向量,
简单来说，Wφ（q）和Wψ（a）可以分别表示问题和答案的向量表示。,Wψ,知识图谱,,向量表示答案的向量,
"当a是q的正确答案时，得分函数S（q,a）被期望得到一个较高的分数，反之亦然。",得分函数S,知识图谱,,期望得到一个较高的分数,
"当a是q的正确答案时，得分函数S（q,a）被期望得到一个较高的分数，反之亦然。",得分函数S,知识图谱,,expect high score when a is the correct answer of q,
5.推荐系统推荐系统的本质是对用户推荐其没有接触过的、但有可能会感兴趣或者购买的服务或产品，包括电影、书籍、音乐、商品等。,推荐系统,知识图谱,,对用户推荐其没有接触过的、但有可能会感兴趣或者购买的服务或产品,
协同过滤算法（Collaborative_Filtering）对用户和物品项目之间的交互进行建模并作为潜在表示取得了很好的效果。,协同过滤算法,知识图谱,,对用户和物品项目之间的交互进行建模并作为潜在表示取得了很好的效果,
协同过滤算法（Collaborative_Filtering）对用户和物品项目之间的交互进行建模并作为潜在表示取得了很好的效果。,协同过滤算法,知识图谱,,Collaborative_Filtering,
在知识图谱嵌入的发展下，推荐系统也尝试借助知识图谱的信息提高推荐系统的能力。,推荐系统,知识图谱,,知识图谱嵌入的发展,
例如，Zhang[20]尝试知识图谱中的三元组、文本信息和图像信息对物品项目进行包含一定语义的编码得到相应的向量表示，然后使用协同过滤算法对用户进行向量表示，对两个向量表示相乘得到分数，得分越高说明该用户越喜好该商品。,向量表示,知识图谱,,vector representation,
2.7本章小结本章比较全面地介绍了知识图谱的表示与建模方法。,本章小结,知识图谱,,知识图谱的表示与建模方法,
目前大部分开放知识图谱的表示语言基于RDF、RDFS和OWL，这几个语言是W3C推荐的标准本体语言。,开放知识图谱的表示语言,知识图谱,,RDF、RDFS和OWL,
除了这些标准语言，本章还介绍了知识图谱的查询语言SPARQL和语义Markup语言。,SPARQL,知识图谱,,查询语言,
除了这些标准语言，本章还介绍了知识图谱的查询语言SPARQL和语义Markup语言。,语义Markup语言,知识图谱,,本章,
除了这些标准语言，本章还介绍了知识图谱的查询语言SPARQL和语义Markup语言。,SPARQL,知识图谱,,查询语言,