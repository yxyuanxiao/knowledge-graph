{"text": "第8章 知识问答丁力 海知智能，杨成彪 南京柯基数据科技有限公司知识问答通过自然语言对话的形式帮助人们从知识库中获取知识，它不但是知识图谱的核心应用之一，也是自然语言处理的重要研究方向。"}
{"text": "随着新技术的不断涌现，知识问答技术取得了长足的进步，在工业界也有广泛的应用。"}
{"text": "本章介绍知识问答系统的基本概念、发展历史、评价体系以及最新进展。"}
{"text": "8.1 知识问答概述知识问答系统是一个拟人化的智能系统，它接收使用自然语言表达的问题，理解用户的意图，获取相关的知识，最终通过推理计算形成自然语言表达的答案并反馈给用户。"}
{"text": "例如，用户想了解“特朗普是哪里人”时，可以在网上搜索关键词“特朗普”，找到相关的百科网页，进而通过阅读文章定位出“纽约”是他的出生地。"}
{"text": "如果换一种思路，用户拿这个问题问身边的人，也许直接就会听到“纽约”这个答案。"}
{"text": "8.1.1 知识问答的基本要素知识问答或问答（Question Answering,QA）是对话的一种形态。"}
{"text": "它强调以自然语言问答为交互形式从智能体获取知识，不但要求智能体能够理解问题的语义，还要求基于自身掌握的知识和推理计算能力形成答案。"}
{"text": "问答是一种典型的智能行为，例如著名的图灵测试就是考验能否通过自然语言对话的方式判定答题者是人还是机器。"}
{"text": "在采用对话方式与用户沟通时，众多问答系统都需要使用一定的知识来解答问题，所以说问答系统实质上就是知识问答，本文后续也不再区分问答系统和知识问答系统。"}
{"text": "也有工作将知识库编码到计算模型中，例如逻辑规则、机器学习模型和深度学习模型。"}
{"text": "图8-1 问答系统的四大要素8.1.2 知识问答的相关工作信息检索（Information Retrieval,IR）或搜索以关键词搜索为代表，帮助用户发现包含搜索关键词的网页或文档。"}
{"text": "近来的信息检索技术也在逐步利用语义信息，例如支持查询扩展[1]、语义相似度匹配[2]以及基于知识图谱的实体识别[3]。"}
{"text": "但是搜索与知识问答有明显差异。"}
{"text": "第一，搜索以文档来承载答案，用户需要阅读搜索找到的文档来发现相关答案，而问答直接将答案交付给用户，而且答案通常来自已经结构化的数据或抽取后结构化的数据，而且结构化数据可以用列表的形式返回，也支持进一步的数据统计分析。"}
{"text": "第二，搜索侧重更简单的用户体验，用户的知识检索诉求主要通过关键词而不是完整的句子，这样需要用户掌握一定的搜索技巧。"}
{"text": "例如同一个问题，大学教授和中学生会采用不同的搜索技巧和搜索关键词组合，而他们得到的搜索结果也会不一样。"}
{"text": "问答则会尝试理解不同自然语言表达方式中固有的语义，然后形成知识查询。"}
{"text": "第三，当用户的问题比较复杂，需要通过多个页面的知识来回答时，搜索是无法完成的。"}
{"text": "例如，需要寻找“在华盛顿的数据挖掘公司”，而公司的地址信息（？公司位于 华盛顿）和公司的专业信息（？公司 业务 数据挖掘）恰好在两个不同网页上，搜索引擎是无能为力的。"}
{"text": "数据库查询（Database  Query）同样可以帮助用户获取知识，但是知识问答和数据库查询仍然存在一定差异。"}
{"text": "第一，数据库查询通常需要用户熟悉结构化数据的组织（Schema），知道如何指代数据中的概念（包括实体名、属性名等），掌握数据库查询语言（包括使用  JOIN  等复杂操作逻辑），而知识问答降低了对这些知识的要求，人们可以用自然语言来查询数据。"}
{"text": "值得注意的是，自然语言查询需要处理歧义现象，例如“Listall employees  in  the  company with a  driving license”（“列举有驾照的公司的雇员”），可以是找“有驾照的公司”也可以是“有驾照的公司雇员”，从常识判断只有后者才是用户的真正意图。"}
{"text": "类似的中文歧义的现象也很多，例如“南京市长江大桥”“教育部长江学者”都需要不同的语义理解歧义消解的方案。"}
{"text": "第二，数据库对知识库有严格限制，要求数据必须结构化存储。"}
{"text": "然而，大量知识存在于文本中而非数据库中，知识问答并不限制知识库的类型。"}
{"text": "第三，数据库查询结果不一定能形成用户可使用的最终答案。"}
{"text": "例如，数据库查询可以查到城市的编码，还需要再查询编码表得到城市的名称，而知识问答则需要直接返回城市的名称。"}
{"text": "知识问答、信息检索和数据库查询的对比如表8-1所示。"}
{"text": "表8-1 知识问答、信息检索和数据库查询的对比续表8.1.3 知识问答应用场景2011年，IBM  研发的超级计算机“沃森”在美国知识竞赛节目《危险边缘》中上演了“人机问答大战”，并一举战胜了两位顶尖的人类选手，成为人工智能发展史上又一标志性事件，如图8-2所示。"}
{"text": "自人工智能概念出现开始，问答系统的研究与应用一直在演进：20世纪60年代诞生了基于模板的问答专家系统，如 ELISA、BaseBall[4]、LUNAR[5]、SHRDLU;20世纪90年代兴起了基于信息检索的问答[6]，如  MASQUE、TREC；到21世纪初，伴随搜索引擎和网络社区而生的社区问答，如搜狗问问、百度知道、YAHOOanswers  等；直到今日，基于结构化数据的知识图谱问答技术、基于文本理解的机器阅读理解技术均取得了长足的进展。"}
{"text": "案例1.知识问答可以直接嵌入搜索引擎的结果页面，将问答的答案与搜索的结果列表同时展示。"}
{"text": "图8-2 “沃森”在《危险边缘》中获得冠军图8-3 问答展示界面案例2.知识问答技术可以应用于智能对话系统、智能客服或智能助理（IntelligentAgent）[7]。"}
{"text": "除了帮助人们获取知识[8]，智能助理也可以跟人闲聊，帮助人执行任务（例如下订单、订酒店、叫外卖），将用户的问题转化为结构化查询，利用多轮对话补全用户的意图等[9]。"}
{"text": "图8-4 基于不同领域知识图谱的问答系统在对话中有不同的理解案例3.知识问答应用于阅读理解。"}
{"text": "各种答题机器人和对话机器人也是知识问答的一个重要应用方向。"}
{"text": "例如，2011年，日本富士通联合日本国力信息学研究所的“多达一”考试机器人，以及国内“国家高技术研究发展计划（863计划）”基于大数据的类人智能关键技术与系统，俗称“高考机器人”，其背后均有知识图谱问答技术的支持。"}
{"text": "以阅读理解为代表的应用也可以被看作是知识问答的特例，它主要限制了知识库的边界（虽然阅读理解的主体知识是指定的章，但是实现理解仍然需要语法、常用词汇概念以及常识等辅助），而问题的形式可以是选择题（判断哪个答案正确）、填空题（直接填写答案）抑或是简答题。"}
{"text": "图8-5展示了一种阅读理解的应用场景，智能体以一段文章（passage）为知识库，针对问题从文章中寻找一段文字形成答案。"}
{"text": "图8-5 SQuAD阅读理解问题示例8.2 知识问答的分类体系本节围绕问答系统四大要素——问题、答案、知识库、智能体，简要梳理知识问答系统的特征并研究知识问答的分类体系。"}
{"text": "问答系统还有很多更深入的综述[10-14]。"}
{"text": "8.2.1 问题类型与答案类型在知识问答中，首先可以通过对问题的类型（Question Type）理解问答目标。"}
{"text": "问答系统可以针对问题类型，选择对应的知识库、处理逻辑来生成答案[15]。"}
{"text": "问题分类体系在很大程度上按照目标答案的差异而区分，所以这里将问题类型和答案类型合并，统一考虑为问题类型。"}
{"text": "通过对问题的类型（也就是用户问题所期望的答案的类型）的分析，问答系统可以有针对性地选择有效的知识库和处理逻辑解答一类问题。"}
{"text": "早期的工作包括TREC测试集问题分类研究[15]和ISI  QA  问题类型分类体系[16]，另外还有更详细的综述[17]。"}
{"text": "LI  等人[15]通过观察  TREC  的  1000  个问题的数据，从答案类型出发建立了一个问题分类体系，包含  6  个大类和  50  个细分类，并对各类问题的占比进行了统计。"}
{"text": "从统计结果中可以看出，TREC  中的大部分问题都集中在这几类数据，占总体问题数量的78%。"}
{"text": "其中，81个问题询问地点（LOCATION）、138个问题询问定义或描述（DESCRIPTION）、65个问题询问人物（HUMAN）、94个问题询问事物（例如动物、颜色、食品等）。"}
{"text": "可见，在知识问答中，一个合理的分类体系能够体现出问题的类型分布，从而帮助开发者有针对性地设计问答解决方案，并形成良好的问答系统。"}
{"text": "图8-6 ISI QA问题类型分类体系及实例后续也出现了基于功能的问题分类体系。"}
{"text": "例如，在英文中一个以“Why”开头的问题侧重询问原因，而以“How”开头的问题侧重询问解决方式。"}
{"text": "但是在中文里，带有“怎么样”这个词的问题，其意图有可能是询问原因，也有可能是询问解决方式。"}
{"text": "BU 等人[18]根据百度知道的数据，建立了一个基于功能（Function-Based）的问题分类体系。"}
{"text": "和  LI  等人[15]从答案类型出发构建分类体系类似，BU 等人[18]从利用功能以达成用户目标的角度来构建分类体系。"}
{"text": "相比于  LI  等人[15]专注于面向事实的知识问答的分类，BU  等人[18]提出的分类体系更面向通用问题。"}
{"text": "表8-2展示了BU等人[18]提出的问题分类体系机制，其中的事实类别和LI  等人[15]提出的分类体系中的大部分类别相对应。"}
{"text": "表8-2 BU等人提出的问题分类机制[18]图8-7 基于功能的问题分类体系在百度知道中的占比[18]综合分类体系的探索工作，本文从问答的功能出发，面向知识图谱问答的构建（即假定知识库的主题为知识图谱）整理出两种问题类型：事实性客观问题和主观深层次问题。"}
{"text": "（1）事实性客观问题。"}
{"text": "特点是语法结构简单（拥有明确的主谓宾结构，不包括例如并列、否定等复杂结构）、语义结构清晰（通常是关于某个事物或事件的简单描述性属性或关系型属性，可以通过简单的数据库查询解答）。"}
{"text": "事实型问题是知识问答中处理频度较高的一种问题类型，其中包含了谓词型问题（答案是一个单一的对象）、列表型问题（返回的不止一个答案，而是一列答案）。"}
{"text": "这两种主要是返回某些对象，从查询的角度来看，类似于数据库的  Select  操作。"}
{"text": "而对错型的问题更像  SPARQL  中的  Ask  类型的查询。"}
{"text": "实际上，这并不需要理解为一种“硬边界”的分类，也可能存在某些问题属于多个类别的情况。"}
{"text": "可以细分如下：① 询问命名实体的基本定义（ENTITY）●事物的分类（IS-A），例如“热带水果有哪些？”●事物的别名（ALIASEs），例如“番茄是西红柿吗？”●事物的定义（WHAT-IS），例如“什么是西红柿？”② 询问实体属性，包括描述性属性和关系性属性 （PROPERTY）●人（WHO），例如“谁写了《平凡的世界》?”●地点（WHERE），例如“《平凡的世界》的主人公是哪里人？”●时间（WHEN），例如“北京奥运会是在哪一年举办的？”●属性（ATTRIBUTE），例如“西红柿是什么颜色的？”③ 复杂知识图谱查询●询问实体列表或统计结果，例如“唐宋八大家是哪几位？”“北京奥运会中国得了多少枚金牌？”“北京四月份的平均气温是多少？”“北京最大的公园是哪一个？”●询问实体差异，例如“颐和园和圆明园哪里相似，哪里不同？”●询问实体关系，例如“王菲和章子怡有什么关系？”“A  公司和  B  公司有没有控制关系？”（2）主观深层次问题。"}
{"text": "包括除事实型问题之外的其他问题，例如观点型、因果型、解释型、关联型与比较型等。"}
{"text": "这一类问题本身的语法结构并不复杂，但是这些问题需要一定的专业知识和主观的推理计算才能解答，而且这一类问题有时甚至不止一个答案，需要结合用户偏好和智能体的配置找到不同的最优解。"}
{"text": "可以细分如下：① 问解释（WHY），例如“为什么天空是蓝色的？”“为什么眼睛会近视？”②  问方法（HOW），例如“怎么做戚风蛋糕？”“如何在  Windows  上创建一个文件夹？”③  问专家意见（CONSULT），例如“左侧内踝骨折累及关节面多少天能下地走路？今年89岁。"}
{"text": "”④ 问推荐（RECOMMENDATION），例如“哪个歌手跟刘德华类似？”另外，问题类型并非问题理解中的唯一语义要素。"}
{"text": "问题焦点（Focus）指的是问句中出现的与答案实体或属性相关的元素，例如问句“In which city was Barack Obama born?”中的city，以及“What is the population of Galway?”中的population。"}
{"text": "问题主题（Topic）反映问题是关于哪些主题的，例如问句“What is the height of Mount Everest?”询问的是关于地理及山脉的信息，而“Which organ is affected by the Meniere's disease?”的问题主题则是医疗方面的内容。"}
{"text": "8.2.2 知识库类型从知识库的内容边界，或者知识库覆盖了哪些领域来看，知识问答可以分两类。"}
{"text": "一是领域相关的问答系统，只回答与选定领域相关的问题。"}
{"text": "这一类系统相对专注，需要领域专家的深入参与，虽然问题覆盖面小，但是答案的正确率高。"}
{"text": "早期的成功问答系统都是与领域相关的。"}
{"text": "近年来，企业的智能客服通常采用领域相关的问答系统，并且逐步转向基于知识图谱的解决方案。"}
{"text": "二是领域无关的问答系统，基于开放知识库回答任意问题。"}
{"text": "这一类系统答案虽然覆盖面大，但答案的正确率有限。"}
{"text": "开放域问答系统经常使用万维网数据（尤其是百科网站、社区问答等）作为数据源解答用户的问题。"}
{"text": "由于用户的期望较高，开放问题结构并不总是简单，开放域知识相对稀疏等原因，实用产品的用户体验还有待提高。"}
{"text": "从知识库的信息组织格式来看，知识库可以是基于文本表示，也可以采用其他组织形式。"}
{"text": "第一，文本类知识库利用纯文本承载知识，也是最常见的知识组织形式。"}
{"text": "这类知识库不但支持基于搜索的问答系统，也可以与基于知识图谱的结构化抽取技术结合，支持基于语义查询的解决方案。"}
{"text": "另外，常见问答对（FAQ）或社区问答也是知识问答（尤其是智能客服）最容易获取的知识，可以直接通过问题匹配帮助用户获取答案。"}
{"text": "第二，半结构化或结构化的知识库。"}
{"text": "这一类知识库侧重知识的细粒度组织，利用结构体现知识的语义。"}
{"text": "电子表格、二维表或者关系数据库是最常见的结构化知识，实体和属性通过简单的二维表表示，大多数事实性客观问题都可以被此类知识解答。"}
{"text": "图数据库，例如  RDF、属性图、语义网络等，将通过节点、有向边来形成基于图的知识组织，并且利用节点和边的名称与上下文对接自然语言处理并支持语义相似度计算，同时还能支持复杂的结构化图查询机制。"}
{"text": "第三，除文字外，知识也可以存储在图片、音频、视频等媒体中，这些都可以作为知识问答中答案的一部分，更有效地反馈给终端用户，从而丰富答案的表示并满足更多的交互场景需求。"}
{"text": "第四，知识库并不限定于文本、符号系统或多媒体，也可以利用可计算的机器学习模型承载。"}
{"text": "例如近年来出现的端到端的问答系统可以直接使用分布式表示模型记录习得的知识。"}
{"text": "另外，知识库的存储访问机制也是知识问答需要考虑的因素。"}
{"text": "知识问答的知识可以采用单一的集中数据存储（例如数据表、数据库），或者分布式存储（例如分布式数据、数据仓库），甚至是基于互联网的全网数据（例如Linked Data）。"}
{"text": "8.2.3 智能体类型智能体利用知识库实现推理。"}
{"text": "根据知识库表示形式的不同，目前的知识问答可以分为传统问答方法（符号表示）以及基于深度学习的问答方法（分布式表示）两种类型。"}
{"text": "传统问答方法使用的主要技术包括关键词检索、文本蕴涵推理以及逻辑表达式等，深度学习方法使用的技术主要是LSTM[19]、注意力模型[20]与记忆网络（Memory Network）[21]等。"}
{"text": "传统的知识库问答将问答过程切分为语义解析与查询两个步骤。"}
{"text": "如图8-8所示，首先将问句“姚明的老婆出生在哪里”通过语义解析转化为  SPARQL  查询语句。"}
{"text": "这个例子中的难点是将问句中的“老婆”映射到知识图谱中的关系“配偶”，这也是传统的知识库问答研究的核心问题之一；再从知识库（知识图谱）中查询，得到问题的答案“上海”。"}
{"text": "不同于传统方法，基于分布式表示的知识库问答利用深度神经网络模型，将问题与知识库中的信息转化为向量表示，通过相似度匹配的方式完成问题与答案的匹配。"}
{"text": "首先，利用神经网络模型，将问题“姚明的老婆出生在哪里”表示成向量，这里使用的是一个递归神经网络的表达形式；然后取知识图谱中与实体“姚明”相关的实体向量，计算与问句向量的语义相似度，从而完成知识问答的过程。"}
{"text": "在整个过程中，并不需要确定问句中的“老婆”与知识图谱中的关系“配偶”的映射，这也是基于深度学习的问答方法的优势所在。"}
{"text": "图8-8 基于符号表示（传统）的知识问答图8-9 基于分布式表示的知识库问答8.3 知识问答系统8.3.1 NLIDB：早期的问答系统20世纪六七十年代，早期的NLIDB（Natural  Language  Interface  to  Data  Base）伴随着人工智能的研发逐步兴起[22]，以1961年的  BASEBALL 系统[4]和1972年的 LUNAR 系统[5]（Woods  1973）为代表。"}
{"text": "BASEBALL  系统回答了有关一年内棒球比赛的问题。"}
{"text": "LUNAR在阿波罗月球任务期间提供了岩石样本分析数据的界面。"}
{"text": "这些系统一般限定在特定领域，使用自然语言问题询问结构化知识库。"}
{"text": "这些数据库与如今讲的关系数据库不同，更像基于逻辑表达式的知识库。"}
{"text": "这一类系统通常为领域应用定制，将领域问题语义处理逻辑（自然语言问题转化为结构化数据查询）硬编码为特定的语法解析规则（例如模板或者简单的语法树），同时手工构建特定领域的词汇表，形成语法解析规则，很难转移到其他的应用领域。"}
{"text": "如图8-10所示为早期NLIDB型问答系统的设计思想。"}
{"text": "依据文献[23]的介绍，NLIDB  系统大多采用的模块包括：①实体识别（Named  EntityRecognition），通过查询领域词典识别命名实体；②语义理解（Question2Query），利用语法解析（例如词性分析，Part-Of-Speech）、动词分析（包括主动和被动）以及语义映射规则等技术，将问题解析成语义查询语句；③回答问题（Answer  Processing），通常通过简单查询和其他复杂操作（例如  Count）获取答案。"}
{"text": "这些工作中的语义理解部分各具特色，也就此奠定了后续问答系统中问题解析的基本套路，下面详细举例说明。"}
{"text": "图8-10 早期NLIDB型问答系统的设计思想（1）基于模式匹配（Pattern-Matching）。"}
{"text": "基于模式匹配的语义理解可以直接将问题映射到查询。"}
{"text": "如图8-11所示，例子“...capital...<country>.....”中，变量“<country>”用来表示Country  类型的一个实体，例如  Italy，而“capital”是一个字符串。"}
{"text": "这个模板可以匹配不同的自然语言说法，例如“What  is  the  capital  of  Italy?”“Could  you  please  tell  me  what  is  thecapital of Italy?”，然后将问题映射到查询“Report Capital of row where Country = Italy”（查询意大利的首都）。"}
{"text": "这种语义理解技术简便灵活且不依赖过多的语法分析工具，后来发展为KBQA中基于模板的语义理解方案。"}
{"text": "图8-11 基于模板匹配的NLIDB解决方案[23]（2）基于语法解析（Syntactic-Parsing）。"}
{"text": "基于语法解析的语义理解将自然语言的复杂语义转化为逻辑表达式。"}
{"text": "如图8-12所示为展示了 LUNAR  系统利用语法树解析初步解析问题。"}
{"text": "句法分析器的树状结果仍然需要人工生成的语义规则和领域知识来理解，进而转化成一种中间层的逻辑表达式。"}
{"text": "通过一个简单的基于  Context-Free  Grammar（CFG）的语法，主语（S）由一个名词短语（NP）加上一个动词短语（VP）组成；一个名词短语（NP）由一个确定词（Det）和一个名词（N）组成；确定词（Det）可以是“what”或“which”等。"}
{"text": "这样，“which rock contains magnesium”就可以解析为后面的语法分for_every  X,“rock”映射到（is析结果，进而通过一系列转换规则，例如“which”映射到 rock  X），形成最终的数据库查询。"}
{"text": "不少后来的系统也在系统的可移植性上有一些进展，包括允许为某一个新领域重新定制词典，构建通用知识表示语言来表达语义规则。"}
{"text": "有些系统甚至还可以允许用户通过交互界面添加新词汇和映射规制，包括  LUNAR系统后期提出的 MRL 语言[23]，将自然语言问题转化为一种基于中间表示语言的逻辑查询表达式。"}
{"text": "这种中间表示语言承载了高层次世界概念以及用户问题的含义，独立于数据库存储结构，可以进一步转换成数据查询的表达式从而获取答案。"}
{"text": "这一类方案后来演进为  KBQA  中基于语义解析（Semantic Parsing）的语义理解方法。"}
{"text": "语法树分析为处理更为复杂的问题以及简单问题的语法变形提供了便利，但是这也同时依赖语法分析工具的正确性（包括词性分析、语法依存分析）。"}
{"text": "另外，当词汇具有多重词性时，也存在潜在问题。"}
{"text": "所以，还需要附加一些规则调整语法解析出来的查询。"}
{"text": "图8-12 LUNAR系统利用语法树解析初步解析问题[23]8.3.2 IRQA：基于信息检索的问答系统基于信息检索的问答系统（Information  Retrieval  based  Question-Answering  System,IRQA）[6]的核心思想是根据用户输入的问题，结合自然语言处理以及信息检索技术，在给定文档集合或者互联网网页中筛选出相关的文档，从结果文档内容抽取关键文本作为候选答案，最后对候选答案进行排序返回最优答案。"}
{"text": "如图8-13所示，参考斯坦福 IRQA  的基本架构[13]，问答流程大致分三个阶段：（1）问题处理（Question  Processing）。"}
{"text": "从不同角度理解问题的语义，明确知识检索Formulation，即问句转化为关键词搜索）和答案类型判定（Answer的过滤条件（Query Type  Detection，例如“谁发现了万有引力？”需要返回人物类实体；“中国哪个城市人口数最多？”需要返回城市类实体）。"}
{"text": "（2）段落检索与排序（Passage  Retrieval  And  Ranking）。"}
{"text": "基于提取出的关键词进行信息检索，对检索出的文档进行排序，把排序之后的文档分割成合适的段落，并对新的段落进行再排序，找到最优答案。"}
{"text": "（3）答案处理（Answer  Processing）。"}
{"text": "最后根据排序后的段落，结合问题处理阶段定义的答案类型抽取答案，形成答案候选集；最终对答案候选集排序，返回最优解。"}
{"text": "此方法以文档为知识库，没有预先的知识抽取工作。"}
{"text": "图8-13 IRQA的基本架构 [13]8.3.3 KBQA：基于知识库的问答系统基于知识库的问答系统（Knowledge-Based  Question  Answering,KBQA)特指使用基于知识图谱解答问题的问答系统。"}
{"text": "KBQA  实际上是20世纪七八十年代对  NLIDB  工作的延续，其中很多技术都借鉴和沿用了以前的研究成果。"}
{"text": "其中，主要的差异是采用了相对统一的基于  RDF  表示的知识图谱，并且把语义理解的结果映射到知识图谱的本体后生成SPARQL  查询解答问题。"}
{"text": "通过本体可以将用户问题映射到基于概念拓扑图表示的查询表达式，也就对应了知识图谱中某种子图。"}
{"text": "KBQA  的核心问题  Question2Query  是找到从用户问题到知识图谱子图的最合理映射。"}
{"text": "QALD（Question  Answering  on Linked  Data）[38]是2011年开始针对KBQA问答系统的评测活动。"}
{"text": "文献[14]分析了参与QALD  的数十个问答系统，并从问题解析、词汇关联、歧义消解、构建查询以及分布式知识库五个阶段做了对比，而前四个问题都是Question2Query的关键步骤。"}
{"text": "（1）问题分析。"}
{"text": "主要利用词典、词性分析、分词、实体识别、语法解析树分析、句法依存关系分析等传统  NLP  技术提取问题的结构特征，并且基于机器学习和规则提取分析句子的类型和答案类型。"}
{"text": "知识图谱通常可以为  NLP  工具提供领域词典，支持实体链接；同时，知识图谱的实体和关系也可以分别用于序列化标注和远程监督，支持对文本领域语料的结构化抽取，进一步增补领域知识图谱。"}
{"text": "（2）词汇关联。"}
{"text": "主要针对在问题分析阶段尚未形成实体链接的部分形成与知识库的链接，包括关系属性、描述属性、实体分类的链接。"}
{"text": "例如“cities”映射到实体分类“城市”,“is  married 果）。"}
{"text": "to”映射到关系“spouse”。"}
{"text": "也包括一些多义词，例如“Apple”（公司还是水（3）歧义消解。"}
{"text": "一方面是对候选的词汇、查询表达式排序选优，一方面通过语义的容斥关系去掉不可能的组合。"}
{"text": "例如，苹果手机是不能吃的，所以吃苹果中苹果的“电器”选项应去掉。"}
{"text": "在很多系统中，歧义消解与构建查询紧密结合：先生成大量可能的查询，然后通过统计方法和机器学习选优。"}
{"text": "（4）构建查询。"}
{"text": "基于问题解析结果，可以通过自定义转化规则或者特定（语义模型+语法规则）将问题转化为查询语言表达式，形成对知识库的查询。"}
{"text": "QALD  的大多系统使用SPARQL  表达查询。"}
{"text": "注意查询语言不仅能表达匹配子图的语义，还能承载一些计算统计功能（average、count函数）。"}
{"text": "8.3.4 CommunityQA/FAQ-QA：基于问答对匹配的问答系统基于常见问答对（Frequently Asked Question,FAQ-QA[24]）以及社区问答（Community  Question  Answering,CQA）[25]都依赖搜索问答FAQ库（许多问答对<Q,A>的集合）来发现以前问过的类似问题，并将找到的问答对的答案返回给用户。"}
{"text": "FAQ  与CQA都是以问答对来组织知识，而且问答对的质量很高，不但已经是自然语言格式，而且受到领域专家或者社区的认可。"}
{"text": "二者的差异包括：答案的来源是领域专家还是社区志愿者，答案质量分别由专家自身的素质或者社区答案筛选机制保障。"}
{"text": "基于 FAQ-QA  的核心是计算问题之间的语义相似性。"}
{"text": "重复问题发现（DuplicateQuestion  Detection,DQD）仅限于疑问句，这是短文本相似度计算的一个特例。"}
{"text": "事实上，语义相似性面临两个挑战：（1）“泛化”。"}
{"text": "相同的语义在自然语言表达中有众多的表示方式，不论从词汇还是语法结构上都可以有显著差异，例如 “How do I add a vehicle to this policy?” 和 “What should Ido to extend this policy for my new car?”。"}
{"text": "（2）“歧义”。"}
{"text": "两个近似的句子可以具有完全不同的语义，例如“教育部/长江学者”和“教育部长/江学者”。"}
{"text": "语义相似度计算一直是 NLP 研究的前沿。"}
{"text": "一种类型的方法试图通过利用语义词典（例如 WordNet）计算词汇相似度，这些语义相似网络来自语言学家的经验总结，受限于特定的语言；另一种方法将此任务作为统计机器翻译问题处理，并采用平行语料学习逐字或短语翻译概率，这种方法需要大量的平行问题集学习翻译概率，通常很难或成本高昂。"}
{"text": "Rodrigues  J  A  等人[26]基于两个测试数据集（AskUbnuntu领域相关问题集，和Quora领域无关）对比了基于规则（JCRD  Jacard）、基于传统机器学习以及基于深度学习的方法。"}
{"text": "发现基于深度学习的方法在领域问题上效果显著，但是开放领域问题中效果与传统方法接近（甚至有所下降）。"}
{"text": "SemEval  2017年[27]评测结果指出，英文句子相似度计算的最佳结果已经达到F1=0.85。"}
{"text": "8.3.5 Hybrid QA Framework 混合问答系统框架从结构化数据出发的  KBQA  侧重精准的问题理解和答案查询，但是结构化的知识库总是有限；从非结构化文本出发的  IRQA侧重于利用大量来自文本的答案，但是文本抽取存在精度问题且不容易支持复杂查询与推理。"}
{"text": "所以，在工业应用中，为了满足领域知识问答的体验，结合有限的高度结构化的领域数据与大量相关的文本领域知识，需要更通用的问答框架，以取长补短。"}
{"text": "1.DeepQA:IRQA主导的混合框架如图8-14所示的  DeepQA[28]综合  IRQA  和  KBQA  形成混合问答系统的架构图，Watson系统的问题处理大致分成四阶段：图8-14 DeepQA综合IRQA和KBQA形成混合问答系统的架构图 [13]（1）问题处理（Question  Processing）。"}
{"text": "主要是理解问题的类型，解析问题语义元素等。"}
{"text": "（2）候选答案生成（Candidate  Answer  Generation）。"}
{"text": "不仅从网络上搜索相关文档并抽取答案，还从知识库直接查询答案，然后合并构成答案候选集。"}
{"text": "（3）候选答案评分（Candidate  Answer  Scoring）。"}
{"text": "针对每个候选答案选取一些重要特征，并对各个特征打分并形成答案的特征向量。"}
{"text": "Watson  会利用很多信息源的佐证对候选答案进行打分，例如答案类型（Lexical  Answer  Type）、答案中的时空信息等。"}
{"text": "以答案类型的人的问答为例，如果已知每个历史人物的出生日期和去世日期（从百科知识图谱获取），同时要求查找一个历史人物并且提到时间范围，则候选答案中非同时期的人物可以被认为是无关的。"}
{"text": "（4）答案融合及排序（Confidence  Merging  And  Ranking）。"}
{"text": "首先把相同的答案进行融合（例如两个候选人名  J.F.K.和  John  F.Kemedy  会被合并成为一个候选答案），形成新的答案候选集，然后对新的答案候选集进行再排序，最终由训练好的逻辑回归分类器模型对每个候选答案计算置信度，并返回置信度最高的答案作为最终答案。"}
{"text": "总之，Watson  架构的创新点是同时从  IRQA  和  KBQA  获取大量候选答案，并以大量答案佐证作为特征形成答案特征评分向量，这一点正是单独IRQA系统和KBQA系统没有做到的。"}
{"text": "2.QALD-Hybrid-QA:KBQA主导的混合框架在QALD-6启动的Hybrid  QA  要求KBQA可以同时利用知识图谱数据和文本数据。"}
{"text": "自然语言先转化为  SPARQL  查询，但是并非所有  SPARQL  查询中的三元组特征（TriplePattern）都可以对应到知识图谱中的词汇，也并非所有知识都可以从掌握的知识图谱中查到，有一部分知识还需要从文档中抽取关系得到解答。"}
{"text": "这样可以避免前期过度的文本抽取工作，也能适应现实中更常见的图谱和文本混合的知识库。"}
{"text": "如图8-15所示[29]，当遇到包含“is  the  front  man  of”关系的三元组特征时，系统首先通过基于知识图谱的关系抽取技术，结合DBpedia[30]和OpenIE[31]从Wikipedia中抽取相关三元组。"}
{"text": "注意，在OpenIE抽取的三元组中，大量谓语  predicate  是没有经过归一融合的。"}
{"text": "然后利用平行语料模型将问句中的关系映射到抽取三元组的谓语上。"}
{"text": "例如，“front man of”映射到“lead vocalist of”上。"}
{"text": "图8-15 基于SPARQL的混合问答系统的架构[29]3.Frankenstein：问答系统的流水线架构Frankenstein[32]通过对60多种  KBQA  系统的研究，将  KBQA  分成基于四类核心模块的流水线，其架构如图8-16所示。"}
{"text": "模块化的流水线设计有利于将复杂的 QA  系统分解为细粒度可优化的部分，而且形成了可插拔的体系，便于系统优化更新。"}
{"text": "但是这样的流水线有两点要求：尽量使用统一的知识表示，例如基于 Ontology/Schema，这样才能保证各模块在接口上可以复用；模块的分解目前只考虑了Question2Query 中针对结构化查询的部分，未覆盖非结构化文本的问答。"}
{"text": "这个框架首先制定了一个可配置的流水线框架，并且分解出KBQA的四个主要模块：的知识库以及通用的RDF 图8-16 问答系统流水线的架构[32]（1）命名实体识别与消解歧义（Named  Entity  Disambiguation,NED）。"}
{"text": "从问题的文本中标记其中涉及的实体。"}
{"text": "（2）实体关系映射（Relation  Linking,RL）。"}
{"text": "将问题文本提及的关系映射到知识库的实体属性或实体关系上。"}
{"text": "（3）实体分类映射（Class  Linking,CL）。"}
{"text": "将问题所需答案的类型映射到知识库的实体分类上。"}
{"text": "（4）构建查询（Query Building,QB）。"}
{"text": "基于上述语义理解的结果综合后形成SPARQL查询。"}
{"text": "同时，框架也利用分类器技术（QA Pipeline Classifier）支持流水线自动配置，也就是说从29个不同的模块（18个NED、5个RL、2个CL、2个QB），针对每一个特定的KBQA问答系统选取最优的流水线组合。"}
{"text": "如图8-17所示，对于“What  is  the  capital  of  Canada?”，理想的NED组件应该将关键字“Canada”识别为命名实体，并将其映射到相应的 DBpedia 资源，即 dbr:Canada。"}
{"text": "然后，RL  模块需要找到知识图谱中对应的实体关系，因此“capital”映射到  dbo:capital。"}
{"text": "最后，QB  模块综合上述结果形成  SPARQL  查询  SELECT  DISTINCT?uri  WHERE{dbr:Canadadbo:capital ?uri.}。"}
{"text": "图8-17 问答系统流水线举例说明[32]8.4 知识问答的评价方法8.4.1 问答系统的评价指标1.功能评价指标问答系统通常可以通过一组预定的测试问题集以及一组预定的维度来评价。"}
{"text": "问答系统的功能评价重点关注返回的答案，正确的答案应当同时具备正确度及完备度，正确但内容不完整的答案被称为不准确答案，没有足够证据及论证表明答案与问题相关性的则是无支撑答案，当答案与问题完全无关时，意味着答案是错误的。"}
{"text": "答案评价通常可以从如下角度考虑：（1）正确性。"}
{"text": "答案是否正确地回答了问题，例如问美国总统是谁，回答“女克林顿”就错了。"}
{"text": "（2）精确度。"}
{"text": "答案是否缺失信息，例如问美国总统是谁，回答“布什”可能存在二义性，到底是老布什，还是小布什；答案中是否包含了多余的信息，同样的问题，“特朗普在纽约州出生”就包含了多余的信息。"}
{"text": "（3）完整性。"}
{"text": "如果答案是一个列表，应当返回问题要求的所有答案。"}
{"text": "例如，列举美国总统，应该把所有满足条件的人都列举出来。"}
{"text": "（4）可解释性。"}
{"text": "在给出答案的同时，也给出引文或证明说明答案与问题的关联。"}
{"text": "根据TREC的测试结果，考虑与未考虑文章支持度的测试结果差距可达十几个百分点。"}
{"text": "（5）用户友好性。"}
{"text": "答案质量由人工评分，很多非事实性问题并非一个唯一的答案，所以需要人工判定答案的质量。"}
{"text": "如果答案被认为没错就按质量打分，Fair为1分、Good为2分、Excellent为3分，如果答不上来或答错则算零分。"}
{"text": "（6）额外的评价维度。"}
{"text": "当答案类型更为复杂时，例如有排序、统计、对比等更多的要求，还应该有额外的评价维度。"}
{"text": "除了上述针对答案的评价，也有针对解答过程复杂程度的评价，例如 Semantic Tractability[33]，用于反映问答之间的词表差异性；AnswerLocality[34]，答案是否零碎地分布在不同的文本或数据集录中；Derivability34，问题的答案是否是某种确定性答案，还是含蓄的、不确定的描述；Semantic  Complexity，问题涉及的语义复杂程度。"}
{"text": "常用的问答指标采用  F1（综合正确率和召回率）和  P@1（第一个答案是否正确的比率）。"}
{"text": "2.性能评价指标除了功能评价指标，参考Usbeck  R等人[35]的评价体系，问答系统从性能角度可以考虑如下指标：（1）问答系统的响应时间（Response  Time）。"}
{"text": "问答系统对用户输入或者请求做出反应的时间。"}
{"text": "问答系统的响应时间是评价系统性能的一个非常重要的指标，如果响应时间过长，会使系统的可用性很低。"}
{"text": "一般问答系统的响应时间应控制在1s以内。"}
{"text": "（2）问答系统的故障率（Error  Rate）。"}
{"text": "在限定时间内给出答案即可，不考虑答案是否正确。"}
{"text": "系统返回错误或者系统运行过程中发生错误数的统计。"}
{"text": "8.4.2 问答系统的评价数据集1.TREC QA：评价IRQATREC  QA[36]是美国标准计量局在1999—2007年针对问答系统设定的年度评价体系，本文关注其问答的核心任务（MAIN  TASK）。"}
{"text": "此评价体系主要针对基于搜索的问答解决方案（IRQA）。"}
{"text": "问题集主要来自搜索引擎的查询日志（也有少部分问题由人工设计）。"}
{"text": "知识库主要采用跨度几年的主流媒体的新闻。"}
{"text": "问答系统返回的结果包括两部分<答案，文档ID>，前者为字符串，后者为问题答案来源的文档的 ID。"}
{"text": "评价方法主要是选取大约1000个测试问题，由1～3人标注评价答案的正确性（答案是否正确回答了问题）、精准度（答案中是否包含多余的内容）以及对应文章的支持度（对应的文章是否支持该答案）。"}
{"text": "评价指标区分了单一答案和列表答案的评价方法。"}
{"text": "2.TREC LIVE QA：评价CQA社区问答TREC  LIVE  QA也[37]是美国标准计量局在2015—2107年从更真实的网络问答出发，主要面向  CQA  社区问答解决方案的评价体系。"}
{"text": "问题集主要来自  Yahoo  Answer  的实时新问题。"}
{"text": "  知识库主要来自Yahoo  Answer  的社区问答数据，以及过往标注的千余条数据。"}
{"text": "评价方法主要选取大约1000个测试问题，每个问题要求在1min内回答。"}
{"text": "由于问题类型不限于简单知识问答，所有的答案由1～3人标注并直接按答案质量打{0,1,2,3}分。"}
{"text": "另外，评价系统也针对测试问题，获取赛后的社区人工答案做类似的评价，然后对比自动生成的答案和人工产生的答案的体验差异。"}
{"text": "3.QALD：评价KBQAQALD[38]是指2011—2017年的链接数据的问答系统评测（Question  Answering onLinked  Data），为自然语言问题转化为可用的SPARQL查询以及基于语义万维网标准的知识推理提供了一系列的评价体系和测试数据集，对  QALD  的工作做了详细介绍。"}
{"text": "QALD的主要任务如下：给定知识库（一个或多个  RDF  数据集以及其他知识源）和问题（自然语言问题或关键字），返回正确的答案或返回这些答案的  SPARQL  查询。"}
{"text": "这样，QALD可以利用工业相关的实际任务评价现有的系统，并且找到现有系统中的瓶颈与改进方向，进而深入了解如何开发处理海量  RDF  数据方法。"}
{"text": "这些海量数据分布在不同的数据集之间，并且它们是异构的、有噪声的，甚至结构是不一致的。"}
{"text": "每一年  QALD  通过不同的任务覆盖了众多的评价体系，包括：面向开放领域的多语种问答，例如  Task  1:Multilingualquestion  answering  over  Dpedia；面向专业领域的问答，例如  MusicBrainz（音乐领域）、Drugbank（医药领域）；结构化数据与文本数据混合的问答，例如  Task  2:Hybrid questionanswering；海量数据的问答，例如Task  3:Large-Scale  Question  answering  over  RDF；新数据源的问答，例如Task 4:Question answering over Wikidata。"}
{"text": "4.SQuAD：评价端到端的问答系统解决方案SQuAD[39]是斯坦福大学推出的一个大规模阅读理解数据集，由众多维基百科文章中的众包工作者提出的问题构成，每个问题的答案都是相应阅读段落的一段文字或跨度。"}
{"text": "在500多篇文章中，有超过100,000个问题—答案对，SQUAD  显著大于以前的阅读理解数据集。"}
{"text": "2017—2018年，国内也有不少类似的阅读理解比赛，例如搜狗问答。"}
{"text": "SQuAD  评价指标主要分两部分：（1）精准匹配。"}
{"text": "正确匹配标准答案，目前效果最好的算法达到74.5%，人类表现是82.3%。"}
{"text": "这个指标准确地匹配任何一个基本事实答案的预测百分比。"}
{"text": "（2）F1值。"}
{"text": "这个指标衡量了预测和基本事实答案之间的平均重叠数。"}
{"text": "在给定问题的F1，然后对所有问题求平均值。"}
{"text": "2018年3月，谷歌公司的所有基础正确答案中取最大值 QAnet[40]达到了F1=89.737，非常接近人工对比指标F1=91.221。"}
{"text": "在此之前，斯坦福大学还发布过  Web  Question  数据集[41]。"}
{"text": "首先通过  Google  SuggestAPI  获取只包含单个实体的问题，然后选取实体前面或后面的语句作为  query，以此作为种子进行问题扩充，每个  query  大约扩充5个候选问题，形成体量大约为100万的问题集；然后随机选取10万个问题，交由众包工作者搜集答案，并对每个答案给出答案来源URL；最后，对问答对进行筛选，形成包括3778条数据的训练集以及2032条数据的测试集。"}
{"text": "在Web  Questions数据集上的F1值为31.3%，后续不少研究者在Web  Questions  提出了一些新Jain  提出的Factual的有效模型，F1值逐年更新。"}
{"text": "目前，效果最好的模型是 Sarthak Memory  Network模型[42]，该模型的平均精确度为55.2%，平均召回率为64.9%，平均精确度和平均召回率的F1值为59.7%，平均F1值为55.7%。"}
{"text": "5.Quora QA：评价问题相似度计算Quora于2017年在Kaggle发布的数据集包含约40万个问题对，每个问题包含两个问题ID  和原始文本，另外还有一个数字标记这两个问题是否等价，即对应到同一个意图的 上。"}
{"text": "这个数据集主要用于验证社区问答或 FAQ  问答的语义相似度计算算法，目前在Kaggle  上的竞赛结果最优者的  Logloss  已经达到0.11。"}
{"text": "这个数据集来自社区问答网站Quora，这种规模抽样的数据的确存在少量噪声问题且话题分布并不一定与  Quora  网站的问题分布一致。"}
{"text": "另外，社区问答中只有少量问题是真正等价，因此通过  C(n,2)随机组合抽取两个问题，绝大多数问题对也不应该等价。"}
{"text": "这40万条数据首先加入了大量正例（等价的问题对），然后利用“related  question”关系添加了负例（相关但不等价的问题对），这样才形成一个相对平衡的训练数据集。"}
{"text": "Elkhan  Dadashov[43]在Quora  QA数据集上尝试了多种不同的LSTM模型，最好的模型的F1值达到了79.5%，准确率还到了83.8%。"}
{"text": "6.SemEval：词义消歧评测SemEval  是由  ACL  词汇与语义小组组织的词汇与语义计算领域的国际权威技术竞赛。"}
{"text": "从1998年开始举办，竞赛包括多方面不同的词汇语义评测任务，如文本语义相似度计算、推特语义分析、空间角色标注、组合名词的自由复述、文本蕴涵识别、多语种的词义消歧等。"}
{"text": "2018的Sameval比赛包含12个任务，主要包括以下几方面的内容：（1）推特情感与创造性语句分析。"}
{"text": "该部分的处理对象来自推特的社交文本数据，其中涵盖英语、阿拉伯语以及西班牙语等多种语言的文本。"}
{"text": "分析的定位包括情感分析（情感的强弱、喜怒哀乐等类型的判断、情绪的积极消极以及识别推文中涵盖的多个情感类型）、符号预测（预测推文中可能嵌入的表情图片或颜文字）、反讽语义识别（识别推文中的讽刺表达）。"}
{"text": "（2）实体关联。"}
{"text": "该部分包含两个子任务。"}
{"text": "一个子任务是多人对话中的人物识别，目标是识别对话中提及的所有人物。"}
{"text": "值得一提的是，这些人物并不一定是对话中的某个谈话者，可能是他们提及的其他人。"}
{"text": "如图8-18所示的多人对话场景，Ross 提到的“mom”并不是参与对话的某人，而是Judy。"}
{"text": "如何有效地识别出对话中提及人物的字符具体指向什么人物实体，是本任务需要解决的重要问题之一。"}
{"text": "另一个子任务则是面向事件的识别以及分析，针对给定的问题，从给定文本中找出问题相关的一个事件或多个事件，以及参与角色之间的关系。"}
{"text": "图8-18 多人对话场景示例（3）信息抽取。"}
{"text": "该部分介绍的信息抽取包含关系（关系抽取与分类）、时间（基于语义分析的时间标准化）等。"}
{"text": "如图8-19所示为时间信息的语义解析示例，对于文本“metevery other Saturday since March 6”，其中的时间信息被解析为时间点与时间段并标准化表示出来。"}
{"text": "图8-19 时间信息的语义解析示例（4）词汇语义学。"}
{"text": "该部分从词汇语义的角度入手，提出了用于反映词汇之间高度关系的上位调发现以及判别属性识别。"}
{"text": "与传统计算词汇语义相似不同，本任务关注词的语义相异性，目标是预测一个词是其他词的一个判别属性。"}
{"text": "例如，给定词语“香蕉”与“苹果”，词语“红色”可以作为判别属性区分两者的相异性。"}
{"text": "红色是苹果的一个颜色属性，但是与香蕉无关。"}
{"text": "（5）阅读理解与推理。"}
{"text": "该部分由两个子任务构成，一个子任务是研究任务包括如何利用常识完成文本阅读理解，另一个子任务是通过推理方式对给定的由声明和理由组成的论点，从两个候选论据中选出正确的论据。"}
{"text": "8.5 KBQA前沿技术目前还存在两个很大的困难阻碍着  KBQA  系统被广泛应用。"}
{"text": "一个困难是现有的自然语言理解技术在处理自然语言的歧义性和复杂性方面还显得比较薄弱。"}
{"text": "例如，有时候一句话系统可以理解，但是换一个说法就不能理解了。"}
{"text": "另一个困难是此类系统需要大量的领域知识来理解自然语言问题，而这些一般都需要人工输入。"}
{"text": "一些系统需要开发一个专用于一个领域的基于句法或者语义的语法分析器。"}
{"text": "许多系统都引入了一个用户词典或者映射规则，用来将用户的词汇或说法映射到系统本体的词汇表或逻辑表达式中。"}
{"text": "通常还需要定义一个世界模型（World  Model），来指定词典或本体中词汇的上下位关系和关系参数类型的限制。"}
{"text": "这些工作都是非常消耗人力的。"}
{"text": "以下围绕  KBQA  的关键阶段——“构建查询”，说明KBQA面临的挑战，然后介绍几种典型的解决方案。"}
{"text": "8.5.1 KBQA面临的挑战图8-20反映了  KBQA  中一个简化的“问题→答案”映射过程，自然语言问题在关联知识库之前，需要转换成结构化查询，利用查询从知识图谱中找到答案后，还需要考虑一个自然语言答案生成的过程。"}
{"text": "这个过程中的主要挑战在于如何将自然语言表达映射到知识库的查询，也就是Question2Query语义理解。"}
{"text": "图8-20 问题到答案的映射过程1.多样的概念映射机制也就是将自然语言表达的查询语义映射知识库的原子查询。"}
{"text": "自然语言的表达的语义包罗万象，常见语义映射现象如表 8-3所示。"}
{"text": "表8-3 常见的语义映射现象2.不完美的知识库首先，知识库未必全都是结构化的数据，还有大量的知识存在于文本中。"}
{"text": "这需要有动态知识抽取解决方案。"}
{"text": "其次，知识库的知识组织机制各不相同，同样的知识在不同的知识库中未必会采用同样的结构，例如三元组（英国，加入欧盟的时间，1973）等价于四个三元组（事件1，加入方，英国）（事件1，被加入方，欧盟）（事件1，年份，1973）（事件1，类型，加入组织），这样也为查询制造了困难。"}
{"text": "再次，用户使用的语言以及知识库采用的工作语言也会影响语义理解，例如用中文查询英文的  DBpedia，从中文的关系名称映射到英文的实体属性就不简单。"}
{"text": "最后，知识库本身并不是完整的，而用户的预期却是希望能找到答案，这样如何判定找不到答案从而避免答非所问也是很重要的。"}
{"text": "3.泛化语义理解的预期当用户使用知识问答时，常见的抱怨就是同一个问题换一种说法就无法理解了。"}
{"text": "这个问题在智能客服中尤其明显，在保障精确度的前提下智能客服应该匹配尽量可解答的问题。"}
{"text": "泛化问题通常可以从词语和句子两个层面来研究。"}
{"text": "（1）词语层面的泛化匹配[44]。"}
{"text": "①  命名实体的不同说法，例如“上海”对应“沪”，需要从网络或领域专家获取背景知识，而“交通银行股份有限公司”可以通过简单的规则得到简称“交通银行”。"}
{"text": "②  生成实体（日期，地址等）的不同说法。"}
{"text": "例如“2018年1月1日”和“2018年元旦”。"}
{"text": "注意，生成实体的识别和解析可以通过常规的语法分析工具达成，但是中英文数字的混合、语音识别错误等现象会令解析难度提升。"}
{"text": "③  实体分类和属性或关系的不同说法。"}
{"text": "例如“还活着吗”对应“死亡日期”，这样的平行语料学习不但可以通过基于知识图谱的关系抽取结果来充实，也可以利用深度学习的分布式表示Embedding来计算。"}
{"text": "另外，这些语料的目标是建立从自然语言表示到知识图谱表示的映射，所以部分词汇还应该直接映射到知识图谱的实体分类和实体（描述或关系）属性上。"}
{"text": "还要注意对知识图谱本体的语义融合归一化处理，例如在  Wikidata  里没有统一的“水果”分类，这样就不能通过简单的实体分类获取完整的水果列表。"}
{"text": "（2）句子层面的泛化处理。"}
{"text": "主要是判断问题的语义相似度（Question-QuestionSimilarity）[44]，常用思路通常采用语言模型、机器翻译模型、句子主题分析模型、句子结构相似度分析模型、基于知识图谱的句子成分相似度模型等，SemEval  的  Task1  和Task3  SubTaskB[45]都对这一方面的关键技术做了评测。"}
{"text": "句子问题相似度算法可以被封装为独立的计算模块，然后将语法分析和前面基于知识图谱的语义解析结果作为特征交给基于LSTM的模型[46]计算相似度。"}
{"text": "8.5.2 基于模板的方法基于模板（Template）或模式（Pattern）的问答系统定义了一组带变量的模板，直接匹配问题文本形成查询表达式。"}
{"text": "这样简化了问题分析的步骤，并且通过预制的查询模板替代了本体映射。"}
{"text": "这样做的优势包括：简单可控，适于处理只有一个查询条件的简单问题；绕过了语法解析的脆弱性。"}
{"text": "这个方案在工业中得到广泛的应用。"}
{"text": "图8-21描述了一个  TrueKnowledge[47]模板示例，其中包含了以下步骤，首先使用已知的模板成分匹配句子中的内容，包括疑问词（What、Which，反映问题的意图），以及部分已知的模板（is a present central form of，某些固定表达词组），对于未知成分则使用变量字符加以替换（固定表达前后的  a、y  等），这种模板可以实现一对多的问题覆盖效果。"}
{"text": "但是其缺点也很明显：成熟的应用需要生成大量的模板，True Knowledge 就依赖手工生成了1200个模板，人工处理成本非常高昂；模板由人工生成，不易复用即一个问题可以用多个不同的模板回答，且需要通过全局排序来调优，容易发生冲突；即使生成的模板遵循知识库的  Schema，但由于知识库自身的不完整性以及语义组合的多样性，这些模板也未必就能保障能在知识库中找到答案。"}
{"text": "注意，TrueKnowledge  利用用户交互的界面降低人工编辑成本，让用户自己将系统无法回答的问题说法链接到一个相关的问题模板上，因此有效地减少了模板的生成数量。"}
{"text": "图8-21 TrueKnolwedge 的模板举例[47]TBSL[48]在QALD  2012测评任务中提出了一种联合使用语义结构分析以及自然语言词汇—URI 间映射的问答方法。"}
{"text": "根据模板匹配结果生成多组可能的  SPARQL  查询，通过筛选这些查询，最终生成答案并返回给用户。"}
{"text": "在基于模板的知识问答框架中，模板一般没有统一的标准或格式，只需结合知识图谱的结构以及问句的句式进行构建即可。"}
{"text": "TBSL中的模板定义为SPARQL查询模板。"}
{"text": "图8-22 典型的TBSL框架流程TBSL  方法有两个重要的步骤：模板生成和模板实例化。"}
{"text": "模板生成步骤解析问句结构并生成对应的  SPARQL  查询模板，该查询模板中可能包含过滤和聚合操作。"}
{"text": "生成模板时，首先需要获取自然语言问题中每个单词的词性标签，然后基于词性标签和语法规则表示问句，接下来利用与领域相关或与领域无关的词汇辅助分析问题，最后将语义表示转化为SPARQL模板。"}
{"text": "同一条自然语言问句可能对应着不止一条查询模板。"}
{"text": "因此，TBSL就查询模板的排序也提出了一种方法：首先，每个实体根据字符串相似度以及显著度获得一个打分；其次，根据填充槽的多个实体的平均打分得到一个查询模板的分值。"}
{"text": "在此基础上，需要检查查询的实体类型。"}
{"text": "形式化来说，对于所有的三元组 ？x rdf: type <class>，对于查询三元组？x  p  e和e  p  ?x，我们需要检查p的定义域（domain）和值域（range）是否与<class>一致。"}
{"text": "模板实例化步骤将自然语言问句与知识库中的本体概念建立映射。"}
{"text": "对于Resources和  Classes，实体识别的常用方法主要有两点，一是用WordNet定义知识库中标签的同义词，二是计算字符串间的相似度。"}
{"text": "对于属性标签，还需要与存储在模式库中的自然语言表示进行比较。"}
{"text": "最高排位的实体将作为填充查询槽位的候选答案。"}
{"text": "1.问题“列举所有的电影出品人”2.模板生成3.资源绑定TBSL  仍然存在的缺点是创建的模板结构未必和知识图谱中的数据契合。"}
{"text": "另外，考虑到数据建模的各种可能性，对应到一个问题的潜在模板数量会非常的多，同时手工准备海量模板的代价也非常大。"}
{"text": "针对此问题，CUI等人[49]针对简单事实问答模板的大规模生成，在自动化处理方面做了进一步优化，如图8-23所示。"}
{"text": "离线过程（Offline  Procedure）侧重基于问题生成模板。"}
{"text": "利用  NER  结果推算简单二元事实问题（Binary  Factorid  Question,BFQ）模板，将问题原文中的实体  e  替换为  e  的实体分类，多个实体分类可生成多个模板；基于DBpedia  知识图谱和  Yahoo  Answer社区问答对数据的训练数据，利用远程监督技术建立从问题到知识图谱查询的映射。"}
{"text": "模板映射支持  BFQ，即询问知识图谱中的三元组，例如“how  many  people  are  there  in  Honolulu?”（实体的描述属性）或者“what  is  thecapital  of  China”（实体的关系属性）。"}
{"text": "同时，模板映射也支持有特色的问题：排序，例如“which  city  has  the  3rd  largest  population?”；对比，例如“which  city  has  more  people,Honolulu  or  New  Jersey?”；列表，例如“list  cities  ordered  by  population”。"}
{"text": "此外，复杂的问题可以利用语法分析技术，先将问题拆分为多个 BFQ，然后再到本体中逐个映射到属性，最后再从这些结果中挑选合理的组合。"}
{"text": "例如，“when  was  Barack  Obama's  wifeborn?”可以拆分为who's  Barack  Obama's  wife?（Michelle  Obama）和when  was  MichelleObama  born?（1964）。"}
{"text": "离线过程采用E-M方法计算条件概率分布P（p|t）,p为属性，t为模板。"}
{"text": "在线过程（Online  Procedure）侧重模板选择。"}
{"text": "通过概率计算给定问题的最优答案。"}
{"text": "基于给定问题  q0，可以提取出  c1个实体，每个实体至多有  c2个实体分类，因而至多有  c3个模板，这些实体至多有 p 个属性（p 为知识库里的所有属性），而每个（实体，属性）c4个值。"}
{"text": "其中，c1、c2、c3、c4都是常数，所以寻求实体的时间复杂度为对最多对应 O（p），这意味每个问题都能快速得到解答，文中报告在线过程回答单个问题的平均时间为79ms。"}
{"text": "要注意的是，这里还包括高效率的内存知识图谱查询引擎。"}
{"text": "这种基于  BFQ  模板的解决方案提升了自动化处理程度，基于2782个意图从语料中学习生成了2700万个模板。"}
{"text": "当然，BFQ也未必能覆盖用户的所有问题。"}
{"text": "图8-23 CUI等人提出的基于模板的KBQA的架构图及示例[49]为了解决人工定义模板成本高的问题，Abujabal 等人[50]提出了 QUINT 模型，可以基于语料自动学习模板，然后基于生成的模板将自然语言查询转换成知识库查询。"}
{"text": "该方法在WebQuestions数据集上取得了接近最好成绩的效果，在  Free917数据集上取得了当时最好的效果，同时人工监督的工作量也是最少的。"}
{"text": "总的来说，基于模板方法的优点是模板查询的响应速度快、准确率较高，可以回答相对复杂的复合问题，而缺点是模板结构通常无法与真实的用户问题相匹配。"}
{"text": "如果为了尽可能匹配上一个问题的多种不同表述，则需要建立庞大的模板库，耗时耗力且会降低查询效率。"}
{"text": "8.5.3 基于语义解析的方法基于语义解析的方法是指通过对自然语言查询的语法分析，将查询转换成逻辑表达式，然后利用知识库的语义信息将逻辑表达式转换成知识库查询，最终通过查询知识库得到查询结果。"}
{"text": "逻辑表达式是语义解析方法与基于模板的方法的主要差异。"}
{"text": "逻辑表达式是面向知识库的结构化查询，用于查找知识库中的实体及实体关系等知识。"}
{"text": "相比于模板预先生成且固定的表达方式，逻辑表达式作为人工智能知识表示的经典传承，具备更完备、灵活的知识查询生成体系，包括带参数的原子逻辑表达式，以及基于操作组合的复杂逻辑表达式。"}
{"text": "原子级别的逻辑表达式通常可分为一元形式（unary）与二元形式（binary），其中一元形式匹配知识库中的实体，二元形式匹配实体之间的二元关系。"}
{"text": "这两种原子逻辑表达式可以利用连接（Join）、求交集（Intersection）及聚合统计（Aggregate）等操作进一步组合为复杂逻辑表达式。"}
{"text": "自然语言转化逻辑表达式需要训练一个语法分析器将过程自动化。"}
{"text": "应注意两个关键步骤：资源映射和逻辑表达式生成。"}
{"text": "资源映射即将自然语言查询中的短语映射到知识库的资源（类别、关系、实体等），根据处理难度分为简单映射和复杂映射两类。"}
{"text": "简单映射是指字符形式上比较相似的，一般可以通过字符串相似度匹配来找到映射关系，例如“出生”和“出生地”的映射。"}
{"text": "复杂映射是指无法通过字符串匹配找到对应关系的映射，例如“老婆”与“配偶”的映射，这类映射在实际问答中出现的概率更高，一般可以采用基于统计的方法来找到映射关系。"}
{"text": "逻辑表达式生成即自底向上自动地将自然语言查询解析为语法树，语法树的根节点即是最终对应的逻辑表达式。"}
{"text": "如图8-24所示，查询“where  was  Obamaborn”对应的逻辑表达式是  Type.Location ⊓ PeopleBornHere.BarackObama，其中  lexicon 是指资源映射操作，PeopleBornHere  和  BarackObama  用Join  连接组合，此组合结果再与Type.Location 用求交集组合成为最终的逻辑表达式。"}
{"text": "图8-24 自然语言查询转换成逻辑表达式[41]训练语法分析器需要大量的标注数据，传统的方法是基于规则生成标注数据，通过手工编写规则虽然直接，但是存在较明显的局限性：一方面，规则的编写需要语言学专家完成，导致规则的建立效率低且成本高，还不具备扩展性；另一方面，这种人工规则可能仅适用于某一类语言甚至某一特定领域，泛化能力较弱。"}
{"text": "为了改进传统方法的缺陷，有大量研究工作采用弱监督或者无监督的方法来训练语法分析器，一个经典的方法是  Berant[41]提出利用“问题/答案对”数据结合  Freebase  作为语法分析器的训练集。"}
{"text": "此方法不需要逻辑表示式的专家人工标注数据，可以低成本地获得。"}
{"text": "Berant  等人[41]提出的方法重点解决了逻辑表达式生成过程中的四个问题：资源映射（Alignment）、桥接操作（Bridging）、组合操作（Composition）和候选逻辑表达式评估。"}
{"text": "（1）资源映射。"}
{"text": "自然语言实体到知识库实体的映射相对比较简单，属于简单映射，但自然语言关系短语到知识库关系的映射相对复杂，属于复杂映射。"}
{"text": "例如将“where  wasObama born”中的实体 Obama 映射为知识库中的实体 BarackObama,Berant 在文中直接使用字符串匹配的方式实现实体的映射，但是将自然语言短语“was  also  born  in”映射到相应的知识库实体关系 PlaceOfBirth  则运用了基于统计的方法。"}
{"text": "首先从文本中收集了大量(Obama,  was  also  born  in,  August  1961)这样的三元组，然后将三元组中的实体进行对齐和并将常量进行归一化，把三元组转换成(BarackObama,  was  also  born  in,  1961-08)这样的标r[t1,t2]的形式，例准形式，再通过知识库得到三元组中实体的类型，将三元组转换成 如“was  also  born  in[Person,  Date]”。"}
{"text": "如图8-25所示，左边的“grew  up  in”是三元组中的自然语言关系短语 r1，右边的“DateOfBirth”是知识库中的关系 r2。"}
{"text": "统计所有自然语言三元组中符合  r1[t1,t2]的实体对，得到集合  F(r1)，统计知识库中符合  r2[t1,t2]的实体对，得到集合F(r2)。"}
{"text": "通过比较集合 F(r1)和集合 F(r2)类似 Jaccard 距离特征确定是否建立 r1与 r2的资源映射。"}
{"text": "图8-25 关系短语映射到知识库关系的方法[41]（2）桥接操作。"}
{"text": "在完成资源映射后仍然存在一些问题，首先，例如 go、have、do 等轻动词（Light  Verb）由于在语法上使用相对自由，难以通过统计的方式直接映射到实体关系上；其次，部分知识库关系的出现频率较低，利用统计也较难找到准确的映射方式。"}
{"text": "这样就需要补充一个额外的二元关系将这些词两端的逻辑表达式连接起来，这就是桥接操作。"}
{"text": "如图8-26所示，“Obama”和“college”映射为  BarackObama和Type.University，但是“goto”却难以找到一个映射，需要寻找一个二元关系 Education  使得查询可以被解析为Type.University ⊓ Education.BarackObama 的逻辑表达式。"}
{"text": "由于知识库中的关系是有定义域和值域的，所以文献基于此特点在知识库中查找所有潜在的关系，例如  Education  的定义域和值域分别是  Person  和  University，则  Education  可以是候选的桥接操作。"}
{"text": "这里针对每一种候选的桥接操作都会生成很多特征，基于这些特征训练分类器，用于最后的候选逻辑表达式评估。"}
{"text": "图8-26 桥接操作示例[41]（3）组合操作。"}
{"text": "即逻辑表达式间的连接、求交集以及聚合三种操作。"}
{"text": "至于最终应该用哪种操作，作者同样通过收集大量的上下文特征，基于这些训练分类器，用于最后的候选逻辑表达式评估。"}
{"text": "（4）候选逻辑表达式评估。"}
{"text": "即训练一个分类器，计算每一种候选逻辑表达式的概率， Discriminative Log-Linear模型，最终实现逻辑表达式的筛选。"}
{"text": "Berant 等人基于前面候选逻辑表达式生成过程中的所有特征，训练了一个8.5.4 基于深度学习的传统问答模块优化基于深度学习的知识问答主要有两个方向，分别是利用深度学习对传统问答方法进行模块级的改进和基于深度学习的端到端问答模型。"}
{"text": "深度学习可以直接用于改进传统问答流程的各个模块，包括语义解析、实体识别、意图分类和实体消歧等。"}
{"text": "实体识别模块可以使用 LSTM+CRF 以及近来兴起的 BERT 提升实体识别正确率；在关系分类、意图分类模块方面，可以使用基于字符级别的文本分类深度学习方法，甚至针对语言和领域提供预训练模型；实体消歧模块也可以使用基于深度学习的排序方法判定一组概念的语义融洽度。"}
{"text": "下面通过 Yih[51]的工作，说明如何使用深度神经网络来提升知识问答的效果。"}
{"text": "传统的基于语义解析的方法需要将问题转换成逻辑表达式，如图8-27所示。"}
{"text": "这类方法最大的问题是找到问题中自然语言短语与知识库的映射关系，Yih  等人提出了一种语义解析的框架，首先基于问句生成对应的查询图（Query  Graph），然后用该查询图在知识库上进行子图匹配，找到最优子图即找到问题的答案。"}
{"text": "因为查询图可以直接映射到  Lambda  Calculus形式的逻辑表达式，并且在语义上与  λ-DCS（Lambda  Dependency-Based  CompositionalSemantics）紧密相关，因此就可以将语义解析的过程转换成查询图生成的过程。"}
{"text": "图8-27 通过逻辑表达式转化成知识库查询的过程查询图由四种节点组成，包括实体（Grounded Entity）、中间变量（ExistentialVariable）、聚合函数（Aggregation  Function）和Lambda变量（Lambda  Variable），图8-28是一个查询图示例，其中实体在图中用圆角矩形表示，中间变量在图中用白底圆圈表示，聚合函数用菱形表示，Lambda  变量（即答案节点）用灰底圆圈表示。"}
{"text": "这个例子对应的问句是“Who first voiced Meg on Family Guy?”，在不考虑聚合操作的情况下，该查询图对应的逻辑表达式是λx.∃y.cast(FamilyGuy, y)∧ actor(y, x)∧ character(y, MegGriffin)。"}
{"text": "图8-28 查询图示例[51]下面介绍查询图的生成过程。"}
{"text": "第一步，选择一个主题实体（Topic  Entity）作为根节点，如图8-29（a）中可以选择 s1“Family Guy”作为根节点。"}
{"text": "第二步，确定一条从根节点到Lambda  变量（答案节点）的有向路径，路径上可以有一个或者多个中间变量，这条路径被称为核心推断链（Core  Inferential  Chain），如图8-29（b）所示从三条路径s3、s4、s5中选取s3作为核心推断链。"}
{"text": "核心推断链上除了根节点为实体，其他的都只能是变量，节点间的关系都是知识库中的关系。"}
{"text": "第三步，给查询图添加约束条件和聚合函数（AugmentingConstraints  &  Aggregations），形式上就是把其他的实体或者聚合函数节点通过知识库中的关系与核心推断链上的变量连接起来，如图8-29（c）所示对  y  增加两个限制argmin和character(y, MegGriffin)。"}
{"text": "图8-29 查询图的生成过程[51]对于生成查询图的第二步，需要一种从众多候选核心推断链中选出最优核心推断链的方法，针对图8-29（b）的例子，要评估{cast-actor,writer-start,genre}三个谓语序列中哪个最接近问题中“Family  Guy”和“Who”的关系，该文献使用一个  CNN  网络将候选序列和问题文本中的关键词向量化，CNN  结构如图8-30所示，通过语义相似度计算找到最优的核心推断链。"}
{"text": "具体做法是将自然语言问题和谓语序列分别通过图8-30所示的网络得到两个300维的分布式表达，然后利用表达向量之间的相似度距离（如cosine距离）计算自然语言问题和谓语序列的语义相似度得分。"}
{"text": "该 CNN 网络的输入运用了词散列技术[52]，将句子中每个单词拆分成字母三元组，每个字母三元组对应一个向量，比如单词  who  可以拆为#-w-h,w-h-o,h-o-#，每个单词通过前后添加符号#来区分单词界限。"}
{"text": "然后通过卷积层将3个单词的上下文窗口中的字母三元组向量进行卷积运算得到局部上下文特征向量ht，通过最大池化层提取最显著的局部特征，以形成固定长度的全局特征向量  v，然后将全局特征向量 v 输送到前馈神经网络层以输出最终的非线性语义特征 y，作为自然语言问题或核心推断链的向量表示。"}
{"text": "图8-30 Yih[51]中的CNN结构8.5.5 基于深度学习的端到端问答模型端到端的深度学习问答模型将问题和知识库中的信息均转化为向量表示，通过向量间的相似度计算的方式完成用户问题与知识库答案的匹配。"}
{"text": "首先根据问题中的主题词在知识库中确定候选答案，然后把问题和知识库中的候选答案都通过神经网络模型映射到一个低维空间，得到它们的分布式向量（Distributed  Embedding），则可计算候选答案分布式向量与问题向量的相似度得分，找出相似度最高的候选答案作为最终答案。"}
{"text": "该神经网络模型通过标注数据对进行训练，使得问题向量与知识库中正确答案的向量在低维空间的关联得分尽量高。"}
{"text": "典型的工作有Bordes  A等人[53]提出的方法，为解决WebQuestions上数据量不够的问题，文献作者使用一些规则从 Freebase、ClueWeb 等知识库中构建了大量（问题，知识库答案）的标注数据用于训练模型。"}
{"text": "如图8-31所示，自底向上计算。"}
{"text": "第一步，利用实体链接定位问题中的核心实体，对应到Freebase的实体；第二步，找到从问题中核心实体到候选答案实体的路径；第三步，生成候选答案的子图；第四步，分别将问题和答案子图映射成Embedding向量；第五步，进行点积运算，获得候选答案和问题之间的匹配度。"}
{"text": "该方法取得了比Berant[41]更好的结果（F1=0.392, P@1=0.40）。"}
{"text": "图8-31 Bordes A等人提出方法的核心流程[53]另一个基于  Multi-Column  CNN[54]的工作，该工作同时训练自然语言问句词向量与知识库三元组，将问题与知识库映射到同一个语义空间。"}
{"text": "该工作针对知识库的特点，定义了答案路径（Answer  Path）、答案上下文（Answer  Context）和答案类型（Answer  Type）三类特征，每一类特征都对应一个训练好的卷积神经网络，以此计算问题和答案的相似度。"}
{"text": "这三个  CNN  被称为多列卷积神经网络（Multi-Column  Convolutional  Neural  Network,Multi-Column  CNN）。"}
{"text": "该方法的核心流程如图8-32所示，对于问题“when  did  Avatarrelease  in  UK”，首先通过  Multi-Column  卷积神经网络提取该问题的三个分布式向量。"}
{"text": "然后利用命名实体识别、实体链接等技术，从问题文本中找到能链接到知识库的实体，与该实体相关联的每一个实体都是候选答案实体；再基于候选答案实体形成三个分布式向量，包括斜线矩形（“Avatar”）对应主题词路径向量，虚线椭圆（“UnitedKingdom”“film.file_region”）对应上下文向量，“datetime”对应答案类型向量。"}
{"text": "最后，通过分别点乘运算再求和的方式得到最终的答案-问题对得分。"}
{"text": "在实验中，该方法取得了当时最好的效果（F1=0.408,P@1=0.45)。"}
{"text": "图8-32 基于Multi-Column CNN方法的核心流程[54]8.6 开源工具实践8.6.1 使用Elasticsearch搭建简单知识问答系统本书第7章介绍了如何基于 Elasticsearch 实现简单的语义搜索，本节则基于Elasticsearch  展示简单的知识问答系统。"}
{"text": "两个案例的基本框架一致，而知识问答增加了将自然语言问题转化为对应逻辑表达式以及查询语句的过程。"}
{"text": "因此，本小节通过一个简单案例介绍自然语言问题到  Elasticsearch  查询语句的转化，而用  Elasticsearch  查询语句进行查询即可得到问答结果。"}
{"text": "注意，真实的知识问答系统的语义理解远比本文方案复杂。"}
{"text": "自然语言问题对应的查询类型同本书第7章中的语义检索，如表8-4所示，主要包含四种类型的查询，即实体检索、实体属性检索、实体属性的多跳检索以及多种属性条件检索实体。"}
{"text": "表8-4 自然语言问题的四种类型自然语言问题转化为逻辑表达式的过程如下：（1）定义逻辑表达式模板。"}
{"text": "如表8-5所示，逻辑表达式的基本元素是三元组的成分，包含S（Subject，主语）、P（Predicate，谓语）和O（Object，宾语）。"}
{"text": "当P是属性时，可以定义属性条件的运算，相关运算符（OP）包括“<”（小于）、“>”（大于）、“<=”（小于或等于）、“>=”（大于或等于）、“:”（属性），属性条件形式表示为“<P> <O>”，例如“职业：演员”,“身高>180”。"}
{"text": "多个属性条件之间可以用逻辑链接符“And”和“Or”连接，表示条件间并且和或者的关系，例如“职业：作家 And 身高>180”。"}
{"text": "<OP>表8-5 自然语言问题对应的逻辑表达式模板（2）解析自然语言问题。"}
{"text": "从自然语言问题中识别出实体名、属性名和属性值等三类要素，并将实体名和属性名映射到知识库中的实体和属性。"}
{"text": "首先，实体和属性的识别可以采用词典的方法，例如从知识库中抽取所有的实体名和属性名，构建分词器的自定义词典。"}
{"text": "然后，对自然语言问题进行分词，可直接识别其中的属性名和实体名。"}
{"text": "其次，属性值的识别比较困难，由于取值范围变化较大，可以采用模糊匹配的方法，也可以采用分词后n-gram检索Elasticsearch的办法。"}
{"text": "最后，查看自然语言问题中属性值和属性名的对应关系，当某属性值没有对应的属性名时，例如“（国籍是)中国(的)运动员”，缺省了“国籍”，就用该属性值对应的最频繁的属性名作为补全的属性名。"}
{"text": "例如下面的两段代码，分别实现了属性名识别和实体名识别。"}
{"text": "（3）后生成逻辑表达式。"}
{"text": "在识别出自然语言问题中所有的实体名、属性名和属性值后，依据它们的数目及位置，确定问题对应的查询类型，以便基于逻辑表达式模板生成对应的逻辑表达式。"}
{"text": "逻辑表达式生成流程如下：查询中含有实体名。"}
{"text": "如果有多个属性名，那么是属性值的多跳检索；如果有一个属性名，则需判断实体名和属性名的位置及中间的连接词(“是”“在”“的”等)，若实体名在前，则是实体的属性查询，例如“姚明的身高”，若属性名在前，则是依据属性查询实体，例如“女儿是姚沁蕾”。"}
{"text": "查询中没有实体名，则认为是依据属性查询实体，需要根据所有属性名和属性值位置的相对关系确定它们之间的对应关系。"}
{"text": "如果缺少属性名但有属性值，则需补全对应的属性名；如果缺少属性值但有属性名，例如“身高大于180cm”，则需通过正则表达式识别出范围查询的属性值。"}
{"text": "工业应用中抽取属性也会采用文法解析器、序列化标注、数字识别与解析等技术。"}
{"text": "在生成逻辑表达式之后，可基于查询的类型及要素，直接用对应的  Elasticsearch  查询模板将逻辑表达式翻译成  Elasticsearch  查询。"}
{"text": "本方法定义了一组  Elasticsearch  查询模板，基于该模板将逻辑表达式按照一定的层次结构自动转换成Elasticsearch查询语句。"}
{"text": "如表8-6所示，对于实体属性查询，包括多跳检索，都是先检索实体，然后获取对应的属性。"}
{"text": "如表8-7所示，对于多个属性条件检索实体，先为每种单个的属性条件创建  Elasticsearch  查询，最后组合成完整的查询，表中part_query表示单个属性条件对应的部分查询。"}
{"text": "表8-6 查询类型与Elasticsearch查询模板的映射关系表8-7 多属性条件组合与Elasticsearch查询模板的映射关系8.6.2 基于gAnswer构建中英文知识问答系统本节进一步介绍一个真实的知识问答系统的架构与接口，帮助开发者理解如何使用知识问答系统。"}
{"text": "gAnswer  系统[55]是一个基于海量知识库的自然语言问答系统，针对用户的自然语言问题，能够输出SPARQL格式的知识库查询表达式以及查询答案的结果。"}
{"text": "gAnswer同时支持中文问答和英文问答。"}
{"text": "gAnswer参加了QALD-9的评测比赛，并取得了第一名的成绩。"}
{"text": "对于中文问答，使用PKUBASE知识库；对于英文问答，使用DBpedia知识库。"}
{"text": "本实践的相关工具、实验数据及操作说明由OpenKG提供，地址为http://openkg.cn。"}
{"text": "此外，我们给出了一个使用gAnswer进行英文问答的示例网站http://ganswer.gstore-pku.com/。"}
{"text": "如图8-33所示为  gAnswer  系统处理流程。"}
{"text": "主要分为三个阶段：构建语义查询图、生成  SPARQL  查询和查询执行。"}
{"text": "在构建语义查询图阶段，系统借助数据集的信息以及自然语言分析工具，对问句进行实体识别和关系抽取，构建语法依存树，并用这些结果构建对应的查询图。"}
{"text": "这时，并不对其中的实体和关系做消歧处理，而是利用谓词词典，记录词或短语可能对应的谓词或实体。"}
{"text": "在生成  SPARQL  查询阶段，系统利用查询图生成多个  SPARQL，并利用数据集中的部分信息对多个  SPARQL  进行过滤和优化，其中就包括歧义的消除。"}
{"text": "在查询执行阶段，借助gStore系统返回的SPARQL查询结果，返回并展示给用户。"}
{"text": "图8-33 gAnswer系统处理流程1.系统配置需求读者可以使用 gAnswer  系统构建自己的领域知识问答。"}
{"text": "在系统配置需求方面，gAnswer系统使用RDF格式的数据集，默认的中文数据集是PKUBASE，默认的英文数据集是DBpedia2016。"}
{"text": "gAnswer系统的运行需要借助支持SPARQL查询的图数据库系统来获取最终答案。"}
{"text": "在目前的版本中，使用 gStore 系统（http://openkg.cn/tool/gstore）。"}
{"text": "gAnswer的部署还依赖一些外部工具包。"}
{"text": "包括Maltparser、StanfordNLP，在生成SPARQL阶段，需要借助 Lucene 对辅助信息进行索引。"}
{"text": "gAnswer 为开发者提供了打包版本，安装流程如下：（1）下载Ganswer.jar。"}
{"text": "访问地址为https://github.com/pkumod/gAnswer/releases。"}
{"text": "（2）下载dbpedia16.rar数据文件。"}
{"text": "注意，完整的dbpedia16.rar数据文件需要较大的内存支持（20GB），也可以从  DBpedia  2016中选择下载抽取生成的小规模数据（5GB）。"}
{"text": "访问地址为https://github.com/pkumod/gAnswer/blob/master/README_CH.md。"}
{"text": "（3）在控制台下解压  Ganswer.jar。"}
{"text": "用户可以解压到任意文件路径下，但需要保证Ganswer.jar文件与解压得到的文件处在统一路径下。"}
{"text": "（4）在控制台下解压  data.rar。"}
{"text": "用户需要把解压得到的文件夹置于  Ganswer.jar  文件所在的路径下。"}
{"text": "（5）在控制台下运行Ganswer.jar，等待系统初始化结束，出现“Server  Ready!”字样后，说明初始化成功，使可以开始通过HTTP请求访问gAnswer服务了。"}
{"text": "（6）配置外部第三方 API 接口。"}
{"text": "目前的 gAnswer 系统需要借助一些外部系统接口。"}
{"text": "在公开的版本中，提供了远程的外部系统调用函数，因此用户并不需要在自己的计算机上安装这些外部系统。"}
{"text": "但是，开发者强烈建议用户安装自己的版本，以保证性能。"}
{"text": "gStore,qa.GAnswer.getAnswerFromGStore2()，版本大于或等于 https://github.com/pkumod/gStore。"}
{"text": "DBpediaLookup,qa.mapping.DBpediaLookup，访问地址为https://wiki.dbpedia.org/lookup/v0.7.0，访问地址为2.访问gAsnwer服务KBQA的问答接口与常规问答的差异主要在返回结果上，具体说就是返回结果可以包括找到的实体、知识图谱的子图等结构化信息，然后利用自然语言生成技术将结构化结果展示为自然语言格式。"}
{"text": "gAnswer可以通过RESTful  HTTP  API通过发送JSON格式的数据进行交互。"}
{"text": "另外，在 gAnswer 源代码的 application.gAnswerHttpConnector 中给出了使用Java访问gAnswer系统的示例。"}
{"text": "（1）配置输入参数。"}
{"text": "若提问“闻一多创作了哪些十四行诗？”输入参数如下：问题是“闻一多创作了哪些十四行诗？”要求最多返回3个不同的答案，1条生成的  SPARQL  查询。"}
{"text": "（2）调用服务。"}
{"text": "将此  JSON  格式的数据转化为字符串，进行  URL  转码，然后使用ip:port/gSolve/?data=%json  string%（在%json  string%处放入  JSON  数据字符串）这一  URI来调用gAnswer系统。"}
{"text": "本地运行IP为localhost，默认端口为9999。"}
{"text": "在样例中，实际访问的URI为：（3）解析返回结果。"}
{"text": "对于上例，返回的结果如下。"}
{"text": "需要特别说明的是，其中“vars”代表识别到的变量名，“results”中为实际得到的答案，“value”中为实际答案的值，“status”则说明这是一次正常的请求返回。"}
{"text": "3.在新的知识库上运行若更换知识库，使gAnswer系统在新的知识库上运行，需要更新查询引擎、离线索引和词典。"}
{"text": "下面具体说明。"}
{"text": "将新的知识库组织成三元组形式，如下所示：将  kb.txt  文件置于  data/kb/中。"}
{"text": "通过  gStore  查询引擎建立基于  kb.txt  的查询服务，访问地址为http://gstore-pku.com/。"}
{"text": "根据kb.txt生成实体、谓词、类型的列表文件，如下所示：将上述三个文件置于 data/kb/fragments/id_mappings entity_fragment.txt、src/fgmt/GenerateFragment.java，程序将产生三个编码后的碎片文件 predicate_fragment.txt  和type_fragment.txt并置于data/kb/fragments/中。"}
{"text": "以entity_fragment.txt为例，格式为中。"}
{"text": "运行示例如下：为提高效率，使用lucene建立索引。"}
{"text": "运行src/lcn/BuildIndexForEntityFragments.java和src/lcn/ BuildIndexForTypeShortName.java。"}
{"text": "程序会在data/kb/lucene下生成索引文件。"}
{"text": "提供新知识库的实体链接词典和谓词复述词典，示例如下所示：其中，“逆时针  逆时针_（张靓颖演唱歌曲）  2”是指短语“逆时针”可以链接到实体“逆时针_（张靓颖演唱歌曲）”，第三列数字2表示这个链接是在短语“逆时针”的所有链接中置信度处于第二高的；“地理位置 在哪里”表示短语“在哪里”可以匹配到谓词“地理位置”，数字“30”为该次匹配的置信度。"}
{"text": "mention2ent  和  pred2phrase  两个文件主要用来支撑实体链接和关系抽取两个子模块。"}
{"text": "将这两个词典文件置于data/kb/parapharse中。"}
{"text": "以上操作完成后，gAnswer即可在新的知识库上提供问答服务。"}
{"text": "8.7 本章小结本章介绍了问答系统的基本概念、主流方法以及评价体系，并详细阐述了知识图谱问答系统的主要方法与最新进展。"}
{"text": "知识问答以自然语言问答的方式简化了人们获取知识的过程，在知识检索过程中增加了泛化、联想、探索等智能化体验并拓展了知识获取的途径。"}
{"text": "KBQA作为知识问答的重要分支，一方面强化了针对结构化信息的检索能力，另一方面也可以利用知识图谱提升问题理解的准确性。"}
{"text": "深度学习技术在  KBQA  也起到了重要的作用，不但可以优化传统  KBQA  的各个模块，尤其是实体识别和语义相似度匹配，而且可以直接作为知识库表示支持端到端的知识问答。"}
{"text": "正如万维网是开放的一样，多种多样的领域知识是不可能被任何一家企业垄断的，所以知识问答应该走万维网一样的开放路线，允许不同的参与者形成生态体系。"}
{"text": "参与者可以从热门领域开始，从全局或细分覆盖不同领域的知识，提供不同特色的领域问答体验，这好比垂直领域的网站，进而组合形成跨领域的知识问答，最终通过一个开放的协作体系，完成全网的开放知识问答体验。"}
