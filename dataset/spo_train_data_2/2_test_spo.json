{"text": "表2-5OWL词汇扩展3.OWL版本目前，OWL2是OWL的最新版本，老的OWL版本也被称为OWL1。"}
{"text": "OWL2定义了一些OWL的子语言，通过限制语法使用，使得这些子语言能够更方便地实现，以及服务不同的应用。"}
{"text": "OWL2的三大子语言是OWL_2_RL、OWL_2_QL和OWL_2_EL。"}
{"text": "OWL_2_QL是OWL2子语言中最为简单的，QL代表Query_Language，所以OWL_2_QL是专为基于本体的查询设计的。"}
{"text": "它的查询复杂度是AC0，非常适合大规模处理。"}
{"text": "它是基于描述逻辑DL-Lite定义的。"}
{"text": "表2-6给出了OWL_2_QL词汇总结。"}
{"text": "表2-6OWL_2_QL词汇总结另外一个能够提供多项式推理的OWL是OWL_2_EL。"}
{"text": "与OWL_2_QL不同，OWL_2_EL专为概念术语描述、本体的分类推理而设计，广泛应用在生物医疗领域，如临床医疗术语本体SNOMED_CT。"}
{"text": "OWL_2_EL的分类复杂度是Ptime-Complete，它是基于描述逻辑语言EL++定义的。"}
{"text": "表2-7给出了OWL_2_QL词汇总结。"}
{"text": "表2-7OWL_2_QL词汇总结例如，OWL_2_EL允许表达如下复杂的概念：Female??likes.Movie??hasSon.(Student??attends.CSCourse)指的是所有喜欢电影、儿子是学生且参加计算机课程的女性。"}
{"text": "下面给出一个例子。"}
{"text": "假设有一个本体，包含以下公理：公理1.Apple??beInvestedBy.(Fidelity?BlackStone)：苹果由富达和黑石投资。"}
{"text": "公理2.?beFundedBy.Fidelity?InnovativeCompanies：借助富达融资的公司都是创新企业。"}
{"text": "公理3.?beFundedBy.BlackStone?InnovativeCompanies：借助黑石融资的公司都是创新企业。"}
{"text": "公理4.beInvestedBy?beFundedBy：投资即是帮助融资。"}
{"text": "由公理1可以推出公理5:Apple??beInvestedBy.Fidelity；由公理5和公理4可以推出公???beFundedBy.Fidelity；最后，由公理6和公理2可以推出公理7:Apple理6:Apple_InnovativeCompanies。"}
{"text": "还有一个推理复杂度是多项式时间的OWL2子语言叫OWL_2_RL。"}
{"text": "OWL_2_RL扩展了RDFS的表达能力，在RDFS的基础上引入属性的特殊特性（函数性、互反性和对称性），允许声明等价性，允许属性的局部约束。"}
{"text": "OWL_2_RL_的推理是一种前向链推理，即将推理规则应用到OWL_2_RL本体，得到新的知识，即OWL_2_RL推理是针对实例数据的推理。"}
{"text": "下面给出两个OWL_2_RL上的推理规则：p_rdfs:domain_x,spo?s_rdf:type_xp_rdfs:range_x,spo?o_rdf:type_x其中，s、p、o、x为变量。"}
{"text": "第一条规则表示如果属性p的定义域是类x，而且实例s和o有关系p（这里把属性与关系看成是一样的），那么实例s是类x的一个元素。"}
{"text": "第二条规则表示如果属性p的值域是类x，而且实例s和o有关系p，那么实例o是类x的一个元素。"}
{"text": "例如exp:hasChild_rdfs:domain_exp:Person,exp:Helen_exp:hasChild_exp:Jack，由第一条规则可以推出_exp:Helen_rdf:type_exp:Person。"}
{"text": "OWL_2_RL允许的核心词汇有：●rdfs:subClassOf；●rdfs:subPropertyOf；●rdfs:domain；●rdfs:range；●owl:TransitiveProperty；●owl:FunctionalProperty；●owl:sameAs；●owl:equivalentClass；●owl:equivalentProperty；●owl:someValuesFrom；●owl:allValuesFrom。"}
{"text": "OWL_2_RL的前向链推理复杂度是PTIME完备的，PTIME复杂度是针对实例数据推理得到的结果。"}
{"text": "2.3.3知识图谱查询语言的表示RDF支持类似数据库的查询语言，叫作SPARQL[1]，它提供了查询RDF数据的标准语法、处理SPARQL查询的规则以及结果返回形式。"}
{"text": "1.SPARQL知识图谱查询基本构成●变量，RDF中的资源，以“?”或者“$”指示；●三元组模板，在WHERE子句中列出关联的三元组模板，之所以称为模板，因为三元组中允许存在变量；●SELECT子句中指示要查询的目标变量。"}
{"text": "下面是一个简单的SPARQL查询例子：这个SPARQL查询指的是查询所有选修CS328课程的学生，PREFIX部分进行命名空间的声明，使得下面查询的书写更为简洁。"}
{"text": "2.常见的SPARQL查询算子（1）OPTIONAL。"}
{"text": "可选算子，指的是在这个算子覆盖范围的查询语句是可选的。"}
{"text": "例如：指的是查询所有选修CS328课程的学生姓名，以及他们的邮箱。"}
{"text": "OPTIONAL关键字指示如果没有邮箱，则依然返回学生姓名，邮箱处空缺。"}
{"text": "（2）FILTER。"}
{"text": "过滤算子，指的是这个算子覆盖范围的查询语句可以用来过滤查询结果。"}
{"text": "例如：指的是查询学生姓名、选修课程以及他们的年龄；如果有年龄，则年龄必须大于25岁。"}
{"text": "（3）UNION。"}
{"text": "并算子，指的是将两个查询的结果合并起来。"}
{"text": "例如：指的是查询选修课程CS328或CS909的学生姓名以及邮件。"}
{"text": "注意，这里的邮件是必须返回的，如果没有邮件值，则不返回这条记录。"}
{"text": "需要注意UNION和OPTIONAL的区别。"}
{"text": "下面给出一个SPARQL查询的例子。"}
{"text": "给定一个RDF数据集：以及一个SPARQL查询：这个SPARQL查询期望查询所有的收购关系，可以得到查询结果如表2-8所示。"}
{"text": "表2-8查询结果给定论文一个SPARQL查询：这个查询期望查询所有具备关联交易的公司。"}
{"text": "假设有下面两条规则：hold_share（X,Y）:-control（X,Y）conn_trans（Y,Z）:-hold_share（X,Y）,hold_share（X,Z）第一条规则指的是如果X控制了Y，那么X控股Y；第二条规则指的是如果X同时控股Y和Z，那么Y和Z具备关联交易。"}
{"text": "通过查询重写技术，可以得到下面的SPARQL查询：但是这个查询比较复杂，可以通过下面的SPARQL查询简化：在这个查询中，SPARQL允许嵌套查询，即WHERE子句中包含SELECT子句。"}
{"text": "2.3.4语义Markup表示语言语义网进一步定义了在网页中嵌入语义Markup的方法和表示语言。"}
{"text": "被谷歌知识图谱以及Schema.Org采用的语义Markup语言主要包括JSON-LD、RDFa和HTML5MicroData。"}
{"text": "1.JSON-LDJSON-LD（JavaScript_Object_Notation_for_Linked_Data）是一种基于JSON表示和传输链接数据的方法。"}
{"text": "JSON-LD描述了如何通过JSON表示有向图，以及如何在一个文档中混合表示链接数据及非链接数据。"}
{"text": "JSON-LD的语法和JSON兼容。"}
{"text": "下面是一个简单的JSON例子：JSON文档表示一个人。"}
{"text": "人们很容易推断这里的含义：“name”是人的名字，“homepage”是其主页，“image”是其某种照片。"}
{"text": "当然，机器不理解“name”和“image”这样的术语。"}
{"text": "JSON-LD通过引入规范的术语表示，例如统一化表示“name”、“homepage”和“image”的URI，使得数据交换和机器理解成为基础。"}
{"text": "如下所示：可以看出，JSON-LD呈现出语义网技术的风格，它们有着类似的目标：围绕某类知识提供共享的术语。"}
{"text": "例如，每个数据集不应该围绕“name”重复发明概念。"}
{"text": "但是，JSON-LD的实现没有选择大部分语义网技术栈（TURTLE/SPARQL/Quad单、不复杂以及面向一般开发人员的方式推进。"}
{"text": "Stores），而是以简2.RDFaRDFa（Resource_Description_Framework_in_attributes）是一种早期网页语义标记语言。"}
{"text": "RDFa也是W3C推荐标准。"}
{"text": "它扩充了XHTML的几个属性，网页制作者可以利用这些属性在网页中添加可供机器读取的资源。"}
{"text": "与RDF的对应关系使得RDFa可以将RDF的三元组嵌入在XHTML文档中，它也使得符合标准的使用端可以从RDFa文件中提取出这些RDF三元组。"}
{"text": "RDFa通过引入名字空间的方法，在已有的标签中加入RDFa相应的属性，以便解析支持RDFa技术的浏览器或者搜索引擎，从而达到优化的目的。"}
{"text": "上面的代码示例中用到了RDFa属性中的about属性和property属性。"}
{"text": "这段代码示例说明了一篇文章，然后描述了和这篇文章相关的信息，例如标题、创建者和创建日期，就可以让支持RDFa的机器识别这些属性。"}
{"text": "RDFa可以从机器可理解的层面优化搜索，提升访问体验以及网页数据的关联。"}
{"text": "3.HTML5_MicrodataMicrodata（微数据）是在网页标记语言中嵌入机器可读的属性数据。"}
{"text": "微数据使用自定义词汇表、带作用域的键值对给DOM做标记，用户可以自定义微数据词汇表，在自己的网页中嵌入自定义的属性。"}
{"text": "微数据是给那些已经在页面上可见的数据施加额外的语义，当HTML的词汇不够用时，使用微数据可以取得较好的效果。"}
{"text": "下面是一个HTML5Microdata的示例。"}
{"text": "这个例子给出了Person类下一个叫Andy的人的照片和URL地址。"}
{"text": "通过HTML5Microdata，浏览器可以很方便地从网页上提取微数据实体、属性及属性值。"}
{"text": "2.4常见开放域知识图谱的知识表示方法不同的知识图谱项目都会根据实际的需要选择不同的知识表示框架。"}
{"text": "这些框架有不同的描述术语、表达能力、数据格式等方面的考虑，但本质上有相似之处。"}
{"text": "这里以三个最典型的开放域知识图谱（Freebase、Wikidata、ConceptNet）为例，尝试比较不同的知识图谱项目选用的知识表示框架，并总结影响知识表示框架选择的主要因素。"}
{"text": "为便于比较分析，以RDF、OWL的描述术语和表达能力为主要比较对象。"}
{"text": "2.4.1FreebaseFreebase的知识表示框架主要包含如下几个要素：对象-Object、事实-Facts、类型-ID，称为MIDTypes和属性-Properties。"}
{"text": "“Object”代表实体。"}
{"text": "每一个“Object”有唯一的（Machine_ID）。"}
{"text": "一个“Object”可以有一个或多个“Types”。"}
{"text": "“Properties”用来描述“Facts”。"}
{"text": "例如，“Barack_Obama”是一个Object，并拥有一个唯一的MID:“/m/02mjmr”。"}
{"text": "这个Object是“/government/us_president”，并有一个称的一个为“/government/us_president/presidency_number”的Property，其数值是“44”。"}
{"text": "Freebase使用复合值类型（Compound_Value_Types,CVT）处理多元关系。"}
{"text": "type如图2-16所示，示例的CVT描述了关于Obama的任职期限的多元关系“government_position_held”。"}
{"text": "这个多元关系包含多个子二元关系：“office_holder”“office_position”“from”“to”等。"}
{"text": "一个CVT就是有唯一MID的Object，也可以有多个Types。"}
{"text": "为了以示区别，Freebase把所有非CVT的Object也称为“Topic”。"}
{"text": "Wikidata起源于Wikipedia，因此与Wikipedia一样，以页面“Page”为基本的组织单元。"}
{"text": "Entities类似于OWL:Things，代指最顶层的对象。"}
{"text": "每一个Entity都有一个独立的维基页面。"}
{"text": "Entities主要有两类：Items和Properties。"}
{"text": "Items类似于RDF中的Instance，代指实例对象。"}
{"text": "Properties和Statements分别等价于RDF中的Property和Statement。"}
{"text": "通常一个Item的页面还包含多个别名-aliases和多个指向Wikipedia的外部链接-Sitelinks。"}
{"text": "一个Statement包含一个Property、一个或多个Values、一个或多个Qualifiers、一个或多个References、一个标识重要性程度的Rank。"}
{"text": "修饰-Qualifiers用于处理复杂的多元表示。"}
{"text": "如一个陈述“spouse:Jane_Belson”描述了一个二元关系。"}
{"text": "可以使用Qualifiers给这个陈述增加多个附加信息来刻画多元关系，如“startdate:25_November_1991”and“end_date:11_May_2011”等。"}
{"text": "引用-References用于标识每个陈述的来源或出处，如来源于某个维基百科页面等。"}
{"text": "引用也是一种Qualifiers，通常添加到Statements的附加信息中。"}
{"text": "Wikidata支持多种数值类型，包括其自有的Item类型、RDF_Literal、URL、媒体类型Commons_Media，以及Time、Globe_coordinates和Quantity三种复杂类型。"}
{"text": "Wikidata允许给每个Statement增加三种权重：normal（缺省）、preferred和deprecated。"}
{"text": "Wikidata定义了三种Snacks作为Statement的具体描述结构：PropertyValueSnack、PropertyNoValueSnack、PropertySomeValueSnack。"}
{"text": "PropertyNoValueSnack类似于OWL中的Negation，表示类似于“Elizabeth_spouse”的知识。"}
{"text": "PropertySomeValueSnack类似于OWL中的存在量词someValuesFrom，表示类似于“PopeLinus_had_a_date_of_birth,but_it_is_unknown_to_us”这样的知识。"}
{"text": "of_England_had_no_I_Wikidata的URI机制遵循了Linked_Open_Data的URI原则，采用统一的URI机制：Item，如Q49，或者一个http://www.wikidata.org/entity/<id>。"}
{"text": "其中，<id>可以是一个Property，如P234。"}
{"text": "2.4.3ConceptNet5ConceptNet5的知识表示框架主要包含如下要素：概念-Concepts、词-Words、短语-Phrases、断言-Assertions、关系-Relations、边-Edges。"}
{"text": "Concepts由Words或Phrases组成，构成了图谱中的节点。"}
{"text": "与其他知识图谱的节点不同，这些Concepts通常是从自然语言文本中提取出来的，更接近自然语言描述，而不是形式化的命名。"}
{"text": "Assertions描述了Concepts之间的关系，类似于RDF中的Statements。"}
{"text": "Edges类似于RDF中的Property。"}
{"text": "一个Concepts包含多条边，而一条边可能有多个产生来源。"}
{"text": "例如，一个“化妆Cause漂亮”的断言可能来源于文本抽取，也可能来源于用户的手工输入。"}
{"text": "来源越多，该断言就越可靠。"}
{"text": "ConceptNet5根据来源的多少和可靠程度计算每个断言的置信度。"}
{"text": "ConceptNet5示例如图2-17所示。"}
{"text": "ConceptNet5中的关系包含21个预定义的、多语言通用的关系，如IsA、UsedFor等，以及从自然语言文本中抽取的更加接近自然语言描述的非形式化的关系，如of,caused_by等。"}
{"text": "on_top图2-17ConceptNet5示例ConceptNet5对URI进行了精心的设计。"}
{"text": "URI同时考虑了类型（如是概念还是关系）、语言、正则化后的概念名称、词性、歧义等因素。"}
{"text": "其中，n代指这是一个名词，basement用于区分歧义。"}
{"text": "在处理表示“x_is_the_first_argument_of_y”这类多元关系的问题上，ConceptNet5把所有关于某条边的附加信息增加为边的属性，如图2-18所示。"}
{"text": "图2-18ConceptNet5的知识表示结构2.5知识图谱的向量表示方法与前面所述的表示方法不同的是，本节要描述的方法是把知识图谱中的实体和关系映射到低维连续的向量空间，而不是使用基于离散符号的表达方式。"}
{"text": "2.5.1知识图谱表示的挑战在前面提到的一些知识图谱的表示方法中，其基础大多是以三元组的方法对知识进行组织。"}
{"text": "在具体的知识库网络中，节点对应着三元组的头实体和尾实体，边对应着三元组的关系。"}
{"text": "虽然这种离散的符号化的表达方式可以非常有效地将数据结构化，但是在当前的大规模应用上也面临着巨大的挑战。"}
{"text": "知识以基于离散符号的方法进行表达，但这些符号并不能在计算机中表达相应语义层面的信息，也不能进行语义计算，对下游的一些应用并不友好。"}
{"text": "数据具有一定的稀疏性，现实中的知识图谱无论是实体还是关系都有长尾分布的情况，也就是某一个实体或关系具有极少的实例样本，这种现象会影响某些应用的准确率。"}
{"text": "从上面的问题可以看出，对于当前的数据量较大的知识图谱、变化各异的应用来说，需要改进传统的表示方法。"}
{"text": "2.5.2词的向量表示方法在介绍有关知识图谱的向量表示方法之前，在此先介绍词的表示方法。"}
{"text": "在自然语言处理领域中，因为离散符号化的词语并不能蕴涵语义信息，所以将词映射到向量空间，这不仅有利于进行相应的计算，在映射的过程中也能使相关的向量蕴涵一定的语义。"}
{"text": "知识图谱中的向量表示方法也在此次有所借鉴。"}
{"text": "1.独热编码传统的独热编码（One-Hot_Encoding）方法是将一个词表示成一个很长的向量，该向量的维度是整个词表的大小。"}
{"text": "对于某一个具体的词，在其独热表示的向量中，除了表示该词编号的维度为1，其余都为0。"}
{"text": "如图2-19所示，假如词Rome的编号为1，则在其独热编码中，仅有维度1是1，其余都是0。"}
{"text": "这种表示方法虽然简单，但是可以看出其并没有编码语义层面的信息，稀疏性非常强，当整个词典非常大时，编码出向量的维度也会很大。"}
{"text": "2.词袋模型词袋模型（Bag-of-Words,BoW）是一种对文本中词的表示方法。"}
{"text": "该方法将文本想象成一个装词的袋子，不考虑词之间的上下文关系，不关心词在袋子中存放的顺序，仅记录每个词在该文本（词袋）中出现的次数。"}
{"text": "具体的方法是先收集所有文本的可见词汇并组成一个词典，再对所有词进行编号，对于每个文本，可以使用一个表示每个词出现次数的向量来表示，该向量的每一个维度的数字表示该维度所指代的词在该文本中出现的次数。"}
{"text": "如图2-20所示，在文本doc1中，Rome出现32次，Paris出现14次，France出现0次。"}
{"text": "图2-19独热编码示例1图2-20独热编码示例23.词向量上面对词的表示方法并没有考虑语义层面的信息，为了更多地表示词与词之间的语义相似程度，提出词的分布式表示，也就是基于上下文的稠密向量表示法，通常称为词向量或词嵌入（Word_Embedding）。"}
{"text": "产生词向量的手段主要有三种：●Count-based。"}
{"text": "基于计数的方法，简单说就是记录文本中词的出现次数。"}
{"text": "●Predictive。"}
{"text": "基于预测的方法，既可以通过上下文预测中心词，也可以通过中心词预测上下文。"}
{"text": "●Task-based。"}
{"text": "基于任务的，也就是通过任务驱动的方法。"}
{"text": "通过对词向量在具体任务上的表现效果对词向量进行学习。"}
{"text": "对词向量的产生方法到现在为止有较多的研究，在本章中并不展开讨论，下面简单介绍经典的开源工具word2vec[8]中包含的CBoW和Skip-gram两个模型。"}
{"text": "CBoW也就是连续词袋模型（Continuous_Bag-of-Words），和之前提到的BoW相似之处在于该模型也不用考虑词序的信息。"}
{"text": "其主要思想是，用上下文预测中心词，从而训练出的词向量包含了一定的上下文信息。"}
{"text": "如图2-21（a）所示，其中wn是中心词，wn?2,wn?1,wn+1,wn+2为该中心词的上下文的词。"}
{"text": "将上下文词的独热表示与词向量矩阵E相乘，提取相应的词向量并求和得到投影层，然后再经过一个Softmax层最终得到输出，输出的每一维表达的就是词表中每个词作为该上下文的中心词的概率。"}
{"text": "整个模型在训练的过程就像是一个窗口在训练语料上进行滑动，所以被称为连续词袋模型。"}
{"text": "Skip-gram的思想与CBoW恰恰相反，其考虑用中心词来预测上下文词。"}
{"text": "如图2-21（b）所示，先通过中心词的独热表示从词向量矩阵中得到中心词的词向量得到投影层，然后经过一层Softmax得到输出，输出的每一维中代表某个词作为输入中心词的上下文出现的概率。"}
{"text": "图2-21CBoW模型在训练好的词向量中可以发现一些词的词向量在连续空间中的一些关系，如图2-22所示。"}
{"text": "vec（Rome）?vec（Italy）≈vec（Paris）?vec（France）可以看出，Roma和Italy之间有is-capital-of的关系，而这种关系恰好也在Paris和France之间出现。"}
{"text": "通过两对在语义上关系相同的词向量相减可以得出相近的结果，可以猜想出Roma和Italy的词向量通过简单的相减运算，得到了一种类似is-capital-of关系的连续向量，而这种关系的向量可以近似地平移到其他具有类似关系的两个词向量之间。"}
{"text": "这也说明了经过训练带有一定语义层面信息的词向量具有一定的空间平移性。"}
{"text": "图2-22词向量在连续空间中的关系上面所说的两个词之间的关系，恰好可以简单地理解成知识图谱中的关系（relation）、（Rome,is-capital-of,Italy）和（Paris,is-capital-of,France），可以看作是知识图谱中的三元组（triple），这对知识图谱的向量表示产生了一定的启发。"}
{"text": "2.5.3知识图谱嵌入的概念为了解决前面提到的知识图谱表示的挑战，在词向量的启发下，研究者考虑如何将知识图谱中的实体和关系映射到连续的向量空间，并包含一些语义层面的信息，可以使得在下游任务中更加方便地操作知识图谱，例如问答任务[9]、关系抽取[10]等。"}
{"text": "对于计算机来说，连续向量的表达可以蕴涵更多的语义，更容易被计算机理解和操作。"}
{"text": "把这种将知识图谱中包括实体和关系的内容映射到连续向量空间方法的研究领域称为知识图谱嵌入（Knowledge_Graph（Representation_Learning）、知识表示学习。"}
{"text": "Embedding）、知识图谱的向量表示、知识图谱的表示学习类似于词向量，知识图谱嵌入也是通过机器学习的方法对模型进行学习，与独热编码、词袋模型的最大区别在于，知识图谱嵌入方法的训练需要基于监督学习。"}
{"text": "在训练的过程中，可以学习一定的语义层信息，词向量具有的空间平移性也简单地说明了这点。"}
{"text": "类似于词向量，经典的知识图谱嵌入模型TransE的设计思想就是，如果一个三元组（h,r,t）成立，那么它们需要符合h+r≈t关系，例如：vec（Rome）+vec（is?capital?of）≈vec（Italy）所以，在知识图谱嵌入的学习过程中，不同的模型从不同的角度把相应的语义信息嵌入知识图谱的向量表示中，如图2-23所示。"}
{"text": "图2-23语义信息嵌入知识图谱的向量表示中2.5.4知识图谱嵌入的优点研究者将目光从传统的知识图谱表示方法转移到知识图谱的嵌入方法，是因为与之前的方法相比，用向量表达实体和关系的知识图谱嵌入方法有很多优点。"}
{"text": "使用向量的表达方式可以提高应用时的计算效率，当把知识图谱的内容映射到向量空间时，相应的算法可以使用数值计算，所以计算的效率也会同时提高。"}
{"text": "用向量表示后，知识图谱将更加适用于当前流行的机器学习算法，例如神经网络等方法。"}
{"text": "因为下游应用输入的并不再是符号，所以可以考虑的方法也不会仅局限于图算法。"}
{"text": "将知识图谱嵌入作为下游应用的预训练向量输入，使得输入的信息不再是孤立的不包含语义信息的符号，而是已经经过一次训练，并且包含一定信息的向量。"}
{"text": "如上所述，知识图谱的嵌入方法可以提高计算的效率，增加下游应用的多样性，并可以作为预训练，为下游模型提供语义支持，所以对其展开的研究具有很大的应用价值和前景。"}
{"text": "2.5.5知识图谱嵌入的主要方法多数知识图谱嵌入模型主要依靠知识图谱中可以直接观察到的信息对模型进行训练，也就是说，根据知识图谱中所有已知的三元组训练模型。"}
{"text": "对于这类方法，常常只需训练出来的实体表示和矩阵表示满足被用来训练的三元组即可，但是这样的结果往往并不能完全满足所有的下游任务。"}
{"text": "所以，当前也有很多的研究者开始关注怎么利用一些除知识图谱之外的额外信息训练知识图谱嵌入。"}
{"text": "这些额外的信息包括实体类型（Entity_Types）、关系路径（Relation_Paths）等。"}
{"text": "根据有关知识图谱嵌入的综述[11]，将知识图谱嵌入的方法分类介绍如下。"}
{"text": "1.转移距离模型转移距离模型（Translational_Distance_Model）的主要思想是将衡量向量化后的知识图谱中三元组的合理性问题，转化成衡量头实体和尾实体的距离问题。"}
{"text": "这一方法的重点是如何设计得分函数，得分函数常常被设计成利用关系把头实体转移到尾实体的合理性的函数。"}
{"text": "受词向量的启发，由词与词在向量空间的语义层面关系，可以拓展到知识图谱中头实体和尾实体在向量空间的关系。"}
{"text": "也就是说，同样可以考虑把知识图谱中的头实体和尾实体映射到向量空间中，且它们之间的联系也可以考虑成三元组中的关系。"}
{"text": "TransE[12]便是受到了词向量中平移不变性的启发，在TransE中，把实体和关系都表示为向量，对于某一个具体的关系（head,relation,tail），把关系的向量表示解释成头实体的向量到尾实体的向量的转移向量（Translation_vector）。"}
{"text": "也就是说，如果在一个知识图谱中，某一个三元组成立，则它的实体和关系需要满足关系head+relation≈tail。"}
{"text": "2.语义匹配模型相比于转移距离模型，语义匹配模型（Semantic_Matching_Models），更注重挖掘向量化后的实体和关系的潜在语义。"}
{"text": "该方向的模型主要是RESCAL[13]以及它的延伸模型。"}
{"text": "RESCAL模型的核心思想是将整个知识图谱编码为一个三维张量，由这个张量分解出一个核心张量和一个因子矩阵，核心张量中每个二维矩阵切片代表一种关系，因子矩阵中每一行代表一个实体。"}
{"text": "由核心张量和因子矩阵还原的结果被看作对应三元组成立的概率，如果概率大于某个阈值，则对应三元组正确；否则不正确。"}
{"text": "其得分函数可以写成DistMul[14]通过限制Mr为对角矩阵简化RESCAL模型，也就是说其限制Mr=diag（r）。"}
{"text": "但因为是对角矩阵，所以存在h?diag（r）t=t?diag（r）h，也就是说这种简化的模型只天然地假设所有关系是对称的，显然这是不合理的。"}
{"text": "ComplEx[15]模型考虑到复数的乘法不满足交换律，所以在该模型中实体和关系的向量表示不再依赖实数而是放在了复数域，从而其得分函数不具有对称性。"}
{"text": "也就是说，对于非对称的关系，将三元组中的头实体和尾实体调换位置后可以得到不同的分数。"}
{"text": "3.考虑附加信息的模型除了仅仅依靠知识库中的三元组构造知识图谱嵌入的模型，还有一些模型考虑额外的附加信息进行提升。"}
{"text": "实体类型是一种容易考虑的额外信息。"}
{"text": "在知识库中，一般会给每个实体设定一定的类别，例如Rome具有city的属性、Italy具有country的属性。"}
{"text": "最简单的考虑实体类型的方法是在知识图谱中设立类似于IsA这样的可以表示实体属性的关系，例如（Rome,IsA,city）（Italy,IsA,Country）这样的三元组。"}
{"text": "当训练知识图谱嵌入的时候，考虑这样的三元组就可以将属性信息考虑到向量表示中。"}
{"text": "也有一些方法[16]考虑相同类型的实体需要在向量表示上更加接近。"}
{"text": "关系路径也可以称为实体之间的多跳关系（Multi-hop_Relationships），一般就是指可以连接两个实体的关系链，例如（Rome,is?capital?of,Italy）（Italy,is?country?of,Europe）.从Rome到Europe的关系路径就是一条is?capital?of→is?country?of关系链。"}
{"text": "当前很多方法也尝试考虑关系路径来提升嵌入模型，这里的关键问题是考虑如何用相同的向量表达方式来表达路径。"}
{"text": "在基于路径的TransE，也就是PTransE[17]中，考虑了相加、相乘和RNN三种用关系表达关系路径的方法：p=r1+r2+?+rlp=r1?r2???rlci=f（W[ci?1;ri]）.在基于RNN的方法中，令c1=r1并且一直遍历路径中的关系，直到最终p=cn。"}
{"text": "对于某一个知识库中存在的三元组，其两个实体间的关系路径p需要和原本两个实体间关系的向量表示相接近。"}
{"text": "文本描述（Textual_Descriptions）指的是在一些知识图谱中，对实体有一些简要的文本描述，如图2-24所示，这些描述本身具有一定的语义信息，对提高嵌入的质量有一定的提升。"}
{"text": "除了某些知识库本身具有的文本描述，也可以使用外部的文本信息和语料库。"}
{"text": "Wang[18]提出了一种在知识图谱嵌入的过程中使用文本信息的联合模型，该模型分三个部分：知识模型、文本模型和对齐模型。"}
{"text": "其中，知识模型对知识图谱中的实体和关系做嵌入，这是一个TransE的变种；文本模型对语料库中词语进行向量化，这是一个Skip-gram模型的变种；对齐模型用来保证知识图谱中的实体和关系与单词的嵌入在同一个空间中。"}
{"text": "联合模型在训练时降低来自三个子模型的损失之和。"}
{"text": "图2-24文本描述示例逻辑规则（Logical_Rules）也是常被用来考虑的附加信息，这里讨论的重点主要是霍恩子句，例如简单规则?x,y:IsDirectorOf（x,y）?BeDirectedBy（y,x）说明了两个不同的关系之间的关系。"}
{"text": "Guo[19]提出了一种以规则为指导的知识图谱嵌入方法，其中提出的软规则（Soft_rule）指的是使用AMIE+规则学习方法在知识图谱中挖掘的带有置信度的规则，该方法的整体框架是一个迭代的过程，其中包含两个部分，称为软标签预测阶段（Soft_Label_Prediction）和嵌入修正阶段（Embedding_Rectification）。"}
{"text": "简单来说，就是讲规则学习和知识图谱嵌入学习互相迭代，最后使得知识图谱嵌入可以融入一定的规则信息。"}
{"text": "2.5.6知识图谱嵌入的应用在知识图谱嵌入的发展中，也有很多的相关应用一起发展起来，它们和知识图谱嵌入之间有着相辅相成的关系。"}
{"text": "本小节将简单介绍一些典型的应用。"}
{"text": "1.链接预测链接预测（Link_Prediction）指通过一个已知的实体和关系预测另一个实体，或者通过两个实体预测关系。"}
{"text": "简单来说，也就是（h,r,?）,（?,r,t）,（h,?,t）三种知识图谱的补全任务，被称为链接预测。"}
{"text": "当知识图谱的嵌入被学习完成后，知识图谱嵌入就可以通过排序完成。"}
{"text": "例如需要链接预测(Roma,is-capital-of,?)，可以将知识图谱中的每个实体都放在尾实体的位置上，并且放入相应的知识图谱嵌入模型的得分函数中，计算不同实体作为该三元组的尾实体的得分，也就是该三元组的合理性，得分最高的实体会被作为链接预测的结果。"}
{"text": "链接预测也常被用于评测知识图谱嵌入。"}
{"text": "一般来说，会用链接预测的正确答案的排序评估某种嵌入模型在链接预测上的能力，比较常见的参数有平均等级（Mean_Rank）、平均倒数等级（Mean_Reciprocal_Rank）和命中前n（Hist@n）。"}
{"text": "2.三元组分类三元组分类（Triple_Classification）指的是给定一个完整的三元组，判断三元组的真假。"}
{"text": "这对于训练过的知识图谱向量来说非常简单，只需要把三元组各个部分的向量表达带入相应的知识图谱嵌入的得分函数，三元组的得分越高，其合理性和真实性越高。"}
{"text": "3.实体对齐实体对齐（Entity_Resolution）也称为实体解析，任务是验证两个实体是否指代或者引用的是同一个事物或对象。"}
{"text": "该任务可以删除同一个知识库中冗余的实体，也可以在知识库融合的时候从异构的数据源中找到相同的实体。"}
{"text": "一种方法是，如果需要确定x、y两个实体指代同一个对象有多大可能，则使用知识图谱嵌入的得分函数对三元组(x,EqualTo,y)打分，但这种方法的前提是需要在知识库中存在EqualTo关系。"}
{"text": "也有研究者提出完全根据实体的向量表示判断，例如设计一些实体之间的相似度函数来判断两个实体的相似程度，再进行对齐。"}
{"text": "4.问答系统利用知识图谱完成问答系统是该任务的一个研究方向，该任务的重心是对某一个具体的通过自然语言表达的问题，使用知识图谱中的三元组对其进行回答，如下：A:Where_is_the_capital_of_Italy？Q:Rome（Rome,is-capital-of,Italy）A:Who_is_the_president_of_USA？Q:Donald_Trump（Donald_Trump,is-president-of,USA）文献[9]介绍了一种借助知识图谱嵌入完成该问题的方法。"}
{"text": "简单来说就是设计一种得分函数，使问题的向量表示和其正确答案的向量表示得分较高。"}
{"text": "S（q,a）是被设计出来的得分函数S（q,a）=（Wφ（q））?（Wψ（a））.式中，W为包含词语、实体和关系的向量表示的矩阵；φ（q）为词语出现的稀疏向量；ψ（a）为实体和关系出现的稀疏向量。"}
{"text": "简单来说，Wφ（q）和Wψ（a）可以分别表示问题和答案的向量表示。"}
{"text": "当a是q的正确答案时，得分函数S（q,a）被期望得到一个较高的分数，反之亦然。"}
{"text": "5.推荐系统推荐系统的本质是对用户推荐其没有接触过的、但有可能会感兴趣或者购买的服务或产品，包括电影、书籍、音乐、商品等。"}
{"text": "协同过滤算法（Collaborative_Filtering）对用户和物品项目之间的交互进行建模并作为潜在表示取得了很好的效果。"}
{"text": "在知识图谱嵌入的发展下，推荐系统也尝试借助知识图谱的信息提高推荐系统的能力。"}
{"text": "例如，Zhang[20]尝试知识图谱中的三元组、文本信息和图像信息对物品项目进行包含一定语义的编码得到相应的向量表示，然后使用协同过滤算法对用户进行向量表示，对两个向量表示相乘得到分数，得分越高说明该用户越喜好该商品。"}
{"text": "2.7本章小结本章比较全面地介绍了知识图谱的表示与建模方法。"}
{"text": "目前大部分开放知识图谱的表示语言基于RDF、RDFS和OWL，这几个语言是W3C推荐的标准本体语言。"}
{"text": "除了这些标准语言，本章还介绍了知识图谱的查询语言SPARQL和语义Markup语言。"}
